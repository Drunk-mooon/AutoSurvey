{
    "survey": "# A Comprehensive Survey on Vision Transformers: Architectures, Efficiency, and Applications\n\n## 1 Introduction and Historical Context\n\n### 1.1 The Paradigm Shift from CNNs to Transformers\n\nThe landscape of computer vision has been historically dominated by Convolutional Neural Networks (CNNs), which have served as the de facto standard for a wide array of visual recognition tasks for over a decade. The success of CNNs is fundamentally rooted in their architectural design, which embeds strong inductive biases tailored specifically to the nature of visual data. These biases primarily consist of two key principles: locality and translation equivariance. By design, convolutional layers operate on local receptive fields, assuming that pixels in close spatial proximity are semantically correlated and that features relevant to a task can be detected regardless of their position in the image. This architectural prior allows CNNs to learn hierarchical representations efficiently, starting from simple edges and textures in early layers to complex object parts in deeper layers, while drastically reducing the number of parameters compared to fully connected architectures. However, these very strengths also constitute their primary limitations. The rigid adherence to local processing can hinder the modeling of long-range dependencies and global context, which are often crucial for understanding complex scenes. Furthermore, while data augmentation can help models learn invariance to transformations, the architectural priors of CNNs do not inherently capture the full spectrum of geometric symmetries beyond translation.\n\nThe introduction of the Vision Transformer (ViT) marked a radical departure from this established paradigm, representing a significant paradigm shift in computer vision [1]. Inspired by the monumental success of Transformers in Natural Language Processing (NLP), ViT proposed to treat an image not as a grid of pixels, but as a sequence of patches. The image is first split into a sequence of fixed-size, non-overlapping patches, which are then linearly embedded and fed into a standard Transformer encoder, originally designed for sequential data. This approach effectively discards the locality and translation equivariance priors hard-coded into CNNs. Instead, ViT relies almost exclusively on the self-attention mechanism to learn relationships between patches. Self-attention allows every patch to directly attend to every other patch, enabling the model to capture global context from the very first layer. This shift from local, hierarchical feature extraction to global, context-aware processing is the core of the paradigm shift.\n\nThe primary advantage of this minimal prior approach is its flexibility. By removing the constraints of local convolution, the model is free to learn whatever spatial relationships are most effective for the task at hand. This has allowed ViTs to achieve state-of-the-art performance on various benchmarks, often surpassing highly optimized CNNs. However, this newfound flexibility comes at a cost. The original ViT model is notoriously data-hungry. Without the strong inductive biases of CNNs, ViTs require massive-scale pre-training datasets like ImageNet-21k or JFT-300M to generalize effectively. On smaller, more common datasets like CIFAR or standard ImageNet-1k, a ViT trained from scratch often underperforms compared to a well-tuned CNN. This data inefficiency highlights a fundamental trade-off: the fewer the architectural priors, the more data is required to learn them implicitly.\n\nThis trade-off has been a central theme in the evolution of Vision Transformers, sparking a rich line of research aimed at reintroducing useful priors without sacrificing the global modeling capabilities of self-attention. Early investigations sought to understand the nature of the priors learned by ViTs. For instance, [2] provided theoretical justification for how ViTs, despite their lack of explicit locality, learn to associate patches with their spatial neighbors during training. This work suggests that the optimization process itself can induce spatial structure, a phenomenon they term \"patch association.\" Similarly, [3] offers an intuitive explanation, showing through visualization and representation similarity analysis that the representations learned by ViTs on small datasets are fundamentally different and less structured than those learned on large datasets, leading to poor performance. This underscores that while ViTs can learn spatial biases, they do so inefficiently without sufficient data.\n\nThe community's response to this data inefficiency has been multifaceted. One prominent direction has been to explicitly re-introduce convolutional inductive biases into the ViT architecture, creating a spectrum of hybrid models. The motivation is to combine the sample efficiency of CNNs with the powerful global context modeling of Transformers. For example, [4] introduces \"gated positional self-attention,\" which can be initialized to mimic the locality of convolutions but is given the freedom to escape it during training. This \"soft\" bias approach allows the model to benefit from convolutional priors early in training while retaining the flexibility to learn global patterns, resulting in better performance and sample efficiency than pure ViTs like DeiT. Similarly, [5] proposes a more explicit hybrid design, integrating convolutional blocks in parallel with self-attention modules within each Transformer layer. This allows the model to collaboratively learn local features (from the convolutional branch) and global dependencies (from the attention branch). Other works like [6] and [7] inject depth-wise convolutions directly into the feed-forward networks or as shortcuts, providing a lightweight way to enhance local feature extraction.\n\nBeyond simply adding convolutions, other research has focused on modifying the core components of the Transformer to better suit visual data. A key challenge is the quadratic complexity of self-attention with respect to the number of patches, which makes processing high-resolution images computationally expensive. This has led to the development of efficient attention mechanisms and hierarchical structures. [8] proposes a regional-to-local attention scheme where regional tokens capture global information and then disseminate it to associated local tokens, effectively creating a multi-scale feature pyramid similar to CNNs. This reintroduces a hierarchical prior that is natural for vision tasks. The concept of shift equivariance, a fundamental property of CNNs, has also been a point of investigation for ViTs. While the self-attention operator is permutation-equivariant, other components like patch embedding and positional encoding can break this property. Works like [9] and [10] propose adaptive designs to ensure that ViT variants achieve true shift-equivariance, making their predictions consistent under small input perturbations and improving their robustness.\n\nThe debate over the necessity and role of inductive biases is ongoing. Some works argue that the strong biases of CNNs are not always beneficial. [11] analyzes the effect of these biases on small to moderately-sized networks and shows that their removal is not always ideal, suggesting a more nuanced view. Conversely, [12] points out two specific weaknesses of ViTs on small data: a lack of spatial relevance learning and insufficient channel representation diversity. They propose a dynamic hybrid architecture (DHVT) to address these issues, reinforcing the idea that some form of locality is crucial for data efficiency. The concept of \"bootstrapping\" ViTs by training them to learn from an agent CNN further illustrates this, as seen in [13], where the goal is to transfer the inductive biases of a CNN to a ViT to liberate it from the need for massive pre-training.\n\nInterestingly, the paradigm shift is not entirely one-way. As ViTs mature, researchers are also exploring how to make CNNs more like Transformers. [14] argues that the initial patch embedding in ViTs is a critical component and that using a convolutional embedding can inject desirable inductive bias, boosting performance. This shows a cross-pollination of ideas, where the best of both worlds are combined. The evolution from pure CNNs to pure ViTs and now to a rich ecosystem of hybrid and specialized architectures demonstrates that the optimal solution likely lies in a thoughtful integration of priors rather than a complete abandonment of them. The initial paradigm shift to pure Transformers was a necessary step to challenge assumptions and unlock new capabilities, but the subsequent refinement phase is about intelligently reintroducing the biases that make vision unique, leading to more robust, efficient, and powerful models.\n\n### 1.2 The Original Vision Transformer (ViT) Architecture\n\nThe Original Vision Transformer (ViT) architecture represents a paradigm shift in computer vision, moving away from the long-standing dominance of convolutional neural networks (CNNs) by adapting the Transformer architecture, originally designed for natural language processing, to image recognition tasks. Introduced in the seminal work \"Vision Transformer (ViT) and its Derivatives\" [15], the core philosophy of ViT is based on the hypothesis that \"an image is worth 16x16 words,\" meaning that an image can be effectively represented as a sequence of fixed-size patches, analogous to words in a sentence. This approach allows the model to leverage the powerful global context modeling capabilities of self-attention mechanisms, which have revolutionized NLP, to capture long-range spatial dependencies within images that are often difficult for CNNs to model efficiently. As discussed in the previous section, this design intentionally discards the locality and translation equivariance priors of CNNs, relying instead on self-attention to learn spatial relationships from data.\n\nThe architecture of the original ViT is characterized by its simplicity and its lack of convolutional operators, relying entirely on standard Transformer components. The process begins with the input image, which is subjected to a series of transformations to convert it into a sequence of tokens suitable for the Transformer encoder. The first step involves splitting the input image into fixed-size non-overlapping patches. Typically, these patches are of size $16 \\times 16$ pixels, although other sizes like $14 \\times 14$ or $32 \\times 32$ have also been explored. For a standard $224 \\times 224$ input image, this results in a grid of $14 \\times 14$ patches, yielding a total of 196 patches. Each patch is then flattened into a vector by concatenating all pixel values within the patch. If the input image has $C$ channels (e.g., 3 for RGB) and the patch size is $P \\times P$, the resulting vector length is $C \\times P \\times P$. This flattened vector is then projected into a $D$-dimensional embedding space using a trainable linear projection layer, often referred to as the \"Patch Embedding\" layer. This projection transforms the raw pixel data into a dense representation that the Transformer can process.\n\nTo preserve spatial information, which is lost during the flattening and projection process, ViT adds a learnable positional embedding to the patch embeddings. Since the Transformer architecture is permutation-invariant, treating the sequence of tokens as an unordered set, the positional embeddings provide the model with information about the location of each patch in the original image. These embeddings have the same dimension $D$ as the patch embeddings and are added element-wise. The original ViT paper explored several schemes for positional embeddings, including learned 1D and 2D embeddings, and fixed sine-cosine embeddings similar to those used in the original Transformer. The learned 1D positional embeddings were found to be sufficient and were used in the final model. A special learnable classification token, often denoted as `[16]`, is prepended to this sequence. This token is initialized similarly to the patch embeddings and is trained to aggregate the global information from the entire image. The resulting sequence of tokens (including the `[16]` token and the patch embeddings) is then passed through a standard Transformer encoder.\n\nThe Transformer encoder consists of a stack of identical layers, each containing two main components: a Multi-Head Self-Attention (MSA) mechanism and a Multi-Layer Perceptron (MLP) block. Layer Normalization (LN) is applied before each sub-layer, and residual connections are used around each sub-layer. The MSA mechanism allows the model to weigh the importance of different patches relative to each other, enabling it to capture global context. For example, the model can learn to attend to patches containing \"ears\" when processing a patch containing \"head\" to better understand the structure of an animal. The MLP block, which consists of two linear layers with a GELU activation function, processes the attended features. The output of the final Transformer encoder layer's `[16]` token is then fed into a standard MLP head (a single linear layer) to predict the class label.\n\nThe original ViT model demonstrated that it could achieve excellent results on image classification benchmarks like ImageNet, rivaling or even surpassing state-of-the-art CNNs, provided it is pre-trained on extremely large datasets (e.g., JFT-300M). This highlighted a key characteristic of ViT, as noted in the previous section: its reliance on massive data for training, stemming from its weaker inductive biases compared to CNNs. While CNNs inherently possess translation equivariance and locality, ViT learns these properties from data. This data-hungry nature was a significant challenge that subsequent works, such as DeiT [17], aimed to address through knowledge distillation and refined training strategies.\n\nThe architectural simplicity of ViT, which eschews convolutions entirely for the core processing, makes it a pure and direct application of the Transformer to vision. This purity allows for the analysis of what the model learns, with studies showing that ViT heads often specialize in detecting specific visual patterns, similar to findings in NLP [18]. The ability to process images as sequences of patches has also opened the door for unified multimodal architectures that can process both text and images using the same backbone, a direction that has become increasingly prominent with the rise of Vision-Language Models. However, the original ViT architecture also faces challenges, particularly regarding computational complexity. The self-attention mechanism has a quadratic complexity with respect to the number of tokens, which can become prohibitive for high-resolution images. Furthermore, the fixed-size patching and lack of hierarchical structure make the original ViT less suitable for dense prediction tasks like object detection and segmentation. These limitations spurred the development of hierarchical variants like Swin Transformer and hybrid architectures, which will be discussed in the following sections.\n\n### 1.3 Addressing Data Inefficiency: DeiT and Knowledge Distillation\n\nThe original Vision Transformer (ViT) demonstrated that a pure transformer architecture, devoid of convolutional inductive biases, could achieve state-of-the-art performance in computer vision. However, its success was heavily contingent upon pre-training on massive proprietary datasets like JFT-300M, which contains hundreds of millions of images. This created a significant barrier to entry for the broader research community, as training ViT from scratch on standard academic datasets such as ImageNet resulted in performance inferior to that of Convolutional Neural Networks (CNNs). This \"data inefficiency\" stemmed from the lack of local priors inherent in ViT, necessitating vast amounts of data to learn robust visual representations. The Data-efficient Image Transformer (DeiT) framework, introduced in [19], directly addressed this critical limitation, proving that ViTs could be trained effectively on a single GPU using only ImageNet-scale data.\n\nThe core innovation of DeiT lies in its sophisticated knowledge distillation strategy. While knowledge distillation (KD) had long been used to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model, DeiT adapted this paradigm to ViTs in a novel way. Instead of relying solely on soft targets (probability distributions) from a teacher, DeiT introduced a dedicated **distillation token** (distill token) alongside the standard classification token (cls token). During training, the model optimizes two distinct losses: one for the cls token to match the ground truth label, and another for the distill token to match the teacher\u2019s prediction. This allows the student ViT to learn from the teacher in two complementary ways: implicitly through the attention mechanism between the two tokens, and explicitly through the distillation loss. The authors of [19] demonstrated that this token-based distillation is significantly more effective than traditional logit-based distillation for ViTs.\n\nCrucially, DeiT showed that the choice of teacher model is vital. While a large ViT teacher could be used, the most significant gains were achieved by distilling from a high-performing CNN teacher, such as RegNet. This is because the CNN possesses strong local inductive biases that the ViT lacks. By distilling from a CNN, the ViT effectively internalizes these local feature extraction capabilities, which are essential for learning from limited data. This synergy allows the student ViT to inherit the generalization power of the teacher, achieving 83.1% top-1 accuracy on ImageNet with a ViT-Base model trained from scratch, a result competitive with much more data-hungry predecessors.\n\nBeyond the architectural modification of the distillation token, DeiT also introduced a refined training recipe tailored for ViTs. The authors found that standard regularization techniques used for CNNs needed adjustment. They employed aggressive data augmentation, including Mixup and CutMix, and a sophisticated stochastic depth strategy. Furthermore, the training schedule and optimizer settings were carefully tuned to stabilize the training of ViTs on smaller datasets. This combination of a novel distillation token, the strategic use of CNN teachers, and a robust training recipe effectively bridged the data gap, democratizing Vision Transformers and enabling their widespread adoption.\n\nThis success spurred further research into data-efficient training and distillation for ViTs, exploring various facets of the problem. For instance, [20] extended the DeiT framework to handle long-tailed distributions, a scenario where data inefficiency is even more pronounced. They proposed re-weighting the distillation loss to focus more on tail classes and using out-of-distribution images to enhance the learning of local features, which are critical for recognizing rare categories. In parallel, the community investigated the nature of the knowledge being distilled. [21] provided a systematic study of feature-based distillation for ViTs, deriving guidelines that often contradicted common practices in the CNN era. They found that distilling features from intermediate layers, rather than just the final output, was highly effective, but required careful selection of layers and normalization strategies, underscoring the unique characteristics of ViT feature spaces.\n\nThe drive for efficiency also led to methods like [22], which focused on distilling knowledge from large pre-trained models into tiny, mobile-friendly ViTs. By sparsifying and storing teacher logits on disk, TinyViT enables efficient distillation without prohibitive memory overhead, allowing small models to benefit from massive pretraining data. In parallel, the concept of \"data-free\" distillation has emerged as a powerful tool for ViTs. [23] addresses the challenge of deploying ViTs on resource-constrained devices by compressing models without access to the original training data, which is often proprietary or privacy-sensitive. This line of work demonstrates that distillation is not just a tool for training from scratch on limited data, but also a key enabler for model compression and deployment in data-scarce scenarios.\n\nFurthermore, the evolution of distillation techniques has been crucial for adapting ViTs to new domains and tasks. For example, [24] proposes a framework (C2VKD) specifically for transferring knowledge from a CNN teacher to a ViT student for dense prediction tasks like segmentation. This work highlights the challenges of cross-architecture distillation for tasks requiring pixel-level precision and introduces novel feature alignment and target decoupling strategies to overcome them. Finally, the principles of data efficiency and distillation have been integrated with other advanced training paradigms. [25] explores distilling knowledge from large Masked Image Modeling (MIM) pre-trained models into smaller ones. Since MIM is a highly effective but computationally expensive pre-training method, distillation provides a pathway to transfer its benefits to smaller models that cannot directly benefit from MIM pre-training, acting as a bridge between state-of-the-art pre-training methods and practical, efficient architectures.\n\nIn summary, the DeiT framework was a watershed moment for Vision Transformers, proving that they could be trained efficiently on ImageNet without massive datasets. By introducing the distillation token and a refined training recipe, it unlocked the potential of ViTs for the broader research community. The subsequent explosion of research in data-efficient training, feature distillation, model compression, and domain adaptation all build upon these foundational concepts, solidifying distillation as an indispensable tool in the modern ViT ecosystem.\n\n### 1.4 Bridging the Gap: Hybrid CNN-Transformer Architectures\n\nThe initial introduction of Vision Transformers (ViTs) presented a paradigm shift in computer vision, moving away from the locality-centric operations of Convolutional Neural Networks (CNNs) toward a global self-attention mechanism. While this shift unlocked the ability to model long-range dependencies across an entire image, it came with a significant cost: the loss of essential inductive biases inherent to convolutions. As noted in the literature, pure ViTs often struggle with data inefficiency and lack the robustness to local structural patterns that CNNs naturally capture [26]. This divergence in architectural philosophies created a performance gap, particularly on smaller datasets or in scenarios where training resources were limited. To address these limitations, the research community turned to hybrid architectures, seeking to marry the local feature extraction capabilities of CNNs with the global context modeling of Transformers. This subsection examines these early hybrid models, which integrate convolutional layers directly into ViTs to inject local inductive biases, thereby combining the best of both worlds.\n\nThe motivation behind hybrid architectures stems from the fundamental differences in how CNNs and ViTs process visual information. CNNs rely on convolutional kernels that slide across the input, enforcing translation equivariance and locality\u2014inductive biases that are perfectly suited for natural images. In contrast, ViTs treat images as sequences of patches and utilize self-attention to compute interactions between all pairs of patches. While this allows for dynamic attention and global context, it requires massive amounts of data to learn the spatial relationships that CNNs assume by default. The paper \"Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision A survey\" highlights that the synergy between these two approaches can push the boundaries of computer vision by leveraging the strengths of each. Specifically, hybrid models aim to retain the \"shift, scale, and distortion invariance\" of CNNs while maintaining the \"dynamic attention, global context, and better generalization\" of Transformers [27].\n\nOne of the pioneering efforts in this direction is the Convolutional vision Transformer (CvT). CvT introduces convolutions to ViT through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding and a convolutional Transformer block leveraging a convolutional projection. By embedding patches using convolutional operations, CvT effectively introduces a locality bias at the very input of the Transformer, allowing the model to capture local features before global attention is applied. This approach not only improves performance and efficiency but also demonstrates that the explicit positional encoding, a crucial component in vanilla ViTs, can be safely removed or rendered less critical when convolutional embeddings are used, as the convolutional operations themselves provide a sense of spatial ordering [27].\n\nBuilding on this concept, researchers explored deeper integrations of convolutional operations within the Transformer blocks themselves. The paper \"Convolutional Embedding Makes Hierarchical Vision Transformer Stronger\" investigates the role of token embedding layers, specifically alias convolutional embedding (CE), and reveals how CE injects desirable inductive bias into ViTs. The study shows that simply replacing linear embeddings with convolutional ones can significantly boost the performance of hierarchical ViTs across various tasks, including classification, detection, and segmentation. This suggests that the macro-architecture of hybrid models, specifically how convolutional layers are embedded, plays a critical role in enhancing the representational power of ViTs.\n\nFurthermore, the integration of depth-wise convolutions has emerged as a lightweight yet effective strategy to introduce locality. The paper \"Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets\" proposes adding a depth-wise convolution module as a shortcut in ViT models. This module bypasses entire Transformer blocks, ensuring that the model captures both local and global information with minimal computational overhead. This approach is particularly beneficial for small datasets, where the lack of inductive bias in pure ViTs leads to poor convergence and overfitting. By explicitly modeling local relationships, these hybrid models can compete with or even outperform CNNs while retaining the global context advantage of Transformers.\n\nAnother notable hybrid architecture is CoAtNet, which stands out for its systematic approach to combining convolution and attention. CoAtNet leverages the insight that depth-wise convolution and self-attention can be naturally unified via simple relative attention mechanisms. The architecture vertically stacks convolution layers and attention layers in a principled way, which has been shown to be surprisingly effective in improving generalization, capacity, and efficiency. CoAtNet demonstrates that by carefully designing the interaction between convolutional and attention layers, it is possible to achieve state-of-the-art performance across various datasets and resource constraints. For instance, CoAtNet achieves 86.0% top-1 accuracy on ImageNet without extra data, highlighting the power of hybrid design [28].\n\nThe paper \"Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets\" further emphasizes the importance of hybrid structures for data efficiency. The authors identify two weaknesses of ViTs: the lack of spatial relevance and diverse channel representation. To address these, they propose Dynamic Hybrid Vision Transformer (DHVT), which integrates convolution into patch embedding and multi-layer perceptron (MLP) modules. This forces the model to capture token features along with their neighboring features, effectively injecting spatial relevance. Additionally, DHVT introduces dynamic feature aggregation and \"head token\" designs to enhance channel representation. The results show that DHVT successfully eliminates the performance gap between CNNs and ViTs on small datasets like CIFAR-100 and ImageNet-1K, achieving state-of-the-art performance with lightweight models.\n\nHybrid models also address the challenge of handling high-resolution inputs and dense prediction tasks. The paper \"ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions\" proposes a plain, pre-training-free backbone that facilitates bidirectional interaction between CNN and Transformer. By injecting spatial pyramid multi-receptive field convolutional features into the ViT architecture, ViT-CoMer effectively alleviates the problems of limited local information interaction and single-feature representation in ViTs. This multi-scale fusion is crucial for tasks like object detection and semantic segmentation, where both fine-grained local details and global context are necessary.\n\nMoreover, the concept of hybridization extends to improving the robustness and generalization of ViTs. The paper \"ACC-ViT: Atrous Convolution's Comeback in Vision Transformers\" introduces Atrous Attention, a fusion of regional and sparse attention inspired by atrous convolution. This approach adaptively consolidates both local and global information while maintaining hierarchical relations, addressing the dilemma between preserving hierarchical structure and attaining global context. Similarly, \"FasterViT\" introduces a Hierarchical Attention (HAT) approach that decomposes global self-attention into multi-level attention, combining the benefits of fast local representation learning in CNNs with global modeling properties in ViTs. These innovations demonstrate that hybrid architectures are not merely about stacking convolutions and attention layers but about designing novel mechanisms that leverage the strengths of both.\n\nThe paper \"Grafting Vision Transformers\" introduces a simple and efficient add-on component called GrafT that considers global dependencies and multi-scale information throughout the network. GrafT can be integrated into existing hybrid and pure Transformer models, providing consistent gains by enhancing high-level semantics, particularly in mobile-size models. This highlights the flexibility of hybrid approaches, where convolutional components can be used to augment Transformer backbones without requiring a complete redesign.\n\nIn the context of mobile and edge deployment, hybrid architectures have proven to be highly effective. \"MobileViT\" presents a light-weight, general-purpose vision transformer that combines CNNs and ViTs to achieve low latency and high accuracy on mobile devices. By reinterpreting transformers as convolutions, MobileViT achieves a balance between local processing and global context, outperforming both CNN-based and ViT-based networks on ImageNet and object detection tasks. Similarly, \"Next-ViT\" introduces Next Convolution Blocks (NCB) and Next Transformer Blocks (NTB) designed specifically for efficient deployment in realistic industrial scenarios. These models demonstrate that hybrid designs can dominate both CNNs and ViTs in terms of the latency/accuracy trade-off, making them suitable for real-world applications.\n\nThe paper \"A ConvNet for the 2020s\" (ConvNeXt) serves as a counterpoint but also reinforces the importance of inductive biases. While ConvNeXt modernizes ResNet to compete with Transformers, it acknowledges that hierarchical Transformers like Swin reintroduced ConvNet priors to make Transformers practically viable. This underscores the fact that the success of hierarchical ViTs is largely due to the reincorporation of convolutional inductive biases, such as local processing and multi-scale feature maps.\n\nFinally, the synergy between CNNs and ViTs is not limited to architectural modifications but also extends to training strategies and initialization. The paper \"Transformed CNNs: recasting pre-trained convolutional layers with self-attention\" explores initializing self-attention layers as convolutional layers to transition smoothly from pre-trained CNNs to hybrid models. This approach significantly reduces training time and improves robustness, providing a practical pathway for leveraging existing CNN models in hybrid architectures.\n\nIn conclusion, hybrid CNN-Transformer architectures represent a crucial evolution in computer vision, addressing the limitations of pure ViTs by reintroducing local inductive biases. Through innovations like convolutional embeddings, depth-wise convolutions, unified attention mechanisms, and multi-scale feature interactions, these models combine the local feature extraction strengths of CNNs with the global context modeling of Transformers. The extensive research summarized in papers such as [27], [28], and [29] demonstrates that hybrid models achieve superior performance, data efficiency, and deployment readiness across a wide range of vision tasks. As the field continues to advance, the integration of convolutional and attention mechanisms will likely remain a cornerstone of efficient and robust vision model design.\n\n### 1.5 Hierarchical Designs for Dense Prediction\n\nThe original Vision Transformer (ViT) architecture, while revolutionary for image classification, presented significant hurdles for dense prediction tasks such as object detection and semantic segmentation. These tasks require the model to produce high-resolution feature maps to accurately localize and delineate objects, a capability inherently challenged by ViT's design. ViT processes an image as a sequence of fixed-size patches, maintaining a constant sequence length and resolution throughout its deep layers. This flat, non-hierarchical structure contrasts sharply with the multi-scale feature hierarchies found in Convolutional Neural Networks (CNNs), which progressively downsample feature maps to capture both fine-grained details and high-level semantic information. Furthermore, the quadratic computational complexity of the global self-attention mechanism in ViT makes it prohibitively expensive to operate on the high-resolution feature maps necessary for dense tasks. This computational bottleneck, combined with the lack of a natural feature pyramid, rendered the \"vanilla\" ViT unsuitable for the nuanced requirements of detection and segmentation.\n\nTo bridge this gap, the research community introduced a new class of Vision Transformers characterized by hierarchical architectures. These models were designed to reintroduce the multi-scale feature pyramid into the Transformer paradigm, effectively mimicking the successful design principles of CNNs while retaining the global context modeling capabilities of Transformers. The seminal work in this direction was the Pyramid Vision Transformer (PVT) [30]. PVT was the first to propose a hierarchical structure for pure Transformers, systematically reducing the spatial resolution of tokens across different stages while increasing the channel dimension. This was achieved by introducing a \"patch merging\" operation, analogous to the pooling or strided convolution layers in CNNs, to downsample the token sequence between Transformer blocks. By progressively reducing the number of tokens, PVT managed the computational cost associated with self-attention, enabling the processing of high-resolution inputs. This hierarchical design allowed PVT to generate feature maps at multiple scales, making it directly compatible with existing dense prediction heads like those used in Mask R-CNN. However, PVT still relied on global self-attention within each stage, which, despite the reduced sequence length, could still be computationally intensive.\n\nA more efficient and impactful solution emerged with the Swin Transformer [31]. Swin Transformer addressed the computational inefficiency of global attention by restricting self-attention computations to non-overlapping local windows. Within each window, tokens attend only to their local neighbors, reducing the complexity from quadratic to linear with respect to the number of patches in an image. However, purely local attention would severely limit the model's ability to capture global context. To overcome this, Swin introduced a clever \"shifted window\" mechanism. By alternating the window partition between consecutive Transformer blocks, it allows information to flow across window boundaries, effectively creating a receptive field that grows with network depth while maintaining the computational efficiency of local attention. This shifted window scheme is the cornerstone of Swin's success, providing a highly effective balance between modeling power and computational cost. The combination of this efficient attention scheme and a hierarchical structure (using patch merging) made Swin Transformer a powerful and versatile backbone, achieving state-of-the-art performance on major benchmarks for object detection (COCO) and semantic segmentation (ADE20K). Its success demonstrated that hierarchical designs were not just beneficial but essential for making ViTs competitive in dense prediction.\n\nThe evolution of hierarchical designs continued with various innovations aiming to refine the balance between performance and efficiency. For instance, Multiscale Vision Transformers (MViT) [32] proposed a different approach to building a feature pyramid. Instead of a post-hoc patch merging strategy, MViT embeds the multiscale concept directly into the attention mechanism itself. It starts with a high-resolution, low-channel-capacity stage and progressively expands the channel capacity while reducing the spatial resolution. This creates a feature pyramid where early layers operate at high resolution to model low-level details and deeper layers operate on coarser but more semantically rich features. MViTv2 [33] further refined this architecture with improvements like decomposed relative positional embeddings and residual pooling connections, solidifying its strong performance across classification, detection, and video recognition tasks. These models highlight a key design philosophy: reintroducing the inductive bias of a feature hierarchy is crucial for dense tasks.\n\nHowever, the rigid structure of hierarchical ViTs can sometimes be a limitation. Most designs use a fixed window size or a predetermined merging strategy. This inspired research into more dynamic and flexible hierarchical structures. For example, Beyond Fixation: Dynamic Window Visual Transformer [34] challenges the default use of a fixed single-scale window. It proposes a dynamic window strategy that assigns windows of different sizes to different attention heads, allowing the model to capture multi-scale information within a single layer. The weights for these multi-scale branches are then dynamically fused, adapting the receptive field to the input. This approach demonstrates that performance can be further improved by moving beyond static, predefined hierarchical structures towards more adaptive ones.\n\nFurthermore, the community has explored ways to enhance the representational power within the hierarchical framework. The Factorization Vision Transformer (FaViT) [35] addresses a key trade-off in window-based attention: while efficient, local windows can hinder the modeling of long-range dependencies and potentially reduce robustness. FaViT proposes a factorization self-attention mechanism that decomposes the standard attention matrix into sparse sub-matrices. This allows the model to capture long-range dependencies and mixed-grained information at a computational cost equivalent to standard local window attention, thereby improving both performance and robustness. Similarly, Brain-Inspired Stepwise Patch Merging [36] introduces a novel SPM technique to enhance the patch merging process itself. By incorporating Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE), SPM enriches feature representations and refines local detail extraction, leading to better performance in dense prediction tasks.\n\nThe drive towards efficiency in hierarchical designs is also evident in models designed for mobile and edge devices. While Swin Transformer improved efficiency over global attention, its shifted window mechanism still involves memory copy operations. Swin-Free [37] proposes to eliminate the shifting operation entirely, instead using size-varying windows across stages to achieve cross-window connections. This results in a conceptually simpler and faster model. On the other hand, models like FasterViT [38] focus on high image throughput by combining the local representation learning of CNNs with the global modeling of ViTs. Its Hierarchical Attention (HAT) approach decomposes global attention into a multi-level structure, enabling efficient cross-window communication and achieving a state-of-the-art Pareto frontier in accuracy versus speed.\n\nIn summary, the development of hierarchical designs was a critical evolutionary step that enabled Vision Transformers to move beyond classification and excel in dense prediction tasks. By reintroducing multi-scale feature maps through mechanisms like patch merging and by addressing the computational challenges with efficient attention schemes like shifted windows, models such as PVT and Swin Transformer established a new paradigm. Subsequent innovations have further refined this paradigm, introducing dynamic windows, factorized attention, and enhanced merging strategies to boost performance, robustness, and efficiency. These hierarchical architectures have successfully bridged the gap between the global context modeling of Transformers and the multi-scale feature extraction needs of dense tasks, making them the new standard backbone for a wide range of computer vision applications.\n\n### 1.6 The Rise of Mobile-Friendly and Efficient ViTs\n\nThe initial wave of Vision Transformers, while revolutionary in their performance, presented a formidable barrier to practical deployment, particularly on resource-constrained mobile and edge devices. The architectural design of the original ViT, which treats an image as a sequence of patches and relies heavily on global self-attention, results in computational and memory footprints that are often orders of magnitude higher than those of optimized Convolutional Neural Networks (CNNs) like MobileNet. This inefficiency stems from the quadratic complexity of the attention mechanism with respect to the number of tokens and the substantial parameter count of the Multi-Layer Perceptron (MLP) blocks. Consequently, a significant research thrust emerged with the singular goal of making ViTs \"mobile-friendly\"\u2014a drive to bridge the gap between the high accuracy of transformers and the stringent latency, energy, and memory constraints of edge hardware. This subsection explores the pivotal architectures and design philosophies that defined this era of efficiency, focusing on models that optimize ViTs for edge devices by reducing parameters, FLOPs, and, most critically, on-device latency.\n\nA foundational step in this direction was the development of hybrid architectures that strategically reintroduce the local inductive biases of convolutions to reduce the computational burden. While pure ViTs excel at capturing global context, convolutions are inherently efficient at extracting local features. Models like MobileViT [39] exemplify this synergy. MobileViT introduces a novel \"MobileViT block\" that combines standard MobileNet-like inverted residual blocks with a lightweight, multi-head self-attention module. The key innovation is the reshaping of feature maps into a sequence of tokens that are processed by the attention mechanism, but only after a convolutional projection has already reduced the spatial dimensions and enriched the features. This approach significantly lowers the number of tokens fed into the attention module, thereby reducing its quadratic complexity. The result is a model that achieves accuracy competitive with CNNs like MobileNetV2 while offering a better accuracy-latency trade-off on mobile CPUs and GPUs. Building on this, [40] further refines the design principles for MobileViT, using a Gaussian Process to systematically explore the non-linear relationships between architectural factors (e.g., resolution, width, depth) and performance. This work demonstrates that a principled, data-driven approach to scaling MobileViT can yield smaller, faster models that outperform both CNNs and other mobile ViTs, highlighting the importance of holistic architectural co-design beyond just block-level innovations.\n\nHowever, the reliance on convolutions in hybrid models was seen by some as a regression from the \"pure transformer\" paradigm. This led to a parallel line of research focused on designing entirely new, efficient transformer blocks that could achieve high performance without any convolutional layers. A landmark contribution in this space is EfficientFormer [41]. The authors of EfficientFormer critically re-examined the latency bottlenecks of standard ViTs on mobile hardware, identifying that operations like tensor reshaping and certain element-wise functions in the attention mechanism were surprisingly slow in practice, despite having low FLOPs. To address this, they proposed a dimension-consistent design paradigm where the feature map dimensions (e.g., channel count, spatial resolution) are maintained consistently throughout the transformer stages, avoiding costly reshaping operations. Furthermore, they introduced a \"sandwich layout\" for the building block, placing the most computationally intensive component, the multi-head self-attention (MHSA), between more efficient feed-forward network (FFN) layers. This design, coupled with a novel \"cascaded group attention\" module that reduces redundant computation across attention heads, allows EfficientFormer to achieve MobileNet-level latency on an iPhone 12 while significantly outperforming MobileNetV2 in accuracy. This work was pivotal because it proved that properly designed, pure transformers could indeed run as fast as CNNs on edge devices, shifting the focus from FLOPs to hardware-aware latency metrics.\n\nThe drive for efficiency also manifested in architectural designs that optimize the entire model pipeline, not just the core attention block. RepViT [42] takes a unique approach by revisiting the design of lightweight CNNs from a ViT perspective. It systematically integrates the efficient architectural choices of lightweight ViTs\u2014such as the use of large kernel convolutions and specific activation functions\u2014into the MobileNetV3 framework. The result is a family of pure CNNs that, while not transformers, borrow the best practices from the efficient ViT literature to achieve state-of-the-art performance and remarkably low latency on mobile devices. RepViT demonstrates that the lines between CNNs and ViTs are blurring, as the most efficient designs converge on similar principles. On the other end of the spectrum, [43] pushes the boundaries of efficiency through extreme quantization. By constraining weights to ternary values {-1, 0, 1} and activations to 8-bit precision, this work drastically reduces the memory and computational overhead of ViTs, making them viable for deployment on hardware with very limited resources. This highlights that algorithmic innovations in quantization are just as crucial as architectural changes for achieving true mobile efficiency.\n\nBeyond designing new architectures, another critical aspect of the mobile ViT revolution is the development of adaptive inference techniques that tailor the computational cost to the input. Standard ViTs process every input image with the same, fixed computational graph, regardless of its complexity. This is inherently inefficient, as simple images (e.g., a clear sky) require far less processing than complex scenes. To address this, methods like PIVOT [44] propose to dynamically skip attention computations based on the input's difficulty. PIVOT employs a co-optimization framework that uses hardware-in-the-loop search to determine optimal attention skip configurations, achieving significant energy savings with minimal accuracy loss. Similarly, PRANCE [45] introduces a framework that jointly optimizes the number of activated channels and tokens based on input characteristics. By using a meta-network and reinforcement learning, PRANCE can dynamically reduce both token sequence length and channel width, achieving substantial FLOP reductions while maintaining accuracy. These adaptive methods represent a paradigm shift from static, one-size-fits-all models to dynamic, input-aware systems that are fundamentally more efficient for real-world applications.\n\nFinally, the push for mobile efficiency has also spurred innovation in the training and deployment pipelines. Self-supervised learning (SSL) techniques like DINO and MAE have proven highly effective for pre-training ViTs on large, unlabeled datasets, reducing the reliance on massive labeled datasets that were a bottleneck for early ViTs. Furthermore, the community has increasingly adopted hardware-aware metrics for evaluating efficiency. Instead of relying solely on FLOPs or parameter counts, which can be poor proxies for on-device performance, researchers now prioritize metrics like measured latency, energy consumption, and memory footprint on real hardware. This is evident in works like TRT-ViT [46], which designs models specifically to be efficient under the TensorRT inference engine, and ElasticViT [47], which uses neural architecture search (NAS) to find optimal sub-networks for a wide range of mobile devices. The rise of mobile-friendly and efficient ViTs is therefore not just a story of new model architectures, but a comprehensive, multi-faceted effort involving architectural co-design, hardware-aware optimization, dynamic inference, and advanced training strategies, collectively enabling the deployment of powerful transformer models on the edge.\n\n### 1.7 Pure Convolution-Free Alternatives\n\nThe paradigm shift from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) was predicated on the idea that the global context captured by self-attention could surpass the local inductive biases inherent to convolutions. However, the initial wave of ViT architectures, including the seminal \"An Image is Worth 16x16 Words\" [48], often relied on hybrid designs or were criticized for being data-hungry and computationally inefficient. In response, a significant line of research has emerged focused on developing pure, convolution-free Transformer variants. These models aim to prove that convolutions are not a necessary component for efficient and high-performing vision backbones. Instead, they innovate at the fundamental levels of tokenization and attention to overcome the limitations of early ViTs, such as the lack of local inductive bias, high computational complexity, and suboptimal feature representation in the early stages.\n\nThe primary motivation behind pure convolution-free alternatives is to strip away any reliance on convolutions to achieve a \"pure\" Transformer architecture that can compete with or even outperform hybrid models. This pursuit addresses the core challenge of ViTs: how to effectively model local structures and manage computational costs without the help of convolutional kernels. The innovations in this domain can be broadly categorized into two interconnected areas: advanced tokenization strategies that create more meaningful token sequences, and novel attention mechanisms that are more efficient or expressive than standard self-attention.\n\nOne of the earliest and most influential works in this category is Tokens-to-Token ViT (T2T-ViT) [49]. The authors identified a key weakness in the original ViT: the simple patchifying tokenization process fails to capture the crucial local structures and correlations among neighboring pixels, leading to low training sample efficiency. To address this, T2T-ViT introduces a \"Tokens-to-Token\" transformation that progressively structurizes the image into tokens. This process recursively aggregates neighboring tokens into a single token, effectively modeling the local structure while simultaneously reducing the sequence length. This layer-wise transformation mimics the hierarchical feature extraction of CNNs without using convolutions. Furthermore, T2T-ViT proposes an efficient \"deep-narrow\" backbone design, motivated by empirical studies of CNN architectures, which reduces parameters and MACs compared to the original ViT. By combining a more intelligent tokenization process with a tailored network structure, T2T-ViT demonstrated that a pure ViT could be trained effectively from scratch on ImageNet, achieving superior performance to vanilla ViT and even competing with lightweight CNNs like MobileNets.\n\nBuilding on the idea of improving tokenization, other works have explored different ways to enrich the initial token representations. For instance, Lite Vision Transformer (LVT) [50] proposes an enhanced self-attention mechanism for low-level features. While LVT incorporates convolutions in its initial stage, its core philosophy aligns with the pure Transformer goal of enhancing representational power without relying on heavy convolutional blocks. It introduces Convolutional Self-Attention (CSA), which integrates local self-attention into a 3x3 convolutional kernel to enrich low-level features. For higher-level features, it uses Recursive Atrous Self-Attention (RASA) to capture multi-scale context. Although not entirely convolution-free, LVT highlights the trend of rethinking how attention can be modified to better suit visual tasks, a theme that pure convolution-free alternatives take to the extreme.\n\nBeyond tokenization, a major thrust of pure convolution-free research is the development of more efficient and effective attention mechanisms. The quadratic complexity of standard self-attention remains a significant bottleneck. To tackle this, models like LightViT [51] argue that the explicit aggregation provided by convolutions can be replicated in a more homogeneous way within the Transformer blocks themselves. LightViT introduces a global yet efficient aggregation scheme into both the self-attention and the Feed-Forward Network (FFN). It achieves this by introducing additional learnable global tokens that capture global dependencies and by applying bi-dimensional channel and spatial attentions over token embeddings. This allows LightViT to achieve a better accuracy-efficiency balance in a purely convolution-free design, demonstrating strong performance on classification, detection, and segmentation tasks with significantly lower FLOPs.\n\nThe quest for linear complexity has also produced several pure Transformer variants. Works like X-ViT [52] and UFO-ViT [53] propose to eliminate the non-linearity of the softmax function from self-attention to achieve linear complexity. By factorizing the matrix multiplication of the attention mechanism without complex approximations, these models can handle long sequences efficiently. Similarly, SOFT [54] proposes a softmax-free transformer by replacing the dot-product similarity with a Gaussian kernel function, enabling a full self-attention matrix to be approximated via low-rank decomposition. These approaches demonstrate that the quadratic bottleneck is not an inherent property of the Transformer architecture but rather a consequence of the specific softmax-based attention formulation, and that pure ViTs can be made highly efficient.\n\nFurthermore, some research questions the necessity of the attention mechanism itself in every layer. LaViT [55] argues against computing attention scores in every layer, proposing to compute them only in a few layers and then propagate the information to other layers via attention transformations. This reduces the computational burden and mitigates attention saturation. In a more radical departure, ShiftViT [56] completely replaces the attention layers with zero-parameter shift operations, which simply exchange a small portion of channels between neighboring features. Surprisingly, ShiftViT performs on par with strong baselines like Swin Transformer, suggesting that the success of ViTs may stem from factors beyond the attention mechanism itself, such as the patch-based tokenization and overall architectural design.\n\nAnother line of work focuses on enhancing the representational capacity of pure ViTs without adding computational overhead. BViT [57] introduces \"broad attention,\" which incorporates attention relationships across different layers. It uses broad connections and a parameter-free attention module to jointly leverage attention information from multiple layers, improving information flow and feature extraction. This approach shows that inter-layer attention dynamics are a valuable, yet underexplored, resource for pure Transformer models.\n\nThe development of pure convolution-free alternatives is also closely tied to the pursuit of mobile-friendly and efficient models. MobileViT [29] is a seminal work in this area, though it is a hybrid model. However, its philosophy of reinterpreting transformers as convolutions has inspired subsequent pure ViT research. Pure CNNs like RepViT [42] have revisited CNN designs from a ViT perspective, but the ultimate goal for many is a pure ViT that is both accurate and efficient. Models like LightViT and the efficient variants of T2T-ViT are steps in this direction, showing that by carefully designing tokenization and attention, pure ViTs can achieve competitive performance on mobile-scale tasks.\n\nIn conclusion, the subfield of pure convolution-free alternatives represents a rigorous examination of the fundamental components of Vision Transformers. By moving beyond the initial hybrid designs, researchers have demonstrated that convolutions are not essential for visual recognition. Instead, innovations in tokenization, as seen in T2T-ViT, and novel attention mechanisms, as proposed in LightViT, X-ViT, and SOFT, are the key drivers of performance. These models successfully address the challenges of local structure modeling, computational complexity, and representational power, proving that a pure Transformer architecture can be a versatile and powerful backbone for a wide range of computer vision tasks. The continued exploration in this area promises to further refine our understanding of what makes ViTs effective and to push the boundaries of efficient, high-performance vision models.\n\n## 2 Core Architectural Variants and Enhancements\n\n### 2.1 Fundamental Building Blocks\n\n### 2.2 Hierarchical Architectures\n\n### 2.3 Hybrid CNN-Transformer Models\n\n### 2.4 Efficient and Linear Attention Mechanisms\n\n### 2.5 Dynamic and Deformable Attention\n\n### 2.6 Structural and Modular Design Innovations\n\n### 2.7 Lightweight and Plug-and-Play Attention Modules\n\n## 3 Training Paradigms and Self-Supervised Learning\n\n### 3.1 Supervised Training Baselines and Optimization Dynamics\n\n### 3.2 Self-Supervised Pre-training: Contrastive and Distillation Approaches\n\n### 3.3 Generative and Reconstruction-Based Self-Supervision\n\n### 3.4 Data Augmentation and Curriculum Learning\n\n### 3.5 Semi-Supervised Learning and Self-Training\n\n### 3.6 Training Efficiency and Convergence Acceleration\n\n## 4 Efficiency, Compression, and Deployment Strategies\n\n### 4.1 Token Pruning and Merging\n\n### 4.2 Weight Compression: Pruning and Quantization\n\n### 4.3 Hardware-Aware Deployment and Compiler Optimization\n\n### 4.4 Distributed and Adaptive Inference Strategies\n\n## 5 Applications in High-Level Vision\n\n### 5.1 Image Classification and ViT Backbones\n\n### 5.2 Object Detection and Segmentation with DETR and Mask2Former\n\n### 5.3 Hybrid Architectures for Efficiency\n\n### 5.4 Hardware-Aware Design and Deployment Optimization\n\n### 5.5 Self-Supervised Learning and Adaptation\n\n## 6 Applications in Low-Level, Multimodal, and Specialized Domains\n\n### 6.1 Low-Level Vision: Restoration and Super-Resolution\n\n### 6.2 Video Understanding and Temporal Modeling\n\n### 6.3 Multimodal Reasoning: Vision-Language Models (VLMs)\n\n### 6.4 Specialized Domains: Medical Imaging and Autonomous Driving\n\n### 6.5 High-Resolution and Region-Level Perception\n\n### 6.6 Efficiency in Multimodal Applications\n\n## 7 Robustness, Interpretability, and Future Directions\n\n### 7.1 Adversarial Robustness and Distribution Shifts\n\nThe remarkable success of Vision Transformers (ViTs) across a wide array of computer vision tasks has been predicated on their ability to model long-range dependencies through self-attention, a stark contrast to the local inductive biases inherent in Convolutional Neural Networks (CNNs). While this architectural shift has unlocked new performance frontiers, particularly in data-rich environments, it has also introduced unique vulnerabilities. Specifically, the robustness of ViTs against adversarial perturbations and natural distribution shifts remains a critical area of investigation. This subsection delves into the sensitivity of ViTs to these challenges, synthesizing findings on their comparative performance against CNNs and reviewing the methodologies proposed to bolster their resilience.\n\n### Understanding Vulnerability: Adversarial Attacks on Vision Transformers\n\nAdversarial attacks, which involve making imperceptible perturbations to an input image to cause a model to misclassify it, expose fundamental weaknesses in deep learning models. The question of whether ViTs are inherently more or less robust than CNNs is complex and has been a subject of intense debate. Early empirical studies suggested that ViTs might possess a degree of natural robustness due to their global processing capabilities. However, a comprehensive analysis presented in **[58]** provides a more nuanced perspective. This work systematically investigates out-of-distribution (OOD) generalization, a broader category that includes adversarial shifts, and finds that ViTs learn weaker biases on backgrounds and textures compared to CNNs. Instead, ViTs develop stronger inductive biases towards shapes and structures, which is more aligned with human cognitive processes. This architectural characteristic allows ViTs to generalize better than CNNs under various distribution shifts, often outperforming corresponding CNN models by over 5% in top-1 accuracy. The study further notes that as model scale increases, ViTs strengthen these shape-based biases, gradually narrowing the performance gap between in-distribution and OOD data.\n\nDespite this inherent advantage in shape-based recognition, ViTs are not immune to adversarial attacks. The quadratic complexity of the self-attention mechanism and the patch-based tokenization process create unique attack surfaces. For instance, an adversary could manipulate a few key patches to disrupt the global context aggregated by the attention mechanism. The vulnerability of ViTs to such attacks underscores the need for specialized defense mechanisms. Adversarial training, the process of training models on a mixture of clean and adversarially generated examples, remains the most effective and widely used defense strategy. However, applying adversarial training to ViTs presents its own set of challenges, including increased training time and potential degradation of performance on clean data. Research in this area is actively exploring how to adapt adversarial training techniques, originally developed for CNNs, to the unique architecture of ViTs without compromising their efficiency or accuracy.\n\n### Generalization Under Natural Distribution Shifts\n\nBeyond adversarial perturbations, ViTs must also contend with natural distribution shifts, where the test data differs from the training data in ways such as changes in lighting, weather, artistic style, or camera quality. The ability to handle such shifts is a hallmark of a robust model. The findings from **[58]** are particularly illuminating here. By demonstrating that ViTs develop a stronger bias towards shapes and structures, the work suggests a reason for their superior performance on benchmarks like ImageNet-C (corrupted images) and stylized datasets. CNNs, with their strong texture-based biases, can be easily fooled by changes in texture, whereas ViTs, focusing on the underlying structure, are more resilient.\n\nThis structural bias is not an accident but a consequence of the ViT architecture. Unlike CNNs, which progressively build up hierarchical features from local to more global, ViTs process the entire image as a sequence of patches from the first layer. This allows them to capture global relationships early on, which helps in disentangling the core object structure from superficial stylistic variations. However, this does not mean ViTs are perfectly robust. They can still be sensitive to severe corruptions or shifts that alter the fundamental spatial relationships between patches. Furthermore, the reliance on large-scale pre-training for ViTs to achieve good generalization is a significant drawback in scenarios where data is limited. This data inefficiency can make them more susceptible to overfitting to the specific distribution of the training set, thereby worsening their performance under distribution shifts. The work in **[13]** highlights this challenge, noting that ViTs tend to overfit on small datasets and thus rely heavily on large-scale pre-training, which is computationally expensive. This dependency implies that without access to massive, diverse datasets, the robustness of ViTs to distribution shifts may be compromised.\n\n### Enhancing Robustness: Methodologies and Architectural Innovations\n\nTo address these vulnerabilities, researchers have proposed a range of techniques, from training paradigms to architectural modifications. A key insight is that the inductive biases of CNNs can be beneficial for robustness. Several works explore hybrid architectures that reintroduce these biases. For example, **[4]** introduces gated positional self-attention (GPSA), which can be initialized to mimic the locality of convolutions. This \"soft\" convolutional bias allows the model to start with a robust, locality-aware foundation and then learn to escape it if the data demands it, leading to better sample efficiency and improved performance, which indirectly contributes to robustness by learning more meaningful features. Similarly, **[5]** explicitly integrates convolutional blocks parallel to self-attention modules, enabling the model to learn local features and global dependencies collaboratively. This fusion of local and global information is shown to improve performance on downstream tasks, suggesting a more robust feature representation.\n\nAnother line of research focuses on improving the fundamental components of ViTs to enhance their stability and robustness. The issue of shift-equivariance, or the lack thereof, is a critical aspect of robustness to spatial shifts. Standard ViTs are not strictly shift-equivariant due to components like patch embedding and positional encoding. **[9]** proposes data-adaptive designs for these modules to achieve true shift-equivariance, ensuring that the model's output changes predictably with input shifts. This property is crucial for applications like object detection and segmentation, where consistency under translation is expected. By enforcing this architectural property, the model becomes more reliable and robust to minor spatial perturbations.\n\nTraining strategies also play a pivotal role. The work in **[59]** and the broader field of equivariant learning provide a framework for building models that are inherently robust to transformations like rotations and translations. While standard ViTs are not equivariant by design, they can learn to be. The analysis in **[60]** shows that as ViT models get larger and more accurate, they tend to display more learned equivariance, regardless of their architecture. This suggests that scale and appropriate training data are key to developing robustness. Self-supervised learning (SSL) methods, such as those discussed in **[61]**, can also contribute. By learning representations from unlabeled data, SSL encourages the model to capture the underlying structure of the data, which can improve its robustness to distribution shifts. For instance, contrastive methods learn features that are invariant to certain augmentations, while reconstruction-based methods like MAE learn to understand the holistic structure of an image.\n\nFurthermore, the very nature of self-attention itself can be analyzed for its robustness properties. The work in **[62]** provides a theoretical lens, showing that self-attention implicitly encourages low-rank structures and token clustering. This regularization effect can prevent the model from overfitting to spurious correlations in the training data, which is a common failure mode under distribution shifts. By promoting a more structured and compact representation, attention mechanisms may contribute to a more robust model.\n\nIn conclusion, while Vision Transformers exhibit promising robustness properties stemming from their structural biases towards shapes, they remain vulnerable to both adversarial and natural distribution shifts. Their performance is often contingent on large-scale pre-training, and their architectural components can lack the strict equivariance properties of CNNs. However, a rich ecosystem of research is actively addressing these issues. By integrating convolutional inductive biases, enforcing architectural properties like shift-equivariance, leveraging advanced training paradigms like self-supervised learning, and drawing insights from theoretical analyses, the community is steadily improving the robustness of ViTs. The future of robust vision models likely lies not in a strict dichotomy between CNNs and ViTs, but in a synthesis of their respective strengths, creating architectures that are both expressive and resilient. This exploration of robustness provides a critical foundation for understanding how ViTs perform in real-world scenarios, which is essential as we consider their deployment and efficiency in practical applications.\n\n### 7.2 Interpretability and Attention Visualization\n\nAs Vision Transformers (ViTs) continue to achieve state-of-the-art performance across diverse computer vision tasks, their deployment in high-stakes domains such as healthcare, autonomous driving, and security necessitates a deeper understanding of their internal decision-making processes. Unlike Convolutional Neural Networks (CNNs), which rely on local inductive biases, ViTs process images as sequences of patches and utilize self-attention mechanisms to model global relationships. This architectural shift introduces unique challenges and opportunities for interpretability. While the attention mechanism inherently provides a window into the model's focus, the reliability and semantic meaning of these visualizations remain subjects of intense research. This subsection explores the landscape of ViT interpretability, focusing on attention visualization techniques, feature attribution methods, and the critical assessment of explanation reliability.\n\n**Attention Visualization and the Semantics of Attention Weights**\n\nThe most direct approach to interpreting ViTs involves visualizing the attention weights learned by the self-attention layers. In the standard ViT architecture, the class token ([16]) attends to all other patch tokens, effectively generating a spatial map of importance. A common practice is to aggregate these weights across layers and heads to produce a heatmap, highlighting the image regions that contributed most to the final classification. However, recent studies suggest that raw attention weights may not always correlate strongly with feature importance, leading to the development of refined visualization techniques.\n\nOne significant line of inquiry investigates the distinct roles played by different components of the ViT architecture. By decomposing the final representation into contributions from individual attention heads and MLP blocks, researchers have found that these components capture specialized features such as shape, color, or texture. This decomposition allows for a more granular interpretation beyond a single attention map. For instance, specific attention heads might focus on object boundaries, while others attend to texture or color consistency. The paper [18] introduces a framework to automate this decomposition and map these contributions to a shared text-embedding space. This allows for interpreting the role of specific model components via text descriptions, offering a bridge between visual attention and semantic concepts. By linearly mapping component contributions to CLIP space, the authors demonstrate that one can identify which parts of the model are responsible for recognizing specific attributes, thereby providing a more nuanced understanding than standard attention heatmaps.\n\nFurthermore, the reliability of attention maps for identifying salient regions is a topic of debate. While attention weights indicate the \"soft\" distribution of where a token gathers information from, they do not necessarily explain the \"hard\" influence of a pixel on the final output. To address this, methods like \"Attention Rollout\" and \"Grad-CAM\" adaptations have been proposed to better visualize the flow of information through the transformer layers. However, the paper [63] provides a comparative analysis, noting that ViTs exhibit more uniform representations across layers compared to CNNs. This uniformity, driven by the global nature of self-attention and strong residual connections, implies that interpretability methods must account for the fact that ViTs preserve input spatial information differently than CNNs. The study suggests that the early aggregation of global information in ViTs allows them to maintain spatial localization effectively, which is a crucial factor when relying on attention maps for localization tasks.\n\n**Feature Attribution and Decomposing Contributions**\n\nBeyond visualizing attention maps, feature attribution methods aim to quantify the contribution of specific input patches or tokens to the model's prediction. These methods are essential for understanding *why* a ViT made a specific decision, rather than just *where* it looked. The challenge in ViTs lies in the interplay between the patch embedding, the positional encoding, and the complex non-linear interactions within the transformer blocks.\n\nA prominent example of feature attribution in ViTs is the use of the class token's representation. The final class token is a composite of information processed through multiple transformer layers. Analyzing how this token evolves and which patches contribute most to its final state can be achieved through gradient-based methods or by analyzing the attention flows. The paper [18] effectively automates this process by isolating the contributions of different layers and heads. This allows for a \"dissection\" of the model's reasoning. For example, one can determine if the model is relying on the background or the foreground for a specific prediction, or if it is focusing on spurious correlations. The authors introduce a scoring function to rank components by their importance with respect to specific features, which is a powerful tool for debugging models and mitigating biases.\n\nHowever, interpreting these contributions requires caution. The paper [64] highlights a fundamental issue with standard ViTs: quantization artifacts arising from the fixed-size patch tokenization. These artifacts lead to coarsely quantized features, which can obscure the true contribution of fine-grained details. The authors propose a method to \"super-resolve\" features by ensembling predictions from sub-token spatial translations. This implies that standard attribution methods might be missing information contained in the \"gaps\" between patches. Consequently, interpreting a ViT based solely on standard patch-level attributions might be incomplete. Methods that account for these quantization effects, or that operate on a sub-token level, provide a more faithful representation of the model's internal reasoning.\n\n**Reliability and Critiques of Attention-based Explanations**\n\nWhile attention visualization is intuitive, its reliability as an explanation is frequently questioned. A key concern is whether high attention weights truly imply causality or importance. If a model attends to a region, does removing that region change the output? If not, the attention might be incidental. This distinction is vital for trustworthy AI.\n\nThe paper [63] offers a critical perspective, analyzing the internal representation structures of ViTs and CNNs. It finds that ViTs have more uniform representations across layers, which is a double-edged sword for interpretability. On one hand, it suggests that information is preserved more consistently; on the other, it makes it harder to isolate distinct \"features\" learned at specific depths, as is often done in CNNs. The study emphasizes the role of self-attention in enabling early aggregation of global information, which fundamentally changes how spatial information is processed. This architectural difference means that explanation techniques developed for CNNs (like Grad-CAM) may not directly translate or may require modification to be faithful to the ViT's operation.\n\nFurthermore, the paper [65] implicitly touches upon the reliability of attention. In the context of adversarial robustness, the authors use attention maps and CLS token representations to distinguish between normal and adversarial samples. This application relies on the assumption that adversarial perturbations disrupt the \"natural\" attention patterns of the model. If attention maps were unreliable or easily manipulated without affecting the output, such detection methods would fail. The success of ViTGuard in detecting adversarial examples using attention features suggests that, at least for distinguishing benign from malicious inputs, attention patterns carry meaningful signal. However, this also highlights that attention is sensitive to input perturbations, which can be both a feature (for detection) and a vulnerability (for attacks).\n\n**Emerging Directions and Future Outlook**\n\nThe quest for reliable interpretability in ViTs is driving research towards more robust and granular explanation methods. One promising direction is the integration of textual explanations, as seen in [18]. By mapping internal representations to a shared text space, we can move beyond heatmaps to generate human-readable rationales for model decisions. This is particularly valuable for complex tasks where a simple bounding box is insufficient.\n\nAnother area of exploration involves addressing the inherent limitations of patch-based processing. The quantization artifacts identified in [64] suggest that interpretability methods should not be bound to patch boundaries. Techniques that can infer finer-grained importance or that explicitly model the uncertainty within patches will provide more accurate explanations.\n\nMoreover, the rise of hybrid architectures and efficient ViTs introduces new interpretability challenges. Models that incorporate convolutions or use dynamic token pruning (e.g., [66]) alter the sequence of tokens during inference. Explaining the decisions of such models requires tracking how tokens are merged or pruned and how this dynamic processing affects the final representation. As ViTs are scaled up and adapted for specific hardware constraints, maintaining the ability to audit and understand their decisions remains a critical research frontier.\n\nIn conclusion, interpreting Vision Transformers requires a departure from CNN-centric techniques. While attention visualization offers a starting point, it must be supplemented with methods that decompose contributions, account for architectural specificities like quantization, and map internal states to semantic concepts. The reliability of these explanations is paramount, particularly as ViTs are deployed in safety-critical applications. Future work must focus on developing standardized benchmarks for explanation faithfulness and creating tools that can provide both visual and textual insights into the complex global reasoning of these powerful models. This focus on interpretability is crucial not only for building trust but also for ensuring the security and robustness of these models, a topic explored in the following section.\n\n### 7.3 Integration with Large Language Models and Foundation Models\n\nThe integration of Vision Transformers (ViTs) with Large Language Models (LLMs) represents a pivotal shift in artificial intelligence, moving the field from specialized, task-specific models toward unified, multimodal foundation models. This convergence leverages the robust visual understanding capabilities of ViTs, originally detailed in [19], and couples them with the exceptional reasoning, generation, and zero-shot generalization abilities of LLMs. By bridging the visual and linguistic modalities, researchers aim to create systems that can perceive the world and reason about it using natural language, effectively fulfilling the long-standing quest for artificial general intelligence in specific domains.\n\n**The Architecture of Multimodal Alignment**\n\nThe fundamental challenge in integrating ViTs with LLMs lies in the modality gap: LLMs operate in a discrete textual token space, while ViTs produce continuous visual feature representations. To address this, the community has developed sophisticated alignment strategies that map visual features into the semantic space of the LLM. A common approach involves utilizing a pre-trained ViT as a visual encoder to extract image features, which are then projected into the LLM\u2019s input space via a trainable adapter or projection layer. This architecture allows the LLM to \"see\" the image by treating visual embeddings as soft prompts or pseudo-tokens.\n\nWhile the original Vision Transformer [19] provided a strong baseline for visual encoding, subsequent research has focused on optimizing these encoders for multimodal contexts. For instance, the development of efficient ViT variants like [22] is crucial for deployment in resource-constrained environments where multimodal models must run locally. Furthermore, the internal representations of ViTs have been analyzed to understand how they can be better utilized for language grounding. The concept of [67] suggests that visual information can be reused and cached, a property highly beneficial when the visual stream needs to be aligned with the sequential generation of text by an LLM.\n\n**Impact on Reasoning and Generalization**\n\nThe synergy between ViTs and LLMs significantly enhances reasoning capabilities compared to standalone visual models. Traditional ViTs excel at classification and detection but struggle with complex, compositional reasoning (e.g., \"Is there a red object to the left of the dog that looks like it is about to catch a frisbee?\"). By integrating with LLMs, the visual features extracted by the ViT are interpreted through the lens of linguistic priors and world knowledge stored in the LLM.\n\nThis integration has led to the rise of \"emergent\" behaviors where models perform tasks they were not explicitly trained for. For example, by leveraging the strong generalization of LLMs, multimodal systems can perform Visual Question Answering (VQA) and complex image captioning with high fidelity. The training paradigms used to develop these systems often borrow from self-supervised learning techniques applied to ViTs, such as those described in [68]. The DINO method, for instance, demonstrated that self-supervised ViTs learn semantic segmentation properties implicitly, which provides a rich, structured representation that is highly valuable for an LLM to parse and reason about.\n\nMoreover, the distillation techniques that made ViTs data-efficient [19] are being adapted to compress these massive multimodal models. As these foundation models grow, the need to distill knowledge from large teacher multimodal models into smaller student models becomes critical. The principles of [25] and [69] are being extended to the multimodal domain, ensuring that the reasoning capabilities of the LLM-ViT fusion can be deployed on edge devices without sacrificing performance.\n\n**Training Paradigms for Multimodal Fusion**\n\nTraining these integrated models requires large-scale datasets containing image-text pairs. The training process typically involves two stages: pre-training to align the modalities and instruction tuning to enable zero-shot generalization. During pre-training, the goal is to minimize the distance between visual and textual representations. The ViT processes the image, and the LLM processes the text; the alignment loss ensures that the visual representation of a \"cat\" is close to the text token \"cat\" in the embedding space.\n\nRecent advancements have explored \"generative\" training objectives, similar to Masked Image Modeling (MIM) [25], where the model learns to reconstruct missing visual or textual information. This encourages the model to learn a deeper understanding of the relationship between visual structures and linguistic concepts. The efficiency of these training processes is paramount. The insights from [26] regarding data augmentation and regularization are directly applicable here, as multimodal datasets are often noisy and require robust training strategies to prevent overfitting.\n\nFurthermore, the concept of \"data-free\" or \"dataset distillation\" [70] could play a future role in creating compact, high-quality training sets for multimodal alignment, reducing the computational burden of training on billions of image-text pairs.\n\n**Efficiency and Deployment Challenges**\n\nDespite their impressive capabilities, LLM-ViT integrations face severe efficiency challenges. The quadratic complexity of the self-attention mechanism in both ViTs and LLMs makes the combined model computationally prohibitive. To mitigate this, researchers are applying efficiency techniques specifically designed for ViTs to the multimodal context.\n\nFor instance, token reduction strategies are critical. Methods like [66] and [71] allow the visual encoder to discard redundant background tokens before passing the remaining tokens to the LLM. This reduces the sequence length that the LLM must process, significantly lowering latency. Similarly, [72] offers a way to dynamically adjust computational cost based on input complexity, which is ideal for real-time multimodal applications.\n\nQuantization and compression are also essential. The principles of [73] and [74] are being explored to reduce the memory footprint of the visual encoder. However, applying aggressive quantization to the integrated system is non-trivial, as the alignment between the quantized visual features and the (often full-precision) LLM weights must be maintained to preserve reasoning capabilities.\n\n**Future Directions: Towards Unified Foundation Models**\n\nThe trajectory of integrating ViTs with LLMs points toward unified foundation models that seamlessly handle vision and language. The distinction between \"vision tasks\" and \"language tasks\" is blurring. We are moving toward models where the ViT is not just a feature extractor but an integral component of the reasoning engine.\n\nOne promising direction is the exploration of State Space Models (SSM) and hybrid architectures, which may offer linear complexity alternatives to the quadratic attention in both ViTs and LLMs, making the integration more scalable. Additionally, the robustness and interpretability of these models remain open challenges. As these models are deployed in high-stakes domains, understanding *why* a model generated a specific text description based on visual input is critical.\n\nIn conclusion, the integration of Vision Transformers with Large Language Models is the defining trend in modern AI research. By building upon the efficient and data-efficient architectures of ViTs [75] and applying rigorous efficiency techniques [66], researchers are unlocking unprecedented reasoning and generalization capabilities. This convergence is not merely about connecting two modalities; it is about creating a unified cognitive architecture that sees, understands, and speaks, paving the way for the next generation of artificial intelligence.\n\n### 7.4 State Space Models and Hybrid Architectures\n\nThe relentless pursuit of efficiency and representational power in computer vision has led to a critical re-evaluation of the architectural foundations inherited from natural language processing. While Vision Transformers (ViTs) have demonstrated remarkable success, their core mechanism, self-attention, suffers from quadratic computational complexity with respect to sequence length. This fundamental limitation poses significant challenges for processing high-resolution images and deploying models on resource-constrained devices, a concern that resonates with the broader efficiency challenges discussed in the context of multimodal LLM-ViT integrations. The community's response has been multifaceted, evolving from architectural tweaks to hybrid designs and, most recently, to the exploration of State Space Models (SSMs) as a potential paradigm shift. This subsection investigates the rise of SSMs, such as Mamba, as compelling alternatives or complements to the Transformer architecture, and discusses hybrid architectures that aim to synthesize the linear efficiency of SSMs with the representational prowess of attention-based models.\n\n### The Quadratic Bottleneck and the Search for Linear Alternatives\n\nThe quadratic scaling of standard self-attention, where computational cost grows as O(N\u00b2) for a sequence of length N, is the primary bottleneck for ViTs. This complexity makes it prohibitively expensive to apply ViTs to large images or long video sequences without resorting to patching, windowing, or other approximations that can compromise global context. To address this, researchers have explored various \"linear attention\" mechanisms, but many of these approaches either introduce significant approximation errors or lack the hardware-aware optimizations that have made Transformers so dominant.\n\nThis context sets the stage for the emergence of State Space Models (SSMs), particularly those inspired by control theory and structured state space sequences (S4). The recent Mamba architecture has garnered significant attention for its ability to model long-range dependencies with linear scaling (O(N)). Unlike Transformers, which compute interactions between all pairs of tokens, Mamba processes sequences by evolving a hidden state through a recurrence mechanism that is highly parallelizable during training. This design elegantly sidesteps the quadratic bottleneck, offering a path towards building vision models that can handle high-resolution inputs more natively and efficiently.\n\n### Mamba for Vision: A New Paradigm\n\nThe adaptation of Mamba to vision tasks, often termed Vision Mamba (ViM), involves rethinking how images are processed. Instead of treating an image as a flat sequence of patches, ViM models typically apply bidirectional SSMs to capture spatial context along two dimensions. This approach has shown promise in tasks like image classification and dense prediction, often outperforming ViTs of similar model sizes, especially on high-resolution inputs where the quadratic cost of attention becomes a major constraint.\n\nThe core advantage of Mamba-like models lies in their ability to maintain a global receptive field without the computational overhead of attention. The SSM mechanism allows information to propagate across the entire sequence, but in a recurrent manner, which is fundamentally more efficient. This property is particularly valuable for vision tasks where long-range dependencies are crucial, such as in medical imaging or satellite imagery analysis. The rise of these models suggests a potential shift in the design space, where the trade-off is no longer between local (CNN) and global (ViT) context, but between different forms of efficient global context modeling.\n\n### Hybrid Architectures: Combining SSMs and Transformers\n\nWhile pure SSMs offer compelling efficiency benefits, they may not always capture the fine-grained, dynamic interactions that self-attention excels at. This has led to the exploration of hybrid architectures that combine the strengths of both worlds. The goal is to create a model that leverages the linear scaling and long-sequence modeling capabilities of SSMs while retaining the powerful representational capacity of attention.\n\nOne line of research involves integrating SSM blocks as a replacement for or an alternative to the feed-forward network (FFN) in a Transformer block. For instance, a hybrid block might use self-attention to model complex token-to-token relationships and an SSM to efficiently process the sequence and capture global context. This allows the model to allocate its computational budget strategically: attention for high-fidelity interaction and SSMs for broad context aggregation. Such designs are particularly promising for hierarchical vision backbones, where different stages of the network can be assigned different architectural priors. For example, earlier stages dealing with high-resolution feature maps could benefit from the efficiency of SSMs, while later stages could use attention for fine-grained semantic reasoning.\n\nAnother hybrid approach involves using convolutions or other local operators in conjunction with SSMs. This is motivated by the observation that while SSMs are good at global modeling, they might lack the strong local inductive bias that is inherent to images. By combining local convolutions with global SSMs, similar to how some hybrid CNN-Transformer models work, these models can learn both local textures and global structures effectively. The paper [76] provides an interesting perspective, suggesting that the principles of local modeling found in CNNs can be used to understand and potentially improve the performance of Transformer-like models, a line of reasoning that is directly applicable to designing SSM-based vision models.\n\n### Efficiency, Deployment, and Hardware Considerations\n\nThe practical viability of any new architecture depends heavily on its efficiency and ease of deployment. SSMs, with their recurrent nature, present a different computational profile than Transformers. While training is highly parallelizable, inference can be naturally implemented in a recurrent fashion, which is very memory-efficient for long sequences. This is a significant advantage over Transformers, whose memory requirements during inference scale with the sequence length.\n\nHowever, realizing the full potential of SSMs in real-world applications requires co-designing algorithms and hardware. The community has been actively working on compiler optimizations and hardware-aware designs for efficient deployment. For SSMs, this involves optimizing the parallel scan algorithms used during training and developing efficient kernels for the recurrent inference mode. The goal is to achieve low latency and high throughput on diverse hardware, from cloud GPUs to mobile edge devices. The efficiency gains promised by SSMs could make high-performance vision models accessible in scenarios where ViTs were previously infeasible, such as real-time video analysis on mobile phones or large-scale processing of scientific imagery.\n\n### Future Directions and Open Challenges\n\nThe integration of State Space Models into the vision ecosystem is still in its early stages, and several open challenges and exciting future directions remain.\n\n1.  **Understanding Representational Differences:** A deeper theoretical understanding is needed to compare the features learned by SSMs versus those learned by ViTs and CNNs. How do the inductive biases of SSMs differ from those of attention or convolution? Analyzing the effective receptive fields and attention maps (or their SSM equivalents) of these hybrid models could provide valuable insights.\n\n2.  **Scalability and Scaling Laws:** While SSMs have shown promise at smaller scales, their scalability to billions of parameters, a key factor in the success of LLMs and large ViTs, is an active area of research. Establishing clear scaling laws for vision-based SSMs and hybrid architectures will be crucial for determining their ultimate potential.\n\n3.  **Self-Supervised Learning (SSL) with SSMs:** The dominant paradigms for training ViTs, such as masked image modeling (MAE) and contrastive learning (DINO), have been instrumental to their success. Adapting and developing SSL strategies that are tailored to the unique properties of SSMs is essential. For instance, how should one design a masking strategy for a recurrent architecture?\n\n4.  **Integration with Generative Models:** The rise of diffusion models for image generation has heavily relied on U-Net architectures, with recent work exploring Transformer backbones. SSMs, with their efficiency in handling long sequences, could be a game-changer for high-resolution generative modeling, potentially replacing or augmenting attention blocks in diffusion U-Nets.\n\n5.  **Multimodal Applications:** As vision models increasingly merge with language models to form unified multimodal foundation models, the efficiency of the visual encoder becomes paramount. An efficient SSM-based vision encoder could reduce the computational burden in large vision-language models, enabling more complex reasoning and interaction capabilities.\n\nIn conclusion, the emergence of State Space Models like Mamba represents a significant and promising development in the evolution of vision architectures. By offering a path to linear complexity and efficient long-range modeling, SSMs address a fundamental limitation of the standard Transformer. The ongoing exploration of hybrid architectures that combine SSMs with attention, convolution, or other mechanisms is likely to yield models that are not only more accurate but also significantly more efficient and practical. This architectural diversification, moving beyond the pure Transformer paradigm, marks a new and exciting chapter in the quest for powerful and scalable vision models.\n\n### 7.5 Safety, Alignment, and Ethical Considerations\n\nThe rapid proliferation of Vision Transformers (ViTs) across diverse computer vision domains has unlocked unprecedented capabilities, ranging from high-fidelity image generation to critical decision-making in autonomous systems and healthcare. However, this widespread adoption necessitates a rigorous examination of the safety, alignment, and ethical implications inherent in these powerful models. As ViTs evolve from research curiosities into foundational components of real-world applications, ensuring their robustness against misuse, alignment with human values, and adherence to ethical standards becomes paramount. This subsection explores the multifaceted challenges associated with deploying ViTs, focusing on the risks of generating harmful content, the ethical dilemmas in high-stakes environments, and the emerging strategies for model alignment and safety.\n\n**Safety Risks: Adversarial Vulnerability and Generative Hazards**\n\nSafety in ViTs encompasses two primary dimensions: robustness against malicious manipulation and the potential for generating harmful outputs. ViTs, despite their global receptive fields, exhibit significant vulnerabilities to adversarial attacks. Unlike Convolutional Neural Networks (CNNs), which possess inherent translation equivariance, ViTs rely heavily on self-attention mechanisms that can be disrupted by subtle, imperceptible perturbations. Research indicates that ViTs are susceptible to both white-box and black-box adversarial examples, where small perturbations can cause misclassification in tasks like image classification [77]. The quadratic complexity of self-attention also introduces unique attack surfaces, such as attention hijacking, where an attacker can manipulate the attention maps to focus on irrelevant background features. Furthermore, the lack of strong inductive biases in standard ViTs often requires large-scale pre-training, making the model susceptible to \"backdoor\" attacks embedded in the training data. In safety-critical applications like autonomous driving or medical diagnosis, such vulnerabilities could lead to catastrophic failures. For instance, in the domain of medical imaging, where ViTs are increasingly used for segmentation and classification [78], an adversarial attack could alter the segmentation of a tumor, leading to incorrect treatment plans. Addressing these vulnerabilities requires not only adversarial training but also architectural innovations that enhance intrinsic robustness, such as the hybrid designs found in models like [79], which blend local and global attention to stabilize feature extraction.\n\nBeyond adversarial robustness, the generative capabilities of ViTs pose significant safety risks. While ViTs are primarily used for discriminative tasks, their underlying architecture is increasingly adapted for generative purposes, often integrated with Large Language Models (LLMs) to create multimodal systems. These systems, capable of generating images or text-to-image outputs, can be misaligned to produce harmful, biased, or explicit content. The \"alignment problem\"\u2014ensuring that model outputs match human intent\u2014is exacerbated in ViTs due to their data-driven nature. If the training data (e.g., internet-scale image datasets) contains stereotypes or toxic content, the model may replicate or amplify these biases. For example, in multimodal reasoning tasks like Visual Question Answering (VQA), ViTs may generate answers that reflect societal biases regarding gender or race, particularly if the model lacks sufficient safety guardrails. The integration of ViTs with LLMs creates a compounded risk where the generative power of LLMs is visualized, potentially leading to the creation of deepfakes or misinformation. Therefore, developing \"safety-aware\" training paradigms, such as those utilizing self-supervised learning with safety constraints, is crucial for mitigating these generative hazards.\n\n**Ethical Considerations in High-Stakes Domains**\n\nThe deployment of ViTs in high-stakes domains introduces profound ethical challenges, particularly regarding fairness, accountability, and transparency. In specialized fields such as medical imaging and autonomous driving, ViTs are relied upon for critical decision-making. For instance, in computational pathology, Hierarchical Vision Transformers (H-ViTs) are used for prostate cancer grading in Whole Slide Images (WSIs) [80]. While these models offer high accuracy, they raise ethical concerns about algorithmic bias. If the training data is skewed towards specific demographics, the model may underperform for underrepresented groups, leading to health disparities. The opacity of ViTs further complicates accountability; when a model makes a diagnostic error, the \"black-box\" nature of self-attention makes it difficult to trace the decision-making process, posing challenges for medical liability.\n\nSimilarly, in autonomous driving, end-to-end ViT-based systems are being explored to process complex visual inputs for navigation. The ethical implications here are stark: a failure in perception or alignment could result in loss of life. The challenge of \"distribution shifts\"\u2014where the model encounters scenarios outside its training distribution\u2014is a major safety concern. ViTs, while powerful, can be brittle to such shifts, necessitating rigorous safety testing and fail-safe mechanisms. Furthermore, the push for efficiency in ViTs for edge deployment raises ethical questions about the trade-off between performance and safety. Compressing models via pruning or quantization can inadvertently remove safety-critical features, making the model more prone to errors in real-world conditions.\n\n**Alignment Strategies and Mitigation Techniques**\n\nTo address these safety and ethical challenges, the research community is exploring various alignment and mitigation strategies. One promising direction is the use of self-supervised learning (SSL) to learn more robust and semantically meaningful representations that are less prone to overfitting to spurious correlations. Frameworks like [81] and [82] allow models to learn from vast amounts of unlabeled data, potentially capturing a more holistic understanding of the visual world, which can serve as a foundation for safer alignment. However, SSL alone is insufficient; it must be coupled with explicit safety objectives.\n\nIn the context of high-level vision tasks like object detection and segmentation, models like [83] simplify the pipeline but must be rigorously tested for edge cases. The concept of \"interpretability\" is intrinsically linked to safety. By understanding *why* a ViT makes a specific prediction, developers can identify and rectify alignment failures. Techniques for visualizing attention maps [84] are essential for debugging, but they must be interpreted with caution, as attention weights do not always correlate with feature importance.\n\nFurthermore, the architectural design of ViTs plays a role in alignment. Hierarchical architectures like Swin Transformer [85] and PVT [30] introduce multi-scale feature processing, which can improve the model's ability to handle objects at various scales and contexts, potentially reducing errors in complex scenes. However, these complex architectures also increase the difficulty of verifying their safety properties. As we move towards \"Foundation Models\" that integrate ViTs with LLMs, the alignment challenge shifts from single-modality safety to cross-modal consistency. Ensuring that a visual encoder does not feed misleading information to a language model is a critical area of research.\n\n**Future Directions: Governance and Ethical Frameworks**\n\nLooking ahead, addressing the safety and ethical considerations of ViTs requires a multi-pronged approach involving technical innovation, regulatory oversight, and ethical governance. Technically, we need to develop \"Safety-by-Design\" ViT architectures that incorporate robustness and alignment as core components, rather than post-hoc fixes. This includes exploring hybrid architectures [86] that leverage the inductive biases of CNNs to stabilize ViTs, or novel attention mechanisms [87] that are inherently more robust to noise.\n\nFrom a governance perspective, the deployment of ViTs in high-stakes domains must be subject to strict auditing and certification processes. This is particularly relevant for specialized applications like medical imaging [78] and autonomous driving, where the cost of failure is high. The community must also address the environmental and social costs of training massive ViT models, ensuring that the benefits of these technologies are distributed equitably.\n\nFinally, the integration of ViTs with LLMs necessitates a re-evaluation of \"alignment.\" It is no longer sufficient to align a model to a specific task; we must align it to broad human values and societal norms. This involves developing better benchmarks for evaluating safety and alignment in multimodal systems and fostering interdisciplinary collaboration between computer scientists, ethicists, and domain experts. As ViTs continue to evolve, the focus must shift from pure performance metrics to a holistic view of reliability, safety, and ethical responsibility. The insights from [88] suggest that the macro-architecture is crucial for performance; similarly, the macro-structure of our safety and ethical frameworks will determine whether ViTs become a net positive for society.\n\n\n## References\n\n[1] Recent Advances in Vision Transformer  A Survey and Outlook of Recent  Work\n\n[2] Vision Transformers provably learn spatial structure\n\n[3] Understanding Why ViT Trains Badly on Small Datasets  An Intuitive  Perspective\n\n[4] ConViT  Improving Vision Transformers with Soft Convolutional Inductive  Biases\n\n[5] ViTAE  Vision Transformer Advanced by Exploring Intrinsic Inductive Bias\n\n[6] Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets\n\n[7] LocalViT  Bringing Locality to Vision Transformers\n\n[8] RegionViT  Regional-to-Local Attention for Vision Transformers\n\n[9] Making Vision Transformers Truly Shift-Equivariant\n\n[10] Reviving Shift Equivariance in Vision Transformers\n\n[11] On the Bias Against Inductive Biases\n\n[12] Bridging the Gap Between Vision Transformers and Convolutional Neural  Networks on Small Datasets\n\n[13] Bootstrapping ViTs  Towards Liberating Vision Transformers from  Pre-training\n\n[14] Convolutional Embedding Makes Hierarchical Vision Transformer Stronger\n\n[15] Vision Transformer  Vit and its Derivatives\n\n[16] Research Re  search & Re-search\n\n[17] How to train your ViT for OOD Detection\n\n[18] Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP\n\n[19] Training data-efficient image transformers & distillation through  attention\n\n[20] DeiT-LT Distillation Strikes Back for Vision Transformer Training on  Long-Tailed Datasets\n\n[21] ViTKD  Practical Guidelines for ViT feature knowledge distillation\n\n[22] TinyViT  Fast Pretraining Distillation for Small Vision Transformers\n\n[23] Optimizing Vision Transformers with Data-Free Knowledge Transfer\n\n[24] Distilling Efficient Vision Transformers from CNNs for Semantic  Segmentation\n\n[25] TinyMIM  An Empirical Study of Distilling MIM Pre-trained Models\n\n[26] How to train your ViT  Data, Augmentation, and Regularization in Vision  Transformers\n\n[27] CvT  Introducing Convolutions to Vision Transformers\n\n[28] CoAtNet  Marrying Convolution and Attention for All Data Sizes\n\n[29] MobileViT  Light-weight, General-purpose, and Mobile-friendly Vision  Transformer\n\n[30] PVT v2  Improved Baselines with Pyramid Vision Transformer\n\n[31] Swin Transformer  Hierarchical Vision Transformer using Shifted Windows\n\n[32] Multiscale Vision Transformers\n\n[33] MViTv2  Improved Multiscale Vision Transformers for Classification and  Detection\n\n[34] Beyond Fixation  Dynamic Window Visual Transformer\n\n[35] Factorization Vision Transformer  Modeling Long Range Dependency with  Local Window Cost\n\n[36] Brain-Inspired Stepwise Patch Merging for Vision Transformers\n\n[37] Swin-Free  Achieving Better Cross-Window Attention and Efficiency with  Size-varying Window\n\n[38] FasterViT  Fast Vision Transformers with Hierarchical Attention\n\n[39] EdgeViTs  Competing Light-weight CNNs on Mobile Devices with Vision  Transformers\n\n[40] Navigating Efficiency in MobileViT through Gaussian Process on Global Architecture Factors\n\n[41] EfficientFormer  Vision Transformers at MobileNet Speed\n\n[42] RepViT  Revisiting Mobile CNN From ViT Perspective\n\n[43] ViT-1.58b: Mobile Vision Transformers in the 1-bit Era\n\n[44] PIVOT- Input-aware Path Selection for Energy-efficient ViT Inference\n\n[45] PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference\n\n[46] TRT-ViT  TensorRT-oriented Vision Transformer\n\n[47] ElasticViT  Conflict-aware Supernet Training for Deploying Fast Vision  Transformer on Diverse Mobile Devices\n\n[48] An Image is Worth 16x16 Words  Transformers for Image Recognition at  Scale\n\n[49] Tokens-to-Token ViT  Training Vision Transformers from Scratch on  ImageNet\n\n[50] Lite Vision Transformer with Enhanced Self-Attention\n\n[51] LightViT  Towards Light-Weight Convolution-Free Vision Transformers\n\n[52] X-ViT  High Performance Linear Vision Transformer without Softmax\n\n[53] UFO-ViT  High Performance Linear Vision Transformer without Softmax\n\n[54] SOFT  Softmax-free Transformer with Linear Complexity\n\n[55] You Only Need Less Attention at Each Stage in Vision Transformers\n\n[56] When Shift Operation Meets Vision Transformer  An Extremely Simple  Alternative to Attention Mechanism\n\n[57] BViT  Broad Attention based Vision Transformer\n\n[58] Delving Deep into the Generalization of Vision Transformers under  Distribution Shifts\n\n[59] Equivariant neural networks and equivarification\n\n[60] The Lie Derivative for Measuring Learned Equivariance\n\n[61] Teaching Matters  Investigating the Role of Supervision in Vision  Transformers\n\n[62] Unraveling Attention via Convex Duality  Analysis and Interpretations of  Vision Transformers\n\n[63] Do Vision Transformers See Like Convolutional Neural Networks \n\n[64] Sub-token ViT Embedding via Stochastic Resonance Transformers\n\n[65] ViTGuard: Attention-aware Detection against Adversarial Examples for Vision Transformer\n\n[66] Token Merging  Your ViT But Faster\n\n[67] Token Recycling for Efficient Sequential Inference with Vision  Transformers\n\n[68] Emerging Properties in Self-Supervised Vision Transformers\n\n[69] Learning Efficient Vision Transformers via Fine-Grained Manifold  Distillation\n\n[70] Dataset Distillation  A Comprehensive Review\n\n[71] Not All Patches are What You Need  Expediting Vision Transformers via  Token Reorganizations\n\n[72] Adaptive Sparse ViT  Towards Learnable Adaptive Token Pruning by Fully  Exploiting Self-Attention\n\n[73] Bi-ViT  Pushing the Limit of Vision Transformer Quantization\n\n[74] BinaryViT  Pushing Binary Vision Transformers Towards Convolutional  Models\n\n[75] DeiT III  Revenge of the ViT\n\n[76] Toward a Deeper Understanding  RetNet Viewed through Convolution\n\n[77] A Comprehensive Survey of Transformers for Computer Vision\n\n[78] A Recent Survey of Vision Transformers for Medical Image Segmentation\n\n[79] ACC-ViT   Atrous Convolution's Comeback in Vision Transformers\n\n[80] Hierarchical Vision Transformers for Context-Aware Prostate Cancer  Grading in Whole Slide Images\n\n[81] R-MAE  Regions Meet Masked Autoencoders\n\n[82] SINDER: Repairing the Singular Defects of DINOv2\n\n[83] Efficient Transformer Encoders for Mask2Former-style models\n\n[84] Transformer Interpretability Beyond Attention Visualization\n\n[85] Video Swin Transformer\n\n[86] Jamba-1.5: Hybrid Transformer-Mamba Models at Scale\n\n[87] A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size  Representations\n\n[88] What Makes for Hierarchical Vision Transformer \n\n\n",
    "reference": {
        "1": "2203.01536v5",
        "2": "2210.09221v1",
        "3": "2302.03751v1",
        "4": "2103.10697v2",
        "5": "2106.03348v4",
        "6": "2407.19394v3",
        "7": "2104.05707v1",
        "8": "2106.02689v3",
        "9": "2305.16316v2",
        "10": "2306.07470v1",
        "11": "2105.14077v1",
        "12": "2210.05958v2",
        "13": "2112.03552v4",
        "14": "2207.13317v2",
        "15": "2205.11239v2",
        "16": "2403.13705v1",
        "17": "2405.17447v1",
        "18": "2406.01583v1",
        "19": "2012.12877v2",
        "20": "2404.02900v1",
        "21": "2209.02432v1",
        "22": "2207.10666v1",
        "23": "2408.05952v1",
        "24": "2310.07265v1",
        "25": "2301.01296v1",
        "26": "2106.10270v2",
        "27": "2103.15808v1",
        "28": "2106.04803v2",
        "29": "2110.02178v2",
        "30": "2106.13797v7",
        "31": "2103.14030v2",
        "32": "2104.11227v1",
        "33": "2112.01526v2",
        "34": "2203.12856v2",
        "35": "2312.08614v1",
        "36": "2409.06963v1",
        "37": "2306.13776v1",
        "38": "2306.06189v2",
        "39": "2205.03436v2",
        "40": "2406.04820v1",
        "41": "2206.01191v5",
        "42": "2307.09283v8",
        "43": "2406.18051v1",
        "44": "2404.15185v1",
        "45": "2407.05010v1",
        "46": "2205.09579v3",
        "47": "2303.09730v2",
        "48": "2010.11929v2",
        "49": "2101.11986v3",
        "50": "2112.10809v1",
        "51": "2207.05557v1",
        "52": "2205.13805v1",
        "53": "2109.14382v2",
        "54": "2110.11945v3",
        "55": "2406.00427v1",
        "56": "2201.10801v1",
        "57": "2202.06268v2",
        "58": "2106.07617v4",
        "59": "1906.07172v4",
        "60": "2210.02984v1",
        "61": "2212.03862v2",
        "62": "2205.08078v2",
        "63": "2108.08810v2",
        "64": "2310.03967v1",
        "65": "2409.13828v1",
        "66": "2210.09461v3",
        "67": "2311.15335v1",
        "68": "2104.14294v2",
        "69": "2107.01378v4",
        "70": "2301.07014v3",
        "71": "2202.07800v2",
        "72": "2209.13802v2",
        "73": "2305.12354v1",
        "74": "2306.16678v1",
        "75": "2204.07118v1",
        "76": "2309.05375v2",
        "77": "2211.06004v1",
        "78": "2312.00634v2",
        "79": "2403.04200v1",
        "80": "2312.12619v1",
        "81": "2306.05411v2",
        "82": "2407.16826v1",
        "83": "2404.15244v1",
        "84": "2012.09838v2",
        "85": "2106.13230v1",
        "86": "2408.12570v1",
        "87": "1609.05866v1",
        "88": "2107.02174v2"
    }
}