{
    "survey": "# A Comprehensive Survey of 3D Gaussian Splatting: Foundations, Evolutions, and Future Directions\n\n## 1 Introduction and Background\n\n### 1.1 The Rise of Neural Radiance Fields and the Need for Explicit Alternatives\n\nThe landscape of computer graphics and 3D computer vision has undergone a seismic shift in recent years, largely driven by the advent of deep learning techniques that aim to solve the long-standing problem of novel view synthesis. For decades, generating photorealistic images of a scene from unseen viewpoints required explicit 3D representations such as polygonal meshes, point clouds, or volumetric grids, coupled with traditional rendering pipelines like ray tracing or rasterization. While effective, these methods relied heavily on meticulously crafted geometric models and complex material definitions, making the capture of real-world scenes a labor-intensive process. The introduction of Neural Radiance Fields (NeRFs) marked a paradigm shift, demonstrating that a continuous volumetric scene representation, parameterized by a simple Multi-Layer Perceptron (MLP), could achieve unprecedented fidelity in synthesizing novel views from a sparse set of input images [1]. This breakthrough ignited a massive wave of research, with over a thousand preprints published in a short span, as noted in comprehensive surveys [2; 3].\n\nAt the core of the original NeRF formulation is the concept of representing a scene as a continuous function that maps a 5D coordinate\u2014comprising 3D spatial location $(x,y,z)$ and 2D viewing direction $(\\theta, \\phi)$\u2014to a color and volume density at that point. This representation is rendered using classic volume rendering techniques, where colors and densities are integrated along camera rays. Because volume rendering is differentiable, the entire pipeline can be optimized using only photometric reconstruction loss (i.e., comparing rendered pixels to ground truth images) without requiring explicit 3D supervision. This ability to learn 3D scenes from 2D images alone is the primary driver behind NeRF's popularity. However, the very nature of this implicit, continuous representation introduces significant limitations, particularly concerning computational efficiency and editability.\n\nThe most glaring limitation of implicit representations like NeRF is the computational cost associated with rendering. To render a single pixel, the original NeRF requires querying the MLP network hundreds of times\u2014once for each sample point along the camera ray. This volumetric ray marching process is inherently sequential and computationally expensive, often taking minutes to render a single low-resolution image and significantly longer for high-resolution outputs. This bottleneck has been a major focus of subsequent research. For instance, methods like [4] attempted to this by adaptively using larger networks only for complex regions, while [5] proposed using multi-input multi-output MLPs to reduce the number of network passes. Despite these improvements, the fundamental requirement of dense sampling along rays persists. Even approaches that attempt to reduce sampling, such as [6], which models radiance distributions using frequency bases, still struggle to achieve true real-time performance on standard hardware without baking the representation into explicit data structures. The work [7] explicitly highlights this trade-off, proposing to \"bake\" the NeRF into a sparse voxel grid (SNeRG) to enable real-time rendering, effectively acknowledging that the implicit representation itself is too slow for interactive applications. This reliance on heavy network inference for every pixel makes NeRFs impractical for resource-constrained environments like mobile devices [8] or for applications requiring immediate feedback, such as immersive VR [9].\n\nBeyond rendering speed, implicit representations pose significant challenges for scene manipulation and editing. Because NeRF encodes the scene geometry and appearance within the weights of a neural network, it is extremely difficult to perform localized edits, such as moving an object, changing its texture, or removing it entirely. The continuous nature of the field means that altering one part of the network often has unpredictable global effects, and there is no inherent segmentation or structural understanding. While some methods have attempted to address this by introducing explicit control mechanisms, such as using mesh proxies to guide ray bending for deformation [10], these approaches often require complex pipelines or additional user inputs. The lack of explicit geometry also hinders the integration of NeRFs into standard graphics workflows, which rely heavily on mesh-based representations for physics simulation, collision detection, and traditional rendering pipelines. Converting a NeRF to a mesh is non-trivial and often results in artifacts, as discussed in [11]. Furthermore, editing the appearance of a NeRF, such as relighting, requires disentangling lighting from geometry and material properties\u2014a task that is inherently ambiguous in the standard NeRF formulation. While specialized methods like [12] have introduced structured representations to better handle specularities, they do not inherently solve the broader problem of flexible, user-friendly editing.\n\nThe difficulty in manipulating implicit representations stems from their \"black box\" nature. Unlike a mesh, where vertices and faces can be directly manipulated, or a point cloud where points can be moved, a NeRF offers no intuitive handles for interaction. This opacity limits its utility in creative industries where iterative editing is standard. For example, in [13], the authors note that while NeRFs excel at reconstruction, providing tools for editing is crucial for practical adoption. Their method requires a single reference inpainted view and complex back-projection to achieve consistency, highlighting the lack of native editing capabilities. Similarly, [14] proposes a system for pixel-level editing but requires a two-stage training strategy and a proxy function to map user edits back to the NeRF space, underscoring the fundamental mismatch between the user's desire for direct manipulation and the NeRF's implicit structure.\n\nFurthermore, the implicit nature of NeRFs makes it difficult to reason about the scene's structure. Tasks like segmentation, object removal, or understanding occlusions require additional networks or priors. In contrast, explicit representations like 3D Gaussian Splatting (3DGS) naturally provide a set of discrete primitives (the Gaussians) that can be individually selected, grouped, and manipulated. The rise of 3DGS can be seen as a direct response to these limitations of implicit methods. By representing the scene as a collection of anisotropic 3D Gaussians, 3DGS maintains the ability to render high-fidelity novel views while offering an explicit, discrete representation that is amenable to efficient rendering and direct editing. The rendering pipeline of 3DGS utilizes tile-based rasterization, which projects these Gaussians into 2D and sorts them for efficient alpha-blending, bypassing the expensive volumetric ray marching of NeRF. This architectural shift enables real-time rendering speeds, a critical requirement for interactive applications that implicit methods have struggled to meet.\n\nIn summary, while Neural Radiance Fields revolutionized novel view synthesis by demonstrating the power of continuous implicit representations learned from images, they are hampered by high computational costs during rendering and a lack of editability. The rendering process, based on volumetric ray marching, is too slow for real-time applications without baking, and the continuous, network-encoded nature of the scene makes intuitive manipulation and integration into standard graphics pipelines challenging. These limitations have created a strong demand for explicit alternatives that can retain the photorealism of NeRF while offering the speed and flexibility required for practical deployment. This need sets the stage for the emergence of 3D Gaussian Splatting, which explicitly addresses these shortcomings by leveraging a discrete, explicit representation combined with a highly efficient rendering algorithm.\n\n### 1.2 Introduction to 3D Gaussian Splatting (3DGS) Paradigm\n\nThe emergence of 3D Gaussian Splatting (3DGS) marks a pivotal shift in the landscape of neural rendering and novel view synthesis. While Neural Radiance Fields (NeRFs) established the dominance of implicit, continuous volumetric representations, 3DGS reintroduces the power of explicit, discrete primitives to the forefront of computer graphics. It effectively bridges the gap between the high fidelity of volumetric methods and the computational efficiency required for real-time applications. At its core, 3DGS represents a scene not as a continuous field queried by a neural network, but as a collection of millions of anisotropic 3D Gaussians, each defined by specific attributes such as position, covariance, opacity, and color. This explicit representation allows for a rendering pipeline based on rasterization rather than volumetric ray marching, resulting in a dramatic acceleration of rendering speeds while maintaining, and often surpassing, the visual quality of state-of-the-art implicit methods [15].\n\nThe fundamental concept of 3DGS is rooted in the representation of scene geometry and appearance through a set of 3D Gaussian distributions. Unlike point clouds which represent geometry as unstructured points, each Gaussian primitive in 3DGS is a continuous probability distribution defined by a mean vector $\\boldsymbol{\\mu}$ and a full 3D covariance matrix $\\Sigma$. This covariance matrix is crucial as it endows each primitive with an anisotropic shape (ellipsoidal volume), allowing it to effectively model surfaces and volumes with varying orientations and scales. To ensure the covariance matrix remains positive semi-definite, it is decomposed into a scaling vector $\\mathbf{s}$ and a rotation quaternion $\\mathbf{q}$, which are learnable parameters. This decomposition ensures that the Gaussians can stretch and rotate to tightly fit the underlying scene geometry, providing a much more compact and accurate representation than spherical primitives or isotropic points. The appearance of each Gaussian is modeled using Spherical Harmonics (SH), which efficiently captures view-dependent lighting effects. Furthermore, each Gaussian possesses a learned opacity $\\alpha$, which controls its contribution to the final pixel color. This combination of geometric flexibility and appearance modeling capability allows 3DGS to reconstruct complex scenes with intricate details and specular highlights [16].\n\nA defining characteristic of the 3DGS paradigm is its revolutionary rendering algorithm, which relies on tile-based rasterization. The rendering process begins by projecting the 3D Gaussians onto the 2D image plane. During this projection, the 3D covariance matrix is transformed into a 2D covariance matrix using the Jacobian of the projection transformation. This step is essential for determining the footprint of each Gaussian in screen space. Once projected, the Gaussians are sorted into tiles (e.g., 16x16 pixel blocks) based on their depth. This sorting is critical for the subsequent alpha-blending step. For each pixel within a tile, the Gaussians that cover it are blended in a front-to-back order. The color of a pixel is computed as a weighted sum of the colors of these Gaussians, modulated by their opacities. This process is highly amenable to parallelization on modern GPUs, enabling real-time rendering performance that was previously unattainable with volumetric methods. The efficiency of this approach stands in stark contrast to the computationally expensive volumetric ray marching used by NeRFs, which requires querying a neural network at numerous points along each ray. 3DGS avoids this heavy neural inference cost entirely, relying instead on the direct rasterization of simple geometric primitives [15].\n\nThe optimization process for 3DGS is also distinct and highly effective. It typically starts with a sparse point cloud generated by Structure-from-Motion (SfM) to initialize the positions of the Gaussians. These positions, along with other attributes, are then optimized using standard gradient descent to minimize the photometric difference between the rendered images and the input training views. A key innovation that ensures high fidelity is the adaptive density control strategy. Throughout optimization, the algorithm dynamically adds or removes Gaussians based on the gradients of the rendering loss. If a region of the scene is not being reconstructed well (high gradient), new Gaussians are introduced (either by splitting existing ones or cloning them) to capture the missing detail. Conversely, Gaussians in regions with low gradients or those that are effectively redundant are pruned. This adaptive mechanism allows the representation to grow in complexity where needed, preventing over-reconstruction in some areas while ensuring fine details are captured in others, a process that is integral to achieving the high quality associated with 3DGS [15].\n\nWhen contrasted with other representations, the advantages and trade-offs of 3DGS become clear. Compared to implicit neural fields like NeRF, 3DGS offers superior rendering speed, often achieving hundreds of frames per second. This explicit representation also facilitates downstream tasks such as editing, manipulation, and dynamic scene modeling, as individual primitives can be directly modified. However, this explicit nature comes with a higher storage cost compared to the compact weights of a NeRF model. In comparison to traditional mesh-based methods, 3DGS does not require a predefined topology and can represent unstructured points, volumetric effects, and fine details without the need for complex meshing algorithms. While meshes are superior for rigid transformations and physics simulations, 3DGS provides a more flexible and easier-to-optimize representation for reconstruction and rendering tasks. The paradigm thus establishes a new trade-off frontier, prioritizing rendering speed and editability while managing storage requirements, which has spurred a vast amount of follow-up research aimed at compression and efficiency [15].\n\nIn summary, the 3D Gaussian Splatting paradigm represents a significant evolution in 3D scene representation. By combining the expressiveness of anisotropic 3D primitives with the efficiency of tile-based rasterization, it achieves a \"best of both worlds\" scenario: the high quality of volumetric rendering and the real-time performance of traditional graphics pipelines. This has not only set a new standard for novel view synthesis but has also opened up new avenues for research in dynamic scenes, generative 3D synthesis, and interactive 3D editing, solidifying its position as a foundational technology for the next generation of 3D vision and graphics applications [16].\n\n### 1.3 Mathematical Foundations and Differentiable Rendering\n\nThe mathematical foundation of 3D Gaussian Splatting (3DGS) rests on the explicit representation of a scene as a collection of anisotropic 3D Gaussian primitives. Unlike implicit neural fields that map coordinates to properties via neural networks, 3DGS utilizes a discrete set of learnable parameters for each primitive, enabling efficient, rasterization-based rendering. Each 3D Gaussian is defined by a probability density function centered at a position $\\mathbf{x}$ in 3D space:\n\n$$ G(\\mathbf{x}) = e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})} $$\n\nwhere $\\boldsymbol{\\mu}$ represents the mean position (center) of the Gaussian, and $\\Sigma$ is the covariance matrix that determines the shape and orientation of the Gaussian ellipsoid. To ensure that the covariance matrix $\\Sigma$ remains positive semi-definite (a requirement for valid Gaussian distributions), it is constructed from a scaling matrix $\\mathbf{S}$ and a rotation matrix $\\mathbf{R}$. Specifically, $\\Sigma = \\mathbf{R} \\mathbf{S} \\mathbf{S}^T \\mathbf{R}^T$. The scaling matrix $\\mathbf{S}$ contains the scaling factors along the principal axes, while the rotation matrix $\\mathbf{R}$, derived from a quaternion, orients the Gaussian in 3D space. This parametrization allows the Gaussians to adapt to the geometry of the scene, stretching and rotating to cover surfaces or volumes efficiently [16].\n\nIn addition to geometric attributes, each Gaussian possesses appearance attributes. The color of a Gaussian is view-dependent, modeled using Spherical Harmonics (SH). This allows the representation of complex lighting effects and varying appearance from different angles without storing explicit textures for every primitive. The opacity of the Gaussian is represented by a scalar value $\\alpha$, which controls the contribution of the primitive to the final pixel color during the blending process. The use of Spherical Harmonics is a critical component, as it compresses view-dependent color information into a compact set of coefficients, though recent works have explored alternative representations to capture higher-frequency details [17] or decouple geometry from appearance [18].\n\nThe rendering process in 3DGS is a differentiable rasterization pipeline, which differs significantly from the volumetric ray marching used in Neural Radiance Fields (NeRF). The pipeline projects the 3D Gaussians into 2D screen space to compute pixel values. This process begins with the projection of the 3D covariance matrix $\\Sigma$ into 2D. Given a viewing transformation matrix $W$ and the camera projection Jacobian $J$, the 2D covariance matrix $\\Sigma'$ is computed as:\n\n$$ \\Sigma' = J W \\Sigma W^T J^T $$\n\nThis projection ensures that the 2D splat (the footprint of the Gaussian on the image plane) accurately reflects the Gaussian's orientation and scale relative to the camera. The projected 2D Gaussians are then sorted into tiles (e.g., $16 \\times 16$ pixel tiles) based on their depth. This sorting is crucial for the correct alpha-blending of semi-transparent primitives.\n\nOnce sorted, the rendering of each pixel involves blending the colors of the overlapping Gaussians. The color $C$ of a pixel is calculated using the standard volume rendering equation, adapted for rasterization:\n\n$$ C = \\sum_{i=1}^{N} c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) $$\n\nwhere $c_i$ is the view-dependent color of the $i$-th Gaussian (evaluated via Spherical Harmonics), and $\\alpha_i$ is its opacity. The product term represents the accumulated transmittance of all Gaussians in front of the current one. This alpha-blending formula is differentiable with respect to all Gaussian parameters (position, covariance, opacity, and color coefficients), allowing for end-to-end optimization via gradient descent. The mathematical formulation of this differentiable rasterization is detailed in the \"Mathematical Supplement for the $\\texttt{gsplat}$ Library\" [19], which provides the forward and backward pass computations required for training.\n\nHowever, the standard rasterization pipeline treats each pixel as a single point sample, which leads to aliasing artifacts when rendering at resolutions different from the training resolution. To address this, several extensions have been proposed to modify the mathematical foundation of the rendering process. For instance, \"Analytic-Splatting\" introduces an analytical integration of the Gaussian within the pixel window area, approximating the cumulative distribution function (CDF) to better capture the intensity response of the pixel [20]. Similarly, \"Mip-Splatting\" applies a 3D smoothing filter to the Gaussians based on the pixel footprint to prevent aliasing. These methods refine the rendering equation to ensure frequency-correct rendering, a limitation inherent in the original discrete sampling scheme.\n\nFurthermore, the explicit nature of the 3D Gaussians allows for the derivation of surface properties. While the original 3DGS represents the scene as a cloud of points, methods like \"Gaussian Opacity Fields (GOF)\" derive a continuous opacity field from the discrete Gaussians to extract surfaces via level-set extraction [21]. Alternatively, \"2D Gaussian Splatting\" reformulates the primitives as oriented 2D disks (surfels) to intrinsically model surfaces, changing the projection math to handle ray-splat intersections rather than volumetric splatting [22]. These variations demonstrate the flexibility of the underlying mathematical framework, which can be adapted from pure volumetric rendering to surface-based rendering while maintaining the core differentiable rasterization pipeline.\n\nThe optimization of these primitives relies on a loss function that minimizes the difference between the rendered image and the ground truth input images. The adaptive density control strategy (densification) is a key component of the optimization, dynamically adjusting the number and properties of the Gaussians. This process involves identifying regions with high gradients (indicating missing geometry or details) and either cloning new Gaussians or splitting existing ones to improve fidelity. This ensures that the mathematical representation of the scene evolves to capture fine details without manual intervention. The differentiability of the rasterization pipeline is essential here, as it allows gradients to flow back through the rendering process to update the Gaussian parameters, ensuring that the explicit representation converges to a high-fidelity reconstruction of the scene.\n\n### 1.4 Optimization and Adaptive Density Control\n\nThe optimization of 3D Gaussian Splatting (3DGS) represents a delicate balance between fitting the input imagery and maintaining a coherent geometric structure. Unlike implicit representations such as Neural Radiance Fields (NeRFs), which optimize a continuous function, 3DGS optimizes a discrete set of anisotropic primitives. This explicit nature necessitates a robust optimization strategy that not only adjusts the attributes of existing Gaussians but also dynamically manages the population of these primitives. The core optimization loop aims to minimize the discrepancy between the rendered image and the ground truth input images, typically using a combination of photometric and perceptual losses. However, the critical component that ensures the success and high fidelity of 3DGS is the Adaptive Density Control (ADC) strategy, often referred to as densification. This mechanism is responsible for adding new Gaussians where detail is lacking and removing or merging those that contribute little to the scene or cause artifacts.\n\nThe optimization process begins with an initial set of Gaussians, often derived from a Structure-from-Motion (SfM) point cloud. These Gaussians possess learnable parameters: position ($\\mathbf{p} \\in \\mathbb{R}^3$), opacity ($\\alpha \\in \\mathbb{R}$), scaling ($\\mathbf{s} \\in \\mathbb{R}^3$), rotation ($\\mathbf{q} \\in \\mathbb{R}^4$), and spherical harmonics (SH) coefficients for view-dependent color. The objective function $\\mathcal{L}$ is typically defined as:\n\n$$ \\mathcal{L} = \\mathcal{L}_1 + \\lambda \\mathcal{L}_{LPIPS} $$\n\nwhere $\\mathcal{L}_1$ is the L1 loss ensuring pixel-wise intensity matching, and $\\mathcal{L}_{LPIPS}$ is a perceptual loss (Learned Perceptual Image Patch Similarity) that encourages structural and semantic consistency, mitigating blurriness often associated with pure L2 minimization. The optimization is performed using standard stochastic gradient descent (SGD) or Adam optimizers. However, the gradient-based updates alone are insufficient for handling the discrete nature of the primitives. Without intervention, the optimization tends to produce large, \"blobby\" Gaussians that cover large areas but lack fine detail, or conversely, leaves gaps in the reconstruction where the initial points are sparse.\n\nThis is where Adaptive Density Control becomes indispensable. The ADC strategy monitors the gradients of the Gaussians with respect to the loss to infer where the scene representation is inadequate. The fundamental intuition is that if a Gaussian is responsible for a region with high rendering error (high gradient magnitude), it likely needs to be refined. Conversely, if a Gaussian is poorly initialized (e.g., placed in empty space) or redundant, it should be removed. The original 3DGS implementation defines specific criteria for these operations based on the view-averaged geometric gradient magnitude.\n\nThe densification process generally operates in two distinct modes: **cloning** and **splitting**. Cloning is typically used when a Gaussian is too large for the region it occupies, resulting in blurry artifacts. By cloning the Gaussian\u2014copying it and slightly perturbing the position or scaling\u2014the system can better cover complex geometry. Splitting, on the other hand, is used when a Gaussian is too small or when the gradients indicate that the current primitive cannot capture the necessary detail. The Gaussian is split into two, usually with reduced scaling, effectively increasing the density of primitives in that area.\n\nHowever, the standard ADC has limitations that have been identified in subsequent research. For instance, the criteria for densification are often heuristic and rely on fixed thresholds that may not adapt well to different scene scales or complexities. Furthermore, the opacity handling during these operations can introduce biases. A notable refinement in this area is presented in **Revising Densification in Gaussian Splatting**, which proposes a more principled, pixel-error driven formulation for density control. This work suggests that relying solely on geometric gradients might not perfectly correlate with the final rendering quality. Instead, leveraging an auxiliary, per-pixel error function as the criterion for densification allows for a more targeted addition of Gaussians where they are visually most needed [23]. This approach ensures that the densification process is directly tied to the reduction of rendering artifacts rather than abstract geometric properties.\n\nMoreover, the management of the Gaussian population is a significant challenge. The number of Gaussians can grow uncontrollably, leading to excessive memory usage and slower rendering times. The work in **Revising Densification in Gaussian Splatting** also introduces a mechanism to control the total number of primitives generated per scene, preventing the model from becoming unnecessarily bulky while maintaining fidelity. Additionally, it corrects a bias in the opacity handling strategy during cloning operations, which previously could lead to suboptimal transparency values that hinder the rendering of semi-transparent or fine structures [23].\n\nThe concept of density management is not unique to 3DGS but is a recurring theme in explicit representation methods. In the broader context of computer vision, methods like **Density-embedding layers a general framework for adaptive receptive fields** explore how density functions can describe the receptive field of neurons, allowing for adaptive processing [24]. While not directly applied to 3DGS, the theoretical underpinnings of density modeling highlight the importance of adaptive structures in capturing complex data distributions. Similarly, in the realm of Neural Radiance Fields, **Density Uncertainty Quantification with NeRF-Ensembles** demonstrates the value of understanding density variance to improve robustness and remove artifacts [25]. These concepts underscore the necessity of not just optimizing density values but actively managing them to ensure the integrity of the reconstruction.\n\nIn 3DGS, the optimization of opacity is particularly sensitive. Gaussians with high opacity in unintended locations can occlude correct geometry, creating \"floaters\" or fog-like artifacts. Conversely, low opacity fails to block light, leading to washed-out colors. The standard training pipeline includes an opacity reset step, where the opacity of all Gaussians is reset to a low value periodically during the early stages of training. This allows the optimization to \"re-discover\" the correct geometry without being locked into poor initial opacity configurations. However, more advanced strategies, such as those proposed in **Gaussian Splatting with Localized Points Management**, move beyond simple resets. This work identifies error-contributing zones using multiview geometry constraints and rendering errors, applying point densification specifically in these zones while resetting the opacity of points residing in front of these regions. This creates a new opportunity to correct ill-conditioned points, effectively performing geometry calibration alongside densification [26].\n\nThe optimization landscape for 3DGS is non-convex and high-dimensional. To navigate this, the learning rate scheduling plays a crucial role. Typically, position and SH coefficients are optimized with higher learning rates initially to allow Gaussians to move and adjust color quickly, while scaling and opacity are optimized with lower rates to prevent instability. As training progresses, these rates are decayed to fine-tune the details.\n\nFurthermore, the optimization process is heavily influenced by the rendering pipeline's differentiability. The tile-based rasterization allows gradients to flow back to the Gaussian parameters efficiently. However, the sorting and blending operations introduce discrete steps that can sometimes lead to vanishing gradients if not handled carefully. The optimization must account for the fact that the contribution of a Gaussian to a pixel is determined by its 2D covariance, opacity, and color, all of which are derived from the 3D parameters. The gradients of these derived parameters with respect to the primitive attributes are complex but essential for the adaptive control to function correctly.\n\nIn summary, the optimization of 3DGS is a sophisticated interplay between photometric loss minimization and geometric structure maintenance. The Adaptive Density Control strategy is the linchpin of this process, transforming a static point cloud into a dynamic, self-correcting representation. By monitoring gradients and rendering errors, ADC dynamically adds, splits, clones, and prunes Gaussians to match the scene's complexity. Recent advancements, such as pixel-error driven densification and localized point management, have refined this process, addressing issues of bias, artifact generation, and uncontrolled growth of primitives. These improvements ensure that 3DGS remains not only fast and efficient but also capable of achieving state-of-the-art fidelity in novel view synthesis. The continuous evolution of these optimization strategies highlights the community's effort to overcome the inherent challenges of explicit, discrete scene representations.\n\n### 1.5 Comparative Analysis: 3DGS vs. NeRF and Mesh-based Methods\n\nThe advent of Neural Radiance Fields (NeRFs) marked a paradigm shift in novel view synthesis, offering unprecedented fidelity by representing scenes as continuous volumetric functions optimized from multi-view images [27]. However, the computational burden of volumetric ray marching inherent in NeRFs has driven the search for explicit, efficient alternatives. 3D Gaussian Splatting (3DGS) has emerged as a leading contender, leveraging a point-based representation coupled with tile-based rasterization to achieve real-time rendering speeds. This subsection provides a comparative analysis of 3DGS against the two dominant paradigms: the implicit continuous fields of NeRF and the structured connectivity of traditional mesh-based methods. We summarize the trade-offs in speed, quality, and editability, highlighting how 3DGS bridges the gap between the high quality of implicit representations and the practical utility of explicit geometry.\n\n**Representation and Rendering Paradigms**\n\nThe fundamental difference between these methods lies in how they represent 3D geometry and how they project that geometry into 2D images. NeRFs utilize an implicit representation, typically a Multi-Layer Perceptron (MLP), to map spatial coordinates and viewing directions to color and volume density. Rendering an image requires querying this network for every sample along every camera ray, a process that is computationally expensive and difficult to parallelize effectively on standard hardware. While recent advancements have improved speed, the fundamental bottleneck of volumetric integration remains.\n\nIn contrast, 3DGS employs an explicit representation consisting of a massive collection of anisotropic 3D Gaussians [15]. Each Gaussian is defined by a position, covariance (controlling scale and rotation), opacity, and view-dependent color represented by Spherical Harmonics. The rendering process is not volumetric ray marching but rather a differentiable tile-based rasterization (splatting). The 3D Gaussians are projected onto the image plane, sorted by depth, and alpha-blended in a forward pass. This approach is highly parallelizable on GPUs, enabling real-time rendering performance (over 100 FPS) that is orders of magnitude faster than NeRFs.\n\nTraditional mesh-based methods, such as those relying on Constructive Solid Geometry (CSG) or polygonal modeling, represent geometry via vertices, edges, and faces with explicit connectivity. Rendering meshes is extremely fast using standard graphics pipelines. However, creating these meshes usually requires manual artist intervention or complex reconstruction algorithms. While methods exist to generate meshes from point clouds or implicit fields, they often struggle with complex topologies or require post-processing steps like marching cubes, which can introduce artifacts or lose fine details. 3DGS offers a middle ground: it is explicit like a mesh but consists of unconnected points, avoiding the need for complex connectivity management while still enabling fast rasterization.\n\n**Quality and Fidelity**\n\nIn terms of rendering quality, NeRFs have long been the gold standard, capable of capturing subtle view-dependent effects and complex lighting. However, they often suffer from \"floaters\" (unseen artifacts in the volume) and can struggle with reconstruction in textureless regions. 3DGS achieves comparable, and often superior, visual fidelity by leveraging the adaptive density control strategy (as detailed in the previous subsection). This optimization process dynamically adds or removes Gaussians based on gradient analysis, ensuring that the primitives are placed precisely where needed to capture details. This explicit control over the distribution of primitives allows 3DGS to recover high-frequency textures and sharp edges more effectively than the continuous fields of NeRFs, which can be prone to over-smoothing.\n\nMesh-based methods, when derived from reconstruction, face challenges in maintaining geometric accuracy without sufficient resolution. High-quality mesh extraction from implicit fields often requires dense sampling and careful regularization. Recent work in the 3DGS domain, such as 3DGSR [28], demonstrates how integrating implicit Signed Distance Fields (SDFs) with 3D Gaussians can yield accurate surface reconstruction while preserving the rendering quality of 3DGS. This hybrid approach suggests that explicit point-based representations can be optimized to recover geometry that is competitive with mesh-based reconstruction techniques, without sacrificing the rendering speed.\n\n**Editability and Manipulation**\n\nA significant advantage of 3DGS over NeRFs is editability. NeRFs are \"black box\" representations; editing a scene requires costly re-optimization or complex semantic disentanglement. Because 3DGS represents the scene as a collection of discrete primitives, it allows for direct manipulation. For instance, GSDeformer [29] extends cage-based deformation to 3DGS, enabling free-form deformation without architectural changes. This allows for intuitive editing of trained models, a capability that is extremely difficult to achieve with implicit NeRF representations.\n\nMesh-based methods have traditionally dominated the domain of interactive editing due to their structured connectivity. Tools for mesh deformation, Boolean operations, and structural modification are well-established. However, the complexity of mesh data structures makes real-time manipulation of high-resolution meshes challenging. Boolean operations on meshes, for example, are notoriously difficult to implement robustly and efficiently, often requiring complex spatial partitioning and intersection detection [30]. 3DGS bypasses the need for explicit connectivity, allowing for simpler operations like object selection and translation by manipulating Gaussian attributes directly. Furthermore, the explicit nature of Gaussians facilitates integration with segmentation models for semantic editing, enabling localized changes without affecting the entire scene [31].\n\n**Scalability and Memory**\n\nScalability remains a challenge for all three paradigms. NeRFs, while compact in terms of network parameters, suffer from slow rendering that scales poorly with scene complexity and resolution. Mesh-based methods scale well in rendering but require efficient data structures (e.g., quad-trees, octrees) to handle large scenes and varying levels of detail (LoD). The memory footprint of high-resolution meshes can be substantial.\n\n3DGS introduces a unique scalability challenge: the number of Gaussians can grow very large (millions) to capture high-fidelity details, leading to significant storage costs. However, the explicit nature of the representation also opens avenues for compression and optimization that are not available in implicit fields. Techniques such as pruning [32] and quantization [33] are actively being developed to reduce the memory footprint. Unlike NeRFs, where reducing memory often degrades quality globally, 3DGS allows for selective reduction of primitives, maintaining quality in salient regions while compressing the representation.\n\n**Conclusion**\n\nIn summary, 3DGS effectively synthesizes the strengths of implicit and explicit representations. It retains the high rendering quality and view-dependent effects of NeRFs while achieving the real-time performance of mesh-based rendering. Its explicit, point-based nature offers superior editability compared to NeRFs and avoids the topological complexities of mesh connectivity. While challenges remain in memory management and geometric precision, the rapid evolution of 3DGS methodologies indicates it is rapidly becoming the preferred representation for applications requiring both high fidelity and interactive performance.\n\n## 2 Core Methodologies and Algorithmic Enhancements\n\n### 2.1 Mathematical Formulation of Gaussian Primitives\n\nThe mathematical foundation of 3D Gaussian Splatting (3DGS) represents a paradigm shift from the implicit continuous volumetric functions used in Neural Radiance Fields (NeRFs) [1] to an explicit, discrete, and anisotropic representation. This shift is motivated by the need for high-fidelity rendering and real-time performance, addressing the computational bottlenecks inherent in volumetric ray marching [2]. Instead of querying a multi-layer perceptron (MLP) hundreds of times per pixel, 3DGS utilizes a collection of 3D Gaussian primitives, each defined by a set of learnable attributes. These attributes include position, covariance (encoding scale and rotation), opacity, and view-dependent color, typically represented using Spherical Harmonics (SH). The optimization of these primitives is driven by differentiable rendering, allowing the reconstruction of complex scenes from multi-view images.\n\n### Position and Covariance\n\nAt the core of the representation is the 3D Gaussian distribution. A 3D point $\\mathbf{x} \\in \\mathbb{R}^3$ has a probability density given by:\n\n$$ G(\\mathbf{x}) = e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})} $$\n\nwhere $\\boldsymbol{\\mu} \\in \\mathbb{R}^3$ is the mean position of the Gaussian, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{3 \\times 3}$ is the covariance matrix representing the spread and orientation of the Gaussian. In the context of 3DGS, the scene is represented by a set of $N$ such Gaussians, where $N$ can vary dynamically during optimization.\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ must be positive semi-definite. To ensure this constraint is met while allowing for efficient optimization via gradient descent, the covariance matrix is not optimized directly. Instead, it is parameterized by a scaling matrix $\\mathbf{S} \\in \\mathbb{R}^{3 \\times 3}$ and a rotation matrix $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$:\n\n$$ \\boldsymbol{\\Sigma} = \\mathbf{R} \\mathbf{S} \\mathbf{S}^T \\mathbf{R}^T $$\n\nHere, the scaling matrix $\\mathbf{S}$ is a diagonal matrix containing the scaling factors $[34]$, allowing the Gaussian to stretch along three orthogonal axes. The rotation matrix $\\mathbf{R}$ is derived from a 4-dimensional quaternion $\\mathbf{q} = [35]$, which is normalized during optimization to ensure it represents a valid rotation. This parameterization decouples scale and rotation, facilitating stable optimization and allowing the Gaussians to adapt to the local geometry of the scene, effectively acting as anisotropic surfels or \"blobs\" that can cover surfaces efficiently.\n\n### Opacity and View-Dependent Color\n\nWhile the geometry is defined by the position and covariance, the appearance is defined by opacity and color. Each Gaussian $i$ has a learned opacity $\\alpha_i \\in [36]$. This opacity is passed through a sigmoid function during rendering to ensure it remains within a valid range, controlling the contribution of the Gaussian to the final pixel color.\n\nTo model view-dependent effects (such as specularities and lighting variations), 3DGS employs Spherical Harmonics (SH). Unlike NeRF, which typically uses a small MLP to predict RGB values conditioned on viewing direction, 3DGS pre-defines a basis of SH functions and optimizes the coefficients for each Gaussian. The color $\\mathbf{c}_i$ viewed from direction $\\mathbf{d}$ is computed as:\n\n$$ \\mathbf{c}_i(\\mathbf{d}) = \\sum_{k=0}^{K} c_{i,k} Y_k(\\mathbf{d}) $$\n\nwhere $Y_k$ are the spherical harmonic basis functions and $c_{i,k}$ are the learned coefficients. Typically, the first 3 bands (corresponding to 9 coefficients) are used for RGB channels, providing a balance between capturing low-frequency lighting changes and maintaining computational efficiency. This explicit representation of color allows for fast evaluation without the overhead of MLP inference, which is a significant contributor to the real-time performance of 3DGS compared to methods like [6] that attempt to optimize NeRF inference speed.\n\n### Optimization and Adaptive Density Control\n\nThe optimization of these primitives is driven by minimizing the photometric loss between the rendered image and the ground truth input images. The loss function typically includes an L1 loss and a D-SSIM (Structural Dissimilarity) term:\n\n$$ \\mathcal{L} = (1 - \\lambda_{\\text{ssim}}) \\mathcal{L}_1 + \\lambda_{\\text{ssim}} \\mathcal{L}_{\\text{D-SSIM}} $$\n\nHowever, a static set of Gaussians is insufficient to reconstruct high-fidelity details. A critical component of the mathematical formulation is the **adaptive density control** strategy. During training, the gradients with respect to the Gaussian parameters are monitored to determine where to add or remove geometry.\n\n1.  **Densification (Splitting):** If the gradient magnitude of a Gaussian's position is large (indicating the Gaussian is covering a region with high-frequency detail or is insufficient to cover the geometry), the Gaussian is cloned. The original Gaussian is scaled down, and a copy is created with a slightly perturbed position.\n2.  **Densification (Cloning):** If a Gaussian covers a large area but has high opacity gradients (indicating it is trying to cover multiple distinct surfaces), it is cloned to separate the surfaces.\n3.  **Pruning:** If a Gaussian's opacity drops below a threshold or it becomes too small (scale magnitude < threshold), it is removed from the scene to maintain efficiency.\n\nThis adaptive control allows the representation to start from a sparse point cloud (often generated via Structure-from-Motion) and progressively add Gaussians where needed, recovering fine details while preventing the proliferation of \"floaters\" or artifacts common in point-based rendering. This dynamic topology adjustment is a key advantage over implicit methods, which have fixed capacity, and explicit meshes, which require complex remeshing operations.\n\n### Conclusion\n\nIn summary, the mathematical formulation of 3D Gaussian Splatting relies on an explicit, anisotropic representation composed of position, covariance (via scale/rotation), opacity, and Spherical Harmonics color coefficients. The optimization is stabilized by adaptive density control, which dynamically adjusts the number and properties of the Gaussians. This combination allows 3DGS to achieve state-of-the-art visual quality with real-time rendering speeds, bridging the gap between the quality of implicit neural fields and the efficiency required for interactive applications. This explicit representation is the foundation upon which the differentiable rasterization pipeline operates, transforming these 3D primitives into a 2D image.\n\n### 2.2 Differentiable Rasterization Pipeline\n\nThe Differentiable Rasterization Pipeline is the computational engine that enables 3D Gaussian Splatting (3DGS) to achieve real-time rendering speeds while maintaining differentiability for optimization. Unlike volumetric ray marching used in Neural Radiance Fields (NeRFs), which requires sampling along rays and querying neural networks at each point, the 3DGS pipeline projects discrete 3D primitives directly onto the image plane and blends them using a tile-based rasterization algorithm. This approach leverages the parallel processing power of modern GPUs to render millions of Gaussians in milliseconds. The pipeline consists of several sequential stages: projection of 3D Gaussians to 2D screen space, determination of the rendering order, and alpha-blending of overlapping splats within each pixel. The differentiability of this process is crucial, as it allows gradients to flow back from the rendered image to the parameters of the 3D Gaussians, facilitating the optimization of scene appearance and geometry.\n\nThe first step in the pipeline is the projection of 3D Gaussians into 2D screen space. A 3D Gaussian is defined by its mean (position) $\\mu \\in \\mathbb{R}^3$ and a covariance matrix $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$. To render the Gaussian, we must transform it into the camera coordinate system and project it onto the image plane. This involves a series of linear transformations. First, the 3D position is transformed by the view matrix $W$ (world-to-camera) and the projection matrix $P$ (camera-to-clipping). However, simply projecting the mean is insufficient; the covariance must also be transformed to represent the Gaussian's extent in the view frustum. The covariance $\\Sigma$ is transformed using the Jacobian of the projection function $J$ and the rotation component of the view matrix $R$. Specifically, the 2D covariance matrix $\\Sigma'$ is approximated as:\n$$ \\Sigma' = J W R \\Sigma R^T W^T J^T $$\nThis approximation treats the projection as a linear operation, which is valid for Gaussians that are not too close to the camera plane. This transformation yields the 2D covariance matrix that defines the shape and orientation of the Gaussian \"splat\" on the screen. The Gaussian is then clipped to the view frustum, and only those that contribute to the current view are processed further. This projection step is highly optimized and is a key reason for the efficiency of 3DGS compared to implicit methods [15].\n\nOnce projected, the Gaussians are sorted to ensure correct blending order. In volumetric rendering, the order is naturally handled by the ray-marching process, but in rasterization, explicit sorting is required to handle occlusion correctly. 3DGS employs a tile-based approach to manage this complexity efficiently. The screen is divided into a grid of fixed-size tiles (e.g., 16x16 pixels). For each Gaussian, its bounding box in screen space is calculated. If the bounding box overlaps with a tile, the Gaussian is added to that tile's list of splats. This culling step is vital for performance, as it avoids processing Gaussians that are far from the pixel being rendered.\n\nAfter assigning Gaussians to tiles, the rendering algorithm must determine the order in which to blend them. The original 3DGS paper [15] proposes a sorting strategy based on the depth of the Gaussians. However, sorting millions of primitives per frame is computationally expensive. To achieve real-time performance, the pipeline uses a hybrid approach. First, the Gaussians within each tile are sorted by their depth (the z-coordinate of their mean in camera space). This ensures that closer surfaces correctly occlude farther ones. However, a global sort across all tiles is not performed; instead, the rendering is performed tile-by-tile. This local sorting is sufficient for most scenes and significantly reduces the computational burden compared to a full per-pixel sort. The use of atomic operations and efficient data structures on the GPU allows for parallel processing of tiles, distributing the workload across thousands of threads.\n\nWith the Gaussians sorted, the final step is alpha-blending. For each pixel within a tile, the algorithm iterates through the sorted list of Gaussians that cover that pixel. The contribution of each Gaussian to the pixel's color is calculated using the Gaussian's opacity and color. The color is typically represented using Spherical Harmonics (SH) to capture view-dependent effects, while the opacity is a scalar value between 0 and 1. The blending formula follows the standard volume rendering equation, approximated discretely:\n$$ C = \\sum_{i=1}^{N} c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) $$\nwhere $c_i$ is the color of the $i$-th Gaussian, $\\alpha_i$ is its opacity, and the product term represents the accumulated transmittance of all preceding Gaussians. This formula ensures that closer Gaussians obscure those behind them. The color $c_i$ is derived from the SH coefficients associated with the Gaussian, evaluated at the viewing direction. The opacity $\\alpha_i$ is derived from the Gaussian's density at the pixel center, scaled by the Gaussian's learned opacity. This alpha-blending process is performed for every pixel, accumulating the contributions of all overlapping Gaussians to produce the final pixel color.\n\nThe differentiability of this rasterization pipeline is what makes 3DGS a learnable representation. The entire process\u2014from projection to sorting to blending\u2014is designed to be differentiable with respect to the parameters of the 3D Gaussians (position, covariance, opacity, and color). Gradients of the rendering loss (e.g., L1 or L2 loss between rendered and ground-truth images) with respect to the pixel colors can be backpropagated through the alpha-blending formula. These gradients then flow to the attributes of the Gaussians that contributed to those pixels. For example, the gradient with respect to a Gaussian's position affects its projected 2D location, influencing which pixels it contributes to and how strongly. Similarly, gradients for covariance affect the shape of the splat, gradients for opacity affect the transmittance, and gradients for SH coefficients affect the color. This end-to-end differentiability allows the optimization to adjust the Gaussians to minimize the reconstruction error, effectively \"fitting\" the scene.\n\nHowever, the naive rasterization pipeline faces challenges, particularly regarding aliasing and view consistency. As discussed in other sections, discrete sampling can lead to artifacts when the pixel footprint does not match the Gaussian size. Furthermore, the simple depth-sorting approach can cause popping artifacts during camera motion because the relative order of Gaussians can change abruptly. To address these, several extensions to the core pipeline have been proposed. For instance, [37] introduces a hierarchical rasterization approach that resorts and culls splats with minimal overhead, eliminating popping artifacts and ensuring view consistency. This method improves upon the global sort by using a more sophisticated data structure to manage splats per pixel, albeit with a slight overhead that is compensated by the ability to use fewer Gaussians.\n\nAnother significant optimization of the rasterization pipeline focuses on reducing the computational cost of culling and sorting. The original pipeline involves checking every Gaussian against every tile it might overlap, which can be inefficient. [38] proposes moving part of the serial culling into an earlier parallel preprocess stage. It uses an adaptive radius to narrow the rendering pixel range for each Gaussian and introduces a load balancing method to minimize thread waiting time during pixel-parallel rendering. This work highlights that the rendering bottleneck often lies in the culling stage, and by optimizing the bounding box calculations and using adaptive radii, they achieve a rendering speedup of over 300% while maintaining quality. Similarly, [39] proposes clustering Gaussians offline and projecting these clusters at runtime to quickly identify unnecessary Gaussians, reducing the rendering computation by almost 38.3% without sacrificing quality.\n\nThe efficiency of the rasterization pipeline is also tied to the underlying hardware. While the standard implementation uses GPU rasterization hardware, alternative approaches exist. [40] explores using GPU ray tracing hardware to render Gaussians. Instead of tile-based rasterization, they build a bounding volume hierarchy (BVH) over the Gaussians and cast rays for each pixel. This allows for handling incoherent rays (e.g., for reflections and shadows) and rendering from highly distorted cameras, which are difficult for standard rasterization. They encapsulate particles with bounding meshes to leverage fast ray-triangle intersections. While this approach offers more flexibility, the standard rasterization pipeline remains the preferred choice for pure novel view synthesis due to its raw speed on standard GPUs.\n\nIn summary, the Differentiable Rasterization Pipeline is a sophisticated yet efficient system that transforms 3D Gaussian primitives into 2D images. It relies on a sequence of optimized steps: projecting 3D Gaussians to 2D, culling them to tiles, sorting them by depth, and blending them using alpha compositing. The differentiability of this pipeline enables the optimization of millions of parameters to achieve photorealistic results. Ongoing research continues to refine this pipeline, addressing aliasing, view consistency, and computational bottlenecks to push the boundaries of real-time neural rendering.\n\n### 2.3 Adaptive Densification and Optimization Strategies\n\nThe optimization process in 3D Gaussian Splatting (3DGS) is a delicate balance between achieving high-fidelity rendering and maintaining a compact, well-structured representation of the scene. Unlike implicit representations such as Neural Radiance Fields (NeRFs), which optimize continuous fields, 3DGS relies on a discrete set of anisotropic primitives. This explicit nature necessitates a dynamic management strategy for these primitives during training. The core of this strategy lies in the adaptive density control mechanism, which was introduced in the foundational work [41] and subsequently refined by numerous researchers to address its inherent limitations. This subsection reviews the critical optimization loop, focusing on the standard densification strategies and recent advancements that refine these processes to recover fine details and improve training stability, ultimately bridging the gap between rendering quality and geometric consistency.\n\nThe original 3DGS framework establishes a robust baseline for optimization. It begins by initializing a set of 3D Gaussians, typically derived from a sparse Point Cloud generated via Structure-from-Motion (SfM). The optimization loop then iteratively refines the attributes of these Gaussians\u2014position ($\\mu$), covariance ($\\Sigma$), opacity ($\\alpha$), and color (represented by Spherical Harmonics coefficients)\u2014to minimize the photometric loss against input training images. However, the initial sparse point cloud is insufficient to cover the entire scene geometry. To address this, the framework employs an adaptive density control strategy, commonly referred to as \"densification,\" which dynamically adds or removes Gaussians based on training dynamics. The primary mechanisms for densification are cloning and splitting.\n\nCloning is primarily used to address areas where the gradient magnitude of a Gaussian\u2019s position is high, indicating that the Gaussian is poorly positioned to capture the local scene content. In such cases, a new Gaussian is created as a copy of the original but with a perturbed position and a scaled covariance, effectively spreading the representation to cover the high-gradient region more effectively. This prevents the formation of \"holes\" in the reconstruction and helps stabilize the optimization of geometry. Splitting, on the other hand, is triggered when a Gaussian becomes too large or anisotropic, often leading to blurry artifacts. The strategy involves splitting a single large Gaussian into two smaller ones, typically along its major axis, and reducing their scales. This allows the representation to capture finer details without increasing the overall opacity excessively. These operations are governed by heuristic thresholds based on gradient magnitude and the magnitude of the Gaussian\u2019s scale, ensuring that the number of primitives grows in complex, detailed areas while remaining sparse in flat, uniform regions.\n\nWhile the original densification strategy is effective, it often struggles to recover high-frequency details and can lead to over-reconstruction or under-reconstruction artifacts. This has spurred a wave of research focused on refining the densification logic and optimization gradients. A key insight is that the original method treats all gradients uniformly, without considering the specific contribution of each Gaussian to the final pixel color. To address this, methods like [42] provide a comprehensive analysis of the \"over-reconstruction\" issue, identifying \"gradient collision\" as a primary cause. Gradient collision occurs when large Gaussians in over-reconstructed regions prevent smaller Gaussians from splitting to capture fine details. To mitigate this, [42] proposes a novel homodirectional view-space positional gradient criterion for densification. This strategy efficiently identifies large Gaussians in over-reconstructed regions and triggers their splitting, thereby recovering fine details that would otherwise be lost.\n\nBuilding on the idea of gradient-aware optimization, [43] introduces a pixel-aware gradient computation. The core observation is that the original 3DGS computes gradients by averaging over all pixels that a Gaussian projects onto. However, a Gaussian might contribute significantly to only a few pixels while being a minor component in others. By averaging gradients across all pixels, the influence of these critical pixels is diluted. [43] proposes to compute the gradient for each Gaussian on a per-pixel basis and then aggregate these gradients in a way that gives more weight to pixels where the Gaussian has a higher contribution. This pixel-aware approach leads to more precise densification, allowing the model to focus its capacity on areas that truly need more detail, resulting in sharper renderings and better handling of complex textures.\n\nFurther improvements to the densification logic have been explored to enhance stability and detail recovery. For instance, [44] analyzes the spatial distribution of Gaussians and introduces strategies for both densification and simplification. It proposes techniques like blur split and depth reinitialization to reorganize the spatial positions of Gaussians, leading to more efficient coverage and improved rendering quality with a constrained number of primitives. This highlights a growing interest in not just adding more Gaussians, but in managing them more intelligently to balance quality and resource consumption.\n\nThe optimization strategies also extend to the management of Gaussian attributes beyond just position and scale. The original 3DGS uses Spherical Harmonics (SH) for view-dependent color, which can be limited in representing high-frequency specularities. To address this, [45] introduces an anisotropic spherical Gaussian (ASG) appearance field, coupled with a coarse-to-fine training strategy. This approach improves the learning efficiency and eliminates floaters by better modeling the anisotropic components of the appearance. Similarly, [46] proposes spatially defined color and opacity within each Gaussian using SH, allowing a single Gaussian to capture more complex appearance variations, thereby enhancing rendering quality without increasing the number of primitives.\n\nAnother critical aspect of the optimization loop is the handling of aliasing artifacts, which are exacerbated by the discrete nature of Gaussian splatting. The original 3DGS rendering pipeline can produce blurring or jaggies when rendering at resolutions different from the training resolution. To address this, [47] proposes an analytical solution by approximating the Gaussian integral within a 2D pixel window area. This makes the rendering process sensitive to changes in pixel footprint at different resolutions, significantly improving anti-aliasing capabilities. Similarly, [48] and [41] introduce multi-scale representations, allowing Gaussians to adapt to different rendering scales, which is crucial for maintaining detail in both zoomed-in and zoomed-out views.\n\nIn summary, the adaptive densification and optimization strategies in 3DGS have evolved significantly from the foundational methods. While the original cloning and splitting mechanisms provide a solid foundation, recent advancements like [43] and [42] have introduced more nuanced, gradient-aware criteria to recover fine details and prevent over-reconstruction. Complementary strategies such as [44]'s spatial management and [45]'s advanced appearance modeling further refine the optimization process. These improvements collectively enhance the fidelity, stability, and efficiency of 3DGS, pushing the boundaries of what is possible with explicit scene representations and setting the stage for more robust geometric reconstruction.\n\n### 2.4 Surface Reconstruction and Geometric Alignment\n\n### 2.4 Surface Reconstruction and Geometric Alignment\n\nWhile 3D Gaussian Splatting (3DGS) excels at novel view synthesis with remarkable speed and fidelity, its explicit, point-based representation often prioritizes photometric consistency over geometric accuracy. This can lead to \"blobs\" or \"flying points\" that do not lie on a coherent surface, making the direct extraction of high-quality meshes challenging. This challenge is particularly relevant given the discussion in the previous subsection on optimization, where the primary goal was to minimize photometric error, often at the expense of geometric regularity. To address this, a significant body of research has focused on enhancing geometric accuracy and ensuring that the Gaussian primitives align with the underlying scene surfaces. These methods introduce various forms of regularization, novel primitive formulations, and intermediate field representations to bridge the gap between discrete points and continuous surfaces.\n\nA primary strategy for improving geometry is to incorporate regularization terms into the optimization objective. The standard 3DGS pipeline relies solely on photometric loss (e.g., L1 and D-SSIM), which encourages Gaussians to explain pixel colors but provides no explicit incentive for them to form a clean, well-defined surface. To counteract this, methods like **GeoGaussian** introduce geometry-aware constraints. The core idea is to encourage Gaussians to be distributed in a way that is consistent with a surface-like structure. This is often achieved by penalizing the volume of the Gaussians or by encouraging them to be flat and oriented along surfaces. By analyzing the gradient flow during optimization, these methods can identify regions where the point cloud is noisy or volumetrically ambiguous and apply targeted regularization to \"flatten\" or \"merge\" Gaussians, thereby aligning them with the true surface. This approach moves beyond simple densification (which adds points to reduce error) and introduces a principled way to control the shape and orientation of existing points to improve the geometric prior of the scene representation.\n\nA significant evolution in geometric modeling comes from the introduction of **2D Gaussian Splatting (2DGS)**. Instead of modeling the scene as a collection of volumetric 3D ellipsoids, 2DGS models it as a set of oriented 2D discs, or \"surfels.\" This change in primitive has profound implications for geometry. A 2D Gaussian has no thickness; it is intrinsically a surface element. This forces the optimization to find a solution where these discs cover the visible surfaces of the scene, effectively \"shrink-wrapping\" the geometry. The optimization process naturally encourages these surfels to align with the surface normal, providing a much cleaner and more accurate geometric representation compared to the anisotropic 3D blobs. This intrinsic surface modeling capability makes mesh extraction straightforward and reliable, as the resulting point cloud is already surface-aligned. Furthermore, by removing the redundant volume along the normal direction, 2DGS can often achieve better geometric detail with fewer primitives, addressing both quality and efficiency concerns.\n\nWhile regularization and primitive changes improve the alignment of Gaussians, extracting a clean mesh directly from the final point cloud can still be challenging. A powerful alternative is to use the optimized Gaussians to define an intermediate volumetric field, which can then be queried for surface extraction. **Gaussian Opacity Fields (GOF)** is a representative technique in this category. After optimizing a standard 3DGS scene, the contributions of all Gaussians are integrated into a regular grid, creating a dense field of opacity values. This field is much smoother and more continuous than the discrete point cloud. The surface can then be extracted by finding the zero-level set of this field (e.g., using Marching Cubes). This two-stage process decouples the fast, differentiable rendering optimization of 3DGS from the final, potentially non-differentiable, meshing step. The GOF acts as a compact and regularized representation of the scene's occupancy, allowing for the recovery of high-fidelity surfaces even from noisy or incomplete Gaussian point clouds.\n\nThe convergence of these techniques highlights a shift in the role of 3DGS from a pure rendering engine to a versatile scene reconstruction tool. The optimization process is no longer just about minimizing pixel error; it is increasingly about balancing photometric fidelity with geometric priors. This involves careful design of loss functions, as seen in the pixel-aware gradient methods [23] which refine where new points are added, and in the geometry-aware regularization of methods like GeoGaussian. The choice of primitive\u2014whether volumetric 3D, surface-aligned 2D, or an intermediate field\u2014becomes a key design decision depending on the target application. Looking forward, the integration of these geometric enhancements with more powerful priors, such as depth or normal estimations from foundation models, promises to further close the gap between fast radiance field rendering and high-precision geometric reconstruction. This focus on geometric integrity is a crucial step towards enabling applications that require not just photorealistic rendering but also an accurate 3D model of the scene, a theme that will be further explored in the context of anti-aliasing and frequency domain handling.\n\n### 2.5 Anti-Aliasing and Frequency Domain Handling\n\n### 2.5 Anti-Aliasing and Frequency Domain Handling\n\nThe explicit, discrete nature of 3D Gaussian Splatting (3DGS) makes it highly susceptible to aliasing artifacts, particularly when rendering scenes at resolutions different from those used during training or when viewing high-frequency details. Aliasing manifests as shimmering, jagged edges, or moir\u00e9 patterns, which degrade the visual fidelity of the rendered novel views. This problem is analogous to the aliasing issues faced in traditional rasterization of triangle meshes, where the finite sampling rate of the pixel grid fails to capture the continuous signal of the scene geometry and appearance. In the context of 3DGS, the rendering process projects a continuous distribution of 3D anisotropic Gaussians onto the 2D image plane, where they are blended. If the projected 2D Gaussians do not adequately represent the underlying continuous signal at the sampling frequency, aliasing occurs. Addressing this is crucial for achieving high-fidelity, resolution-independent rendering, a key goal for any practical novel view synthesis system.\n\nThe foundational 3DGS paper introduced a basic form of anti-aliasing by scaling the 2D covariance matrix of each Gaussian based on the distance to the camera, effectively blurring distant Gaussians to account for their smaller projection on the image plane. However, this simple scaling is insufficient to handle more complex scenarios, such as rendering at arbitrary resolutions or dealing with high-frequency textures and geometry. Consequently, a significant body of research has emerged to develop more robust anti-aliasing techniques specifically tailored for the Gaussian Splatting paradigm. These methods can be broadly categorized into approaches that modify the 3D primitives themselves, those that operate in the 2D rasterization pipeline, and those that leverage analytic integration for a more principled solution.\n\nOne of the most influential and widely adopted solutions is **Mip-Splatting** [49]. This method directly addresses the issue of scale-dependent aliasing by introducing a 3D smoothing filter and a 2D Mip filter into the rendering pipeline. The core insight is that the frequency content of a Gaussian primitive is related to its spatial extent. The 3D smoothing filter pre-filters the Gaussian primitives in their local 3D coordinate frame before projection. This effectively blurs the Gaussians in a manner that accounts for their anisotropic nature, ensuring that primitives that are too \"sharp\" for the current viewing conditions are appropriately regularized. Following projection, the 2D Mip filter operates on the screen-space primitives. It computes the integral of the Gaussian over the area of a pixel (or a pixel footprint) rather than just evaluating it at the pixel center. This is achieved by adjusting the opacity of the Gaussian based on its projected size relative to the pixel area. For Gaussians that are smaller than a pixel, their contribution is integrated over the pixel area, preventing them from \"popping\" in and out of existence as the camera moves. For Gaussians larger than a pixel, the standard alpha-blending is effectively used. This two-stage filtering process ensures that the rendered signal remains consistent across different resolutions, providing a form of resolution independence. The result is a significant reduction in aliasing artifacts, especially for distant objects and high-frequency textures, leading to smoother and more temporally stable renders.\n\nBuilding on the principles of pre-filtering, other works have explored alternative formulations. The concept of **Analytic-Splatting** can be understood as a more rigorous approach to this problem. Instead of approximating the integral of the Gaussian over a pixel with discrete samples or simple scaling, analytic methods aim to compute this integral in closed form. The projection of a 3D anisotropic Gaussian onto the 2D image plane results in a 2D Gaussian. The color contribution of this primitive to a pixel is the integral of its color function (often represented by Spherical Harmonics) weighted by its alpha value, over the pixel's area. Computing this integral analytically is complex, especially with view-dependent colors. However, for the case of constant color per Gaussian, the integral of the alpha-blended contribution can be computed exactly. This approach fundamentally solves the aliasing problem by correctly modeling the continuous nature of the signal being sampled. While computationally more demanding than the approximations used in Mip-Splatting, analytic integration provides a theoretically sound foundation for resolution-independent rendering and can be seen as the gold standard against which other anti-aliasing techniques are measured. It eliminates the need for heuristics related to pixel footprints by directly solving the underlying rendering equation in the continuous domain.\n\nBeyond these primary methods, the community has also investigated modifications to the core 3D representation to intrinsically improve anti-aliasing properties. For instance, **SAGS: Structure-Aware 3D Gaussian Splatting** [50] proposes a method that implicitly encodes the geometry of the scene through a graph-based structure. By enforcing meaningful point displacements that preserve the scene's geometry, SAGS can lead to a more regular and structured distribution of Gaussians. While not explicitly an anti-aliasing technique, a more structurally sound and less chaotic arrangement of primitives can reduce high-frequency noise in the initial representation, which in turn can mitigate some forms of aliasing that arise from optimizing on a discrete set of poorly distributed points. Similarly, methods that focus on surface reconstruction and geometric alignment, such as **3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting** [51], aim to align Gaussians to a continuous surface defined by an implicit Signed Distance Field (SDF). By tying the Gaussians to a continuous surface representation, these methods can produce a more coherent geometric signal, which is less prone to the high-frequency artifacts that cause aliasing-like noise. The SDF-to-opacity transformation and consistency regularization in 3DGSR help eliminate redundant surfaces and floating artifacts, which are geometric sources of aliasing-like noise.\n\nThe choice of anti-aliasing strategy involves a trade-off between rendering speed, memory overhead, and visual quality. The basic scaling in the original 3DGS is the fastest but least effective. Mip-Splatting adds a small computational overhead for the filtering steps but provides a substantial improvement in quality and is widely used in practice. Analytic methods offer the highest theoretical quality but may be slower or more complex to implement, especially when combined with complex view-dependent appearance models. The ongoing research in this area highlights the importance of frequency domain considerations in explicit rendering methods. As 3DGS is increasingly applied to large-scale scenes, video, and interactive applications, robust anti-aliasing is not just an aesthetic improvement but a fundamental requirement for usability. Future work may explore adaptive filtering techniques that vary the level of anti-aliasing based on local scene complexity or developing unified frameworks that seamlessly integrate anti-aliasing with other enhancements like compression and dynamic scene modeling. The ultimate goal remains to achieve a rendering pipeline that is both highly efficient and produces visually perfect, resolution-independent results, free from the sampling artifacts that have long plagued discrete rendering techniques.\n\n### 2.6 Compression and Efficiency Enhancements\n\nThe remarkable success of 3D Gaussian Splatting (3DGS) in achieving high-fidelity novel view synthesis with real-time rendering capabilities has established it as a powerful alternative to implicit representations like Neural Radiance Fields (NeRFs). However, a significant bottleneck hindering its widespread deployment, particularly in resource-constrained environments or large-scale applications, is the substantial storage footprint and memory bandwidth required. A typical 3DGS model trained on a high-resolution dataset can contain millions of Gaussian primitives, each parameterized by a set of attributes including position, rotation, scaling, opacity, and color coefficients (typically Spherical Harmonics). This explicit representation, while enabling efficient rendering, results in file sizes often exceeding hundreds of megabytes, a stark contrast to the compact neural weights of NeRFs. Consequently, a vibrant and critical subfield of 3DGS research has emerged, focused exclusively on compression and efficiency enhancements. These techniques aim to reduce the number of Gaussian primitives, quantize their attributes, and optimize the data structure for storage and transmission, all while striving to preserve the pristine rendering quality of the original model. This subsection surveys these crucial advancements, which are fundamental to making 3DGS a practical and scalable technology.\n\n### Attribute Quantization and Pruning Strategies\n\nA primary approach to reducing the storage cost of 3DGS involves directly reducing the number of primitives and the precision of their associated attributes. Early efforts focused on post-training optimization, but more recent methods integrate these strategies directly into the training loop for more effective co-optimization.\n\nOne of the foundational works in this area is **[52]**, which identifies the large number of Gaussians and the high precision of their attributes as the main culprits for storage bloat. To address this, the authors propose a multi-pronged strategy. First, they introduce a learnable mask strategy that effectively prunes redundant Gaussians without sacrificing performance. This is not a simple magnitude-based pruning but a learned selection process that identifies and removes primitives that contribute little to the final rendered image. Second, they tackle attribute compression. Instead of relying on the standard Spherical Harmonics (SH) for view-dependent color, which can be parameter-intensive, they propose a grid-based neural field to represent color, leading to a more compact latent representation. For geometric attributes like position, rotation, and scale, they employ vector quantization using learnable codebooks. This transforms continuous-valued attributes into discrete indices, which can then be compressed further using entropy coding techniques like Huffman coding. The combined effect of these methods is a dramatic reduction in storage, reportedly achieving over 25x compression compared to the original 3DGS while maintaining comparable rendering quality. This work laid the groundwork by demonstrating that explicit quantization and pruning are highly effective for 3DGS.\n\nBuilding upon the concept of pruning, **[53]** and **[54]** present more sophisticated pruning mechanisms. These methods recognize that naive pruning can lead to irreversible loss of important scene details. LP-3DGS, for instance, learns the importance of each Gaussian during optimization, allowing for the removal of less significant primitives in a data-driven manner. SafeguardGS likely introduces safeguards to prevent over-pruning, ensuring that the geometric and photometric integrity of the scene is maintained. These learned pruning strategies are superior to heuristic methods because they adapt to the specific characteristics of the scene, retaining Gaussians in complex, high-frequency regions while aggressively pruning those in flat, low-detail areas.\n\n### Advanced Coding and Sparse Representations\n\nBeyond simple quantization and pruning, advanced coding schemes and sparse data structures have been developed to further enhance efficiency. These methods often draw inspiration from established techniques in geometry processing and data compression, adapting them to the unique structure of Gaussian primitives.\n\nA significant contribution in this domain is **[55]**. HAC addresses the challenge of compressing the large number of Gaussian primitives by leveraging their spatial locality. The core idea is to use a hash grid to partition the 3D space, which allows for efficient context modeling. When encoding the attributes of a Gaussian, the model can use the attributes of its spatially nearby Gaussians (as identified by the hash grid) as context. This context-aware coding is highly effective because neighboring Gaussians in a scene are often correlated in their properties (e.g., color, scale). By predicting an attribute based on its context, the residual (the difference between the prediction and the actual value) can be encoded with fewer bits. HAC demonstrates that by intelligently exploiting spatial relationships, significant compression gains can be achieved without resorting to complex neural encoders.\n\nComplementing context-based coding, **[51]** focuses on creating a sparse representation of the scene. The core insight is that many scenes, even complex ones, can be represented by a much smaller set of Gaussians than typically used if they are placed and shaped optimally. EfficientGS likely employs a more aggressive densification and pruning strategy, guided by a perceptual loss or a gradient-based importance metric, to arrive at a highly sparse yet effective set of primitives. This sparsity directly translates to lower memory usage and faster rendering, as the rasterization pipeline's cost is proportional to the number of Gaussians that need to be processed per tile. By focusing on the \"efficiency\" of the representation itself, rather than just compressing a dense one, EfficientGS tackles the problem at its root. The work **[56]** extends these compression techniques to the dynamic domain, showing that similar principles of quantization, pruning, and codebook-based representation can be applied to 4D Gaussians, achieving over 12x storage efficiency for dynamic scenes.\n\n### The Role of Spherical Harmonics and Alternative Primitives\n\nWhile much of the compression literature focuses on reducing the number of Gaussians or quantizing their values, some work also examines the efficiency of the mathematical primitives themselves. The standard 3DGS uses Spherical Harmonics (SH) to model view-dependent color. While SH provides a good balance of expressiveness and efficiency, it is not always the most compact representation, especially for high-frequency specular effects. This has led to research on alternative appearance models, as seen in **[57]** and **[17]**. While these papers primarily aim to improve visual fidelity, their methods for representing appearance can have implications for efficiency. For instance, a more expressive basis might allow for fewer Gaussians to achieve the same quality, indirectly reducing storage. Conversely, a more complex appearance model might require more parameters per Gaussian. The trade-off between the number of primitives and the complexity of each primitive's attribute is a central theme in designing efficient 3DGS systems.\n\nFurthermore, the exploration of alternative Gaussian primitives, such as the **[22]**, also has efficiency implications. By collapsing the 3D volume into 2D oriented disks, 2DGS can potentially represent surfaces with fewer primitives than 3DGS, as each 2D Gaussian is inherently surface-aligned. This geometric efficiency can lead to a more compact representation for scenes that are predominantly surface-based. Similarly, **[58]** and **[59]** investigate alternative kernel functions that might offer better representational power, potentially allowing for sparser scenes. The choice of primitive is a fundamental design decision that affects both the quality and the efficiency of the final model.\n\n### Future Directions and Open Challenges\n\nThe field of 3DGS compression is rapidly evolving. The techniques surveyed here\u2014pruning, quantization, context-aware coding, and sparse representations\u2014have already made significant strides in reducing the storage burden. However, several open challenges remain. First, achieving extreme compression ratios (e.g., 100x or more) without noticeable degradation in quality is still an active area of research. This may require more powerful neural compression techniques or even more fundamental changes to the representation. Second, the computational cost of decompression, especially for methods relying on complex context models or neural fields, must be considered for real-time applications. An ideal method would offer both high compression and fast decompression. Third, standardizing benchmarks and metrics for evaluating compression methods is crucial for fair comparison and progress in the community. Finally, the principles of compression and efficiency are not just for storage but are also critical for reducing the memory bandwidth bottleneck during rendering, which is key to achieving even higher frame rates on a wider range of hardware. As 3DGS continues to be adopted in mobile, web, and interactive applications, the innovations in this subfield will be just as important as the core rendering algorithm itself.\n\n## 3 High-Fidelity Static Scene Reconstruction and Rendering\n\n### 3.1 Fundamental Techniques for Static 3DGS\n\nThe foundational principles of 3D Gaussian Splatting (3DGS) represent a paradigm shift from the volumetric ray marching utilized by Neural Radiance Fields (NeRF) towards an explicit, point-based representation optimized for real-time rendering. While NeRF models scenes as continuous volumetric functions parameterized by MLPs, requiring hundreds of network evaluations per pixel [1],>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n, an>\n\n>\n\n>\n\n>\n\n an primitives an, are an primitives>\n\n>\n\n these>\n>\n\n these these>\n\n>\n\n>\n\n>\n these,>\n\np>\n\n they>\n these a scaling in p>\n\n allows>\n\ning the,,, H the explicit representation>\n\n a collection, t a raster pipeline, but rasterization, achieving rendering speeds orders of magnitude faster than volumetric integration [2].\n\nThe optimization of these static scenes relies heavily on a carefully designed training loop that balances a balances photometric reconstruction with geometric regularization. The core loss function typically typically typically consists of a simple L1 or L2 norm between the rendered image and the ground truth input views, often augmented by a D-SSIM term to preserve structural details. However, the critical component that distinguishes 3DGS from traditional point-based methods is the adaptive density control strategy, often referred to as densification. Unlike NeRF, which implicitly handles density through a continuous field, 3DGS starts with a sparse point cloud (often derived from Structure-from-Motion data During optimization, the gradients with respect to the positions and opacities of these Gaussians are monitored. If a Gaussian exhibits high gradient magnitudes, it indicates that the primitive is failing to capture the detail in that region, likely because it is too coarse to represent fine structures. Conversely, if a Gaussian is located in a region with low gradients and low opacity, it may be redundant or floating in empty space.\n\nTo address these issues, the densification strategy employs two primary operations: cloning and splitting. When a Gaussian is in a high-gradient region, it is split into two copies with reduced scaling, effectively increasing the resolution of the representation in that area. This allows the model to recover high-frequency details that would otherwise be lost. In regions where Gaussians are poorly optimized or drifting, they are removed to maintain a clean scene structure. This dynamic management of the Gaussian population is essential for achieving high fidelity. The evolution of these strategies has been significant. For instance, early implementations focused on view-independent gradients, but subsequent research identified that view-dependent variations could also drive unnecessary density. Methods like Pixel-GS introduced pixel-aware gradient computation, ensuring that densification is driven by errors visible across multiple views, thereby preventing the over-population of the scene with primitives that only fit a single viewpoint.\n\nA major challenge in explicit point-based rendering is aliasing. When discrete samples (Gaussians) are projected onto a discrete pixel grid, high-frequency details can cause shimmering or moir\u00e9 patterns, especially when the camera moves or the scene is viewed at varying distances. NeRF-based methods handled this through integrated positional encoding or mip-NeRF approaches that model the conical frustum along a ray. 3DGS, being discrete, requires specific anti-aliasing measures. The seminal work on Mip-Splatting addresses this by applying a 3D smoothing filter to the scales of the Gaussians before projection. This effectively blurs the primitives in a way that accounts for the pixel footprint in screen space, ensuring that the Gaussian\u2019s covariance matches the sampling resolution. This prevents the \"popping\" artifacts seen when a Gaussian suddenly becomes visible or invisible as the camera moves. Furthermore, Mip-Splatting introduces a 2D Mip filter that operates in screen space, convolving the accumulated alpha-blended features with a low-pass filter. This two-stage approach\u20143D smoothing for geometric consistency and 2D filtering for sampling consistency\u2014allows 3DGS to render resolution-independent results, bridging the gap between the discrete nature of splatting and the continuous nature of the real world.\n\nThe rendering pipeline itself is a study in efficiency. Instead of marching rays through a volume, 3DGS projects all Gaussians to the 2D screen, computes their axis-aligned bounding boxes, and sorts them by depth. Pixels are rendered by iterating through these sorted Gaussians and performing alpha-blending. This tile-based approach is highly parallelizable on GPUs and allows for real-time frame rates. However, the sorting step can be a bottleneck. To mitigate this, the implementation often uses a \"differsort\" strategy, leveraging the coherence between frames to minimize sorting operations. The differentiability of this rasterizer is paramount; it allows gradients to flow from the rendered pixels back to the Gaussian parameters (position, rotation, scale, opacity, color), enabling end-to-end optimization using standard gradient descent.\n\nRecent advancements have further refined these fundamental techniques. For example, the introduction of \"Revised Densification\" logic has addressed the issue of \"blooming\" artifacts where Gaussians grow uncontrollably. By constraining the growth rates and introducing heuristics based on the opacity of the primitives, these methods ensure that the optimization converges to a stable and accurate representation. Additionally, the representation of color has evolved. While the original 3DGS used Spherical Harmonics (SH) for view-dependent appearance, some static scene methods have explored using simple MLPs or even constant colors for distant or diffuse regions to reduce computational overhead, though SH remains the standard for capturing specularities.\n\nIt is important to contrast these explicit techniques with the implicit methods that preceded them. While methods like \"Efficient Neural Radiance Fields for Interactive Free-viewpoint Video\" attempted to speed up NeRF by using cost volumes to sample near surfaces, they still relied on volumetric rendering. 3DGS abandons this entirely in favor of rasterization. This shift is comparable to the difference between ray tracing and rasterization in traditional graphics: rasterization is faster but harder to differentiate, yet 3DGS manages to maintain differentiability through its specific projection and blending formulas. Furthermore, unlike mesh-based methods that require explicit connectivity and suffer from discretization errors, 3DGS allows for topological flexibility during optimization. The scene can start as a cloud of points and evolve into dense surfaces without the need for remeshing, although extracting a watertight mesh from the final Gaussians remains a non-trivial challenge compared to implicit SDFs.\n\nIn summary, the fundamental techniques of static 3DGS revolve around the interplay between explicit Gaussian primitives and a highly optimized, differentiable rasterizer. The adaptive density control ensures that the scene complexity matches the visual complexity of the input images, while anti-aliasing techniques like Mip-Splatting ensure temporal stability and visual fidelity. These components work together to solve the \"speed vs. quality\" trade-off that plagued earlier neural rendering methods, providing a robust foundation for the high-fidelity reconstruction of static scenes.\n\n### 3.2 Large-Scale Urban Reconstruction and Scalability\n\nThe application of 3D Gaussian Splatting (3DGS) to large-scale urban environments represents a significant leap from object-centric reconstruction to complex, unbounded scenes covering vast areas. While the original 3DGS formulation demonstrated remarkable fidelity on bounded datasets, its direct application to city-scale reconstructions faces critical bottlenecks in memory consumption, rendering efficiency, and training scalability. Urban scenes typically contain billions of pixels and require an immense number of Gaussian primitives to capture geometric and photometric details, often exceeding the memory capacity of standard GPUs. Furthermore, the unbounded nature of outdoor scenes complicates the optimization process, as the \"adaptive density control\" strategies designed for isolated objects struggle to manage the distribution of primitives across varying depth ranges. To address these challenges, recent research has focused on partitioning strategies, Level-of-Detail (LoD) management, and hybrid representations that decouple rendering complexity from scene size.\n\n**Partitioning and Divide-and-Conquer Strategies**\n\nOne of the most effective strategies for scaling 3DGS to massive scenes is spatial partitioning, which breaks a large scene into manageable chunks that can be processed independently or in a hierarchical manner. This \"divide-and-conquer\" approach mitigates the memory pressure on the GPU and allows for parallel training. A prominent example of this strategy is found in **A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets** [60], which introduces a method to train large scenes by dividing them into independent chunks. These chunks are then consolidated into a hierarchy, enabling the optimization of merged Gaussians at intermediate nodes to further improve visual quality. This hierarchical structure is crucial not only for training but also for rendering, as it facilitates an efficient Level-of-Detail (LoD) solution that selects appropriate representations based on the camera's distance, ensuring real-time performance even for scenes spanning several kilometers.\n\nSimilarly, **CityGaussian** explicitly addresses the challenges of large-scale urban reconstruction by utilizing a divide-and-conquer strategy for training and a sophisticated LoD management system for rendering. By partitioning the scene into overlapping blocks, CityGaussian ensures consistent geometry across boundaries and allows for the parallel training of 3DGS models on different parts of the city. This parallelization significantly reduces training time and memory usage per GPU. Moreover, CityGaussian introduces a LoD strategy that dynamically selects the appropriate level of detail for rendering based on the distance from the camera, effectively handling the massive scale variations inherent in urban environments. This approach prevents the rendering pipeline from being overwhelmed by excessive Gaussian counts in distant views, maintaining high frame rates without sacrificing visual fidelity in close-up views.\n\n**Level-of-Detail (LoD) Management and Rendering Efficiency**\n\nBeyond training, efficient rendering of large-scale urban scenes requires robust LoD mechanisms to handle the vast number of Gaussians that fall within the viewing frustum, particularly during zoom-out views. The standard 3DGS rendering pipeline, which relies on sorting and alpha-blending Gaussians per tile, can become a bottleneck when the scene complexity is high. **Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians** [61] proposes an LoD-structured approach that dynamically selects the appropriate level from a set of multi-resolution anchor points. By organizing Gaussians within an octree structure, the method ensures consistent rendering performance across varying scales, adapting the number of active Gaussians to the current view requirements. This adaptive adjustment is essential for urban scenes, where the density of details varies significantly across different regions.\n\nAnother approach to managing rendering complexity is through the adaptive culling of unnecessary Gaussians. **Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting** [39] introduces a technique that clusters Gaussians offline and projects these clusters during runtime to quickly identify and exclude Gaussians that do not contribute to the current view. This method significantly reduces the computational cost of the projection and blending steps, achieving substantial speedups in rendering large scenes. By reducing the number of Gaussians that need to be processed per frame, such techniques make real-time rendering of complex urban environments feasible on a wider range of hardware.\n\n**Hybrid Representations and Memory Optimization**\n\nTo further tackle the memory constraints associated with large-scale 3DGS, researchers have explored hybrid representations and compression techniques. While partitioning and LoD manage the active memory footprint during rendering, the storage requirements for the full scene model remain a challenge. **FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering** [62] extends the concept of LoD to storage, allowing a scene to be represented with varying numbers of Gaussians depending on the hardware capabilities. This flexibility enables the deployment of 3DGS models on resource-constrained devices by providing a trade-off between rendering quality and memory usage.\n\nFurthermore, **Compact 3D Scene Representation via Self-Organizing Gaussian Grids** [63] proposes a method to drastically reduce storage requirements by organizing Gaussian parameters into a 2D grid with local homogeneity. This self-organizing approach exploits perceptual redundancies in natural scenes, achieving compression factors of 8x to 26x without compromising visual quality. For urban scenes, which often contain repetitive structures (e.g., buildings, windows), such compression techniques are invaluable for storing and distributing large-scale reconstructions.\n\n**Handling Unbounded Scenes and Geometric Consistency**\n\nUrban environments are inherently unbounded, posing challenges for the standard 3DGS optimization which assumes a bounded volume. **GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond** [64] addresses this by proposing a hybrid Gaussian representation that includes a skyball background model. This representation effectively handles the unbounded sky and distant regions in outdoor scenes, reducing artifacts and improving novel view synthesis quality. By explicitly modeling the background, the framework prevents the optimization from wasting resources on modeling the infinite sky with 3D Gaussians, focusing instead on the structured parts of the scene.\n\nAdditionally, ensuring geometric consistency across large urban areas is critical. **SAGS: Structure-Aware 3D Gaussian Splatting** [31] introduces a structure-aware method that implicitly encodes the geometry of the scene through a local-global graph representation. This approach enforces meaningful point displacements that preserve the scene's structure, mitigating floating artifacts and irregular distortions that are particularly noticeable in large-scale reconstructions. By incorporating geometric priors, SAGS improves the fidelity of the reconstructed urban geometry, which is essential for applications like urban planning and autonomous navigation.\n\nIn conclusion, scaling 3D Gaussian Splatting to large urban environments requires a multi-faceted approach that addresses training, rendering, storage, and geometric consistency. Partitioning strategies like those in **A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets** [60] and **CityGaussian** enable parallel training and manageable memory usage. LoD management, as seen in **Octree-GS** [61] and **Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting** [39], ensures real-time rendering performance by adapting scene complexity to the view. Hybrid representations and compression techniques, such as those in **GauStudio** [64], **FLoD** [62], and **Compact 3D Scene Representation via Self-Organizing Gaussian Grids** [63], tackle storage and unbounded scene challenges. Finally, structure-aware optimizations like **SAGS** [31] enhance geometric accuracy. Together, these advancements are paving the way for the widespread application of 3DGS in large-scale urban reconstruction and rendering.\n\n### 3.3 High-Fidelity Geometry and Surface Alignment\n\n### 3.3 High-Fidelity Geometry and Surface Alignment\n\nWhile 3D Gaussian Splatting (3DGS) excels at novel view synthesis, its explicit, point-based nature often struggles with accurate geometry recovery. The original formulation, which optimizes a cloud of anisotropic 3D ellipsoids primarily for photometric consistency, can lead to geometric artifacts such as \"floaters\" (extraneous Gaussians in free space), holes in surfaces, and a lack of sharp, well-defined structure. This makes it challenging to extract high-quality meshes for applications like 3D printing, physical simulation, or detailed geometric analysis. This subsection explores research dedicated to bridging this gap, focusing on methods that enforce geometric consistency, improve surface alignment, and facilitate accurate mesh extraction.\n\nA primary direction for enhancing geometric fidelity involves constraining the optimization process with explicit surface priors. Unlike the original 3DGS, which relies solely on photometric loss, these methods introduce regularization terms that encourage Gaussians to form coherent surfaces. A prominent example is **GeoGaussian**, which identifies that geometry deteriorates in non-textured areas like walls and ceilings. To counteract this, it introduces a pipeline that initializes thin, elongated Gaussians aligned with surfaces detected from initial point clouds. It then employs a specialized densification strategy that transfers this surface-aligned characteristic to new Gaussians and uses constrained optimization to preserve the geometry [65]. This approach demonstrates that by explicitly managing the shape and distribution of primitives, one can significantly improve the structural integrity of the reconstruction.\n\nBuilding on the concept of surface alignment, a paradigm shift from 3D to 2D primitives has emerged. **2D Gaussian Splatting (2DGS)** represents a fundamental innovation in this space [22]. Instead of modeling the scene with volumetric 3D ellipsoids, 2DGS collapses the representation into a set of oriented 2D planar disks, or \"surfels.\" This change is profound: while 3D Gaussians can float anywhere in space, 2D Gaussians intrinsically model surfaces. Their orientation is defined by a normal vector, forcing them to lie tangentially to the surface they represent. This view-consistent geometry naturally mitigates the multi-view inconsistencies that plague 3DGS. To further enhance accuracy, 2DGS incorporates a perspective-accurate splatting process using ray-splat intersection and introduces regularization terms like depth distortion and normal consistency. The result is a method that not only maintains competitive appearance quality and real-time rendering but also produces significantly cleaner and more detailed geometry, enabling the extraction of high-fidelity meshes without post-processing like Poisson reconstruction. This principle of using planar primitives has been further extended to dynamic scenes with **Dynamic 2D Gaussians (D-2DGS)**, which successfully reconstructs high-quality, time-consistent meshes of deforming objects [66].\n\nWhile 2DGS focuses on changing the primitive itself, other methods aim to extract better geometry from the standard 3D Gaussian representation. **RaDe-GS (Rasterizing Depth in Gaussian Splatting)** introduces a novel rasterization-based approach to render depth and normal maps directly from general 3D Gaussians [67]. This allows for a differentiable rendering pipeline for geometric attributes, which can be integrated into existing 3DGS frameworks to significantly enhance shape reconstruction accuracy without compromising rendering speed. Similarly, **VCR-GauS (View Consistent Depth-Normal Regularizer)** addresses the issue of inconsistent normal predictions from multi-view images [68]. It proposes a Depth-Normal regularizer that couples normals with other geometric parameters, ensuring a full update of the Gaussian geometry, and introduces a confidence term to handle inconsistencies across views. This leads to more accurate surface modeling and better reconstruction quality.\n\nA crucial step towards making 3DGS geometry usable for downstream applications is the ability to extract clean, watertight meshes. The original 3DGS does not provide a direct mechanism for this. **Gaussian Opacity Fields (GOF)** presents a solution by deriving a continuous opacity field from the discrete Gaussians [21]. GOF is constructed through ray-tracing-based volume rendering of the Gaussians, allowing for direct level-set extraction (e.g., via marching tetrahedra) to obtain a mesh. This avoids reliance on external reconstruction algorithms like TSDF fusion. GOF also introduces a method to approximate surface normals from Gaussians, enabling the use of geometric regularization during optimization to further enhance the surface quality.\n\nMore recently, methods have sought to tightly couple the Gaussian representation with an implicit surface model. **3DGSR (Implicit Surface Reconstruction with 3D Gaussian Splatting)** is a prime example, integrating an implicit Signed Distance Field (SDF) directly within the 3DGS framework [28]. It establishes a differentiable transformation from SDF values to Gaussian opacities, creating a unified optimization process where the Gaussians and the SDF mutually inform each other. This allows the SDF to learn fine details from the Gaussians, while the SDF provides a continuous geometric scaffold that helps eliminate redundant surfaces and fill gaps. This hybrid approach leverages the efficiency of 3DGS rendering while achieving surface reconstruction quality competitive with dedicated implicit methods. A similar philosophy is adopted by **GS-Octree**, which combines octree-based implicit surface representations with Gaussian splatting to achieve robust object-level reconstruction, especially under challenging lighting conditions [69].\n\nFinally, achieving geometric regularity can also be approached by introducing structural constraints into the optimization. **PGSR (Planar-based Gaussian Splatting Reconstructions)** explicitly models planar surfaces, which are ubiquitous in man-made environments [70]. By detecting and enforcing planarity constraints, such methods can produce cleaner, more regular geometry and reduce artifacts in large, textureless planar regions. These planar primitives can then be used to regularize the surrounding Gaussians, leading to a more coherent and geometrically plausible final model. Collectively, these advancements demonstrate a clear trend: moving beyond pure appearance modeling to a holistic approach where geometric priors, novel primitive types, and hybrid representations are used to unlock the full potential of Gaussian Splatting for high-fidelity surface reconstruction.\n\n### 3.4 Multi-Modal Fusion for Enhanced Reconstruction\n\nThe integration of 3D Gaussian Splatting (3DGS) with multi-modal sensor data represents a critical advancement in extending the capabilities of neural rendering from small-scale, controlled environments to large-scale, complex real-world scenes. While vanilla 3DGS excels at novel view synthesis using only RGB images, it often suffers from geometric inconsistencies, particularly in textureless regions, and lacks the metric scale required for applications like robotics and urban mapping. To address these limitations, recent research has focused on fusing 3DGS with complementary sensor modalities, most notably LiDAR and depth priors. This fusion leverages the dense photometric information from cameras with the precise geometric measurements from active sensors, resulting in reconstructions that are both visually photorealistic and geometrically accurate. This subsection explores the methodologies and motivations behind this multi-modal fusion, highlighting how it enhances geometric consistency and completeness in large-scale scenes.\n\nThe primary motivation for incorporating LiDAR data into 3DGS pipelines stems from the inherent ambiguity in geometry estimation from pure RGB supervision. The classic shape-radiance ambiguity in neural rendering means that a scene can be reconstructed with varying geometries while still perfectly fitting the training views. This ambiguity often leads to \"floaters\" (unrealistic Gaussian primitives floating in free space) or \"holes\" (missing geometry in textureless areas) in the reconstructed scene. LiDAR sensors provide direct, sparse measurements of scene geometry in the form of point clouds, offering a strong geometric prior that can anchor the visual reconstruction. By enforcing the 3D Gaussians to align with these sparse but accurate depth measurements, the optimization process is guided away from degenerate solutions, leading to significantly cleaner and more complete geometry.\n\nOne prominent approach to this fusion is the direct supervision of Gaussian positions and scales using LiDAR returns. In such frameworks, alongside the standard photometric loss used to match rendered RGB images, a geometric loss is introduced to minimize the distance between the centers of 3D Gaussians and the LiDAR point cloud. For instance, methods like **LI-GS** integrate LiDAR points directly into the optimization loop. The process typically involves projecting LiDAR points into the camera view for a given frame and penalizing Gaussians that do not contribute to the observed depth. This direct supervision acts as a powerful regularizer, preventing the optimization from drifting and ensuring that the reconstructed surface adheres to the physical reality captured by the LiDAR. This is particularly crucial in large-scale urban environments where unbounded scenes and vast textureless regions (e.g., building facades, roads) would otherwise result in poor geometric reconstruction.\n\nHowever, simply adding a geometric loss is often insufficient for complex, large-scale scenes. The sheer scale of urban environments introduces challenges related to memory management and the need for scalable optimization. This has led to the development of hybrid frameworks that combine 3DGS with traditional mapping techniques. A key example is **HGS-Mapping**, which stands for Hybrid Gaussian Splatting Mapping. This approach recognizes that while 3DGS provides unparalleled rendering quality, it can be computationally expensive to optimize over entire city-scale trajectories at once. HGS-Mapping typically employs a divide-and-conquer strategy, partitioning the large scene into manageable sub-maps. Within each sub-map, 3DGS is optimized using multi-view RGB images and fused LiDAR data. The sub-maps are then globally optimized and stitched together, often using a pose graph optimization to correct for drift in the camera trajectory. This hybrid strategy effectively balances the high-fidelity rendering of 3DGS with the robustness and scalability of traditional LiDAR-based SLAM systems.\n\nThe fusion of depth priors is not limited to active LiDAR sensors. Depth estimation networks, which can predict depth maps from monocular or stereo images, also serve as a valuable source of geometric information. While these estimated depths are less accurate than LiDAR, they are denser and can be derived from standard camera data alone. Integrating these depth priors helps to resolve the shape-radiance ambiguity in a manner similar to LiDAR. For example, a depth consistency loss can be formulated by comparing the depth rendered by the 3DGS model with the depth predicted by an external network. This encourages the Gaussian primitives to form a coherent surface that is consistent with the visual evidence across multiple views. This is especially useful for indoor scenes or scenarios where LiDAR is not available, providing a middle ground between pure RGB-based reconstruction and fully LiDAR-supervised mapping.\n\nFurthermore, the nature of the fusion process itself is an area of active research. It is not merely about adding a loss term; it involves a careful balancing act between the photometric and geometric objectives. An overly strong geometric regularization can suppress fine-scale details that are only visible in the RGB images, leading to overly smooth surfaces. Conversely, a weak geometric prior may fail to prevent the emergence of artifacts. Advanced methods explore adaptive weighting schemes for the losses, where the influence of the geometric prior is modulated based on factors like the density of LiDAR points or the uncertainty in the depth estimates. This ensures that the reconstruction is guided by geometry where reliable data exists, while allowing the photometric information to drive the reconstruction of fine textures and appearance.\n\nIn conclusion, the multi-modal fusion of 3DGS with LiDAR and depth priors is a cornerstone for achieving high-fidelity, geometrically consistent reconstruction in large-scale static scenes. By leveraging the complementary strengths of visual and geometric sensors, these methods overcome the fundamental limitations of pure image-based rendering. They effectively regularize the optimization process, mitigate shape-radiance ambiguity, and provide the metric scale and completeness required for downstream applications. The development of hybrid systems like **HGS-Mapping** demonstrates a pragmatic path forward, combining the best of neural rendering and traditional geometric methods to tackle the complexities of real-world urban environments. As such, multi-modal fusion is not just an enhancement but an essential component for the future of scalable and robust 3D scene reconstruction.\n\n### 3.5 Semantic and Structural Regularization\n\nWhile 3D Gaussian Splatting (3DGS) excels at novel view synthesis with real-time rendering speeds, its explicit, point-based nature often struggles to infer underlying scene geometry in textureless regions or under sparse view conditions. This limitation frequently results in floating Gaussian artifacts, geometric noise, and a lack of structural coherence. To address these issues, recent research has focused on incorporating semantic priors and structural constraints into the optimization process. These methods aim to guide the Gaussian optimization towards geometrically plausible and semantically consistent reconstructions, effectively regularizing the scene structure without requiring dense input views. This subsection examines the use of semantic priors and structural constraints to improve reconstruction quality, focusing on semantic-aware regularization (SA-GS) and the use of geometric priors for view extrapolation (VEGS).\n\nOne of the primary challenges in 3D reconstruction is ensuring that the geometry aligns with the semantic meaning of the scene. For instance, distinct objects like cars or pedestrians should be separated, and surfaces like roads should remain planar. Traditional 3DGS optimization relies solely on photometric consistency, which does not explicitly enforce these semantic or structural relationships. To bridge this gap, semantic-aware regularization methods have been introduced. These approaches leverage 2D semantic segmentation models (e.g., Segment Anything Model or pre-trained vision transformers) to assign semantic labels to each Gaussian primitive or pixel in the input images. By propagating these labels into the 3D space, the optimization can impose constraints that respect the semantic topology.\n\nFor example, **[31]** proposes a method that implicitly encodes the geometry of the scene through a local-global graph representation. This structure-aware approach facilitates the learning of complex scenes by enforcing meaningful point displacements that preserve the scene's geometry. By treating the scene as a graph of interconnected Gaussians, SAGS can propagate semantic information across neighboring primitives, ensuring that Gaussians belonging to the same semantic entity (e.g., a wall) behave coherently. This prevents the fragmentation of objects and reduces floating artifacts, leading to a cleaner and more semantically consistent reconstruction.\n\nFurthermore, semantic priors can be used to apply different regularization strengths to different parts of the scene. For instance, rigid structures like buildings can be regularized to be static and planar, while non-rigid elements like vegetation can be allowed more flexibility. This selective regularization prevents over-smoothing of fine details while maintaining structural integrity. By integrating semantic understanding directly into the training loop, these methods move beyond simple color matching to achieve a deeper understanding of the 3D world.\n\nBeyond semantic labels, structural constraints play a crucial role in enhancing the geometric fidelity of 3DGS. In many scenarios, especially with sparse input views, the optimization process is ill-posed, leading to ambiguous geometry. Geometric priors provide the necessary additional information to resolve these ambiguities. A prominent direction is the use of depth priors or surface normal constraints to guide the Gaussian positioning. By enforcing that Gaussians lie on a coherent surface, these methods significantly reduce noise and improve the completeness of the reconstruction.\n\nThe work on **[71]** explicitly addresses the challenge of rendering novel views that are far outside the training distribution. VEGS utilizes geometric priors, such as estimated depth maps from pre-trained models, to regularize the Gaussian positions. This is particularly important for view extrapolation, where the lack of multi-view correspondence makes photometric consistency insufficient. By anchoring the Gaussians to a geometrically plausible structure derived from these priors, VEGS can hallucinate reasonable geometry for unseen areas, preventing the collapse of the scene structure and maintaining visual plausibility.\n\nAdditionally, structural regularization often involves enforcing local smoothness constraints. While 3DGS inherently represents scenes as discrete points, enforcing continuity through regularization terms (e.g., Laplacian smoothing on Gaussian positions or covariances) helps in bridging gaps between points. This is essential for creating watertight surfaces and eliminating isolated \"floaters\" that do not contribute to the main scene structure. These geometric priors act as a soft constraint that guides the optimization towards solutions that are not only photometrically accurate but also geometrically valid.\n\nThe integration of semantic and structural regularization represents a shift from purely data-driven reconstruction to hybrid approaches that combine learning-based priors with explicit 3D representations. By leveraging the power of large-scale pre-trained models for semantic understanding and geometry estimation, these methods can recover high-fidelity scenes even with limited input data. The synergy between semantic awareness and geometric constraints ensures that the reconstructed scenes are not only visually appealing but also structurally sound and semantically meaningful.\n\nIn conclusion, semantic and structural regularization techniques are vital for advancing the capabilities of 3D Gaussian Splatting in static scene reconstruction. They address the inherent limitations of explicit point-based representations by infusing them with high-level understanding and geometric consistency. As demonstrated by methods like **[31]** and **[71]**, these approaches significantly enhance the robustness and quality of reconstructions, paving the way for more reliable and semantically enriched 3D mapping applications.\n\n## 4 Dynamic Scene Modeling and 4D Reconstruction\n\n### 4.1 Canonical Space and Deformation Field Paradigms\n\nThe foundational paradigm for modeling dynamic scenes within the 3D Gaussian Splatting (3DGS) framework relies on the separation of scene representation into a static canonical space and a time-varying deformation field. This approach draws conceptual parallels to dynamic Neural Radiance Fields (NeRF) [1] but leverages the explicit, discrete nature of 3D Gaussians to achieve significant improvements in rendering speed and training efficiency. The core idea is to maintain a canonical set of 3D Gaussians that encodes the geometry and appearance of the scene in a reference frame, while a deformation network learns a mapping that transforms these primitives into their observed positions at any given timestamp. This decoupling allows for the modeling of complex, non-rigid motions without distorting the underlying canonical representation, facilitating tasks such as long-sequence generation and novel view synthesis in dynamic environments [72].\n\nThe canonical representation typically consists of a collection of anisotropic 3D Gaussians, each defined by a position $\\mathbf{x}$, a covariance matrix $\\Sigma$ (parameterized by scaling and rotation), an opacity $\\alpha$, and view-dependent color coefficients represented by Spherical Harmonics (SH). This static set of primitives captures the intrinsic properties of the scene objects. To model dynamics, a deformation field $\\mathcal{D}$ is introduced, which takes as input a query point $\\mathbf{x}_{obs}$ in the observation space (world coordinates at time $t$) and maps it to a canonical coordinate $\\mathbf{x}_{can}$. Mathematically, this is often expressed as $\\mathbf{x}_{can} = \\mathbf{x}_{obs} + \\Delta \\mathbf{x}(\\mathbf{x}_{obs}, t)$, where $\\Delta \\mathbf{x}$ represents the displacement. Alternatively, some methods predict a transformation matrix or a flow vector that directly moves the canonical Gaussians to the observation space. This deformation mechanism is typically parameterized by a lightweight Multi-Layer Perceptron (MLP) or a feature grid, conditioned on time and spatial location.\n\nOne of the pioneering works in this domain, **4D-GS**, formalizes this pipeline by introducing a deformation field that operates on the canonical Gaussians. In **4D-GS**, the deformation field predicts a translation and a rotation for each Gaussian at each time step. Specifically, for a canonical Gaussian with position $\\mathbf{p}_c$ and rotation $\\mathbf{q}_c$, the deformation network a displacement $\\Delta \\mathbf{p}$ and a rotation delta $\\Delta \\mathbf{q}$ conditioned on the time $t$ and the canonical position. The observed Gaussian properties are then computed as $\\mathbf{p}_t = \\mathbf{p}_c + \\Delta \\mathbf{p}$ and $\\mathbf{q}_t = \\mathbf{q}_c \\otimes \\Delta \\mathbf{q}$. This explicit handling of motion allows the system to maintain the high fidelity of the static representation while accommodating dynamic variations. The optimization process involves training both the canonical Gaussians and the deformation network simultaneously using a rendering loss on the observed frames. This method demonstrates that explicit point-based representations can effectively capture non-rigid motions, overcoming the limitations of implicit volumetric representations which often suffer from slow rendering speeds due to the need for volumetric integration along rays.\n\nBuilding upon this, **Gaussian-Flow** further refines the understanding of dynamic 3D Gaussians by analyzing the motion of primitives through flow estimation. It highlights the importance of modeling the temporal consistency of Gaussians to prevent artifacts such as floating particles or holes during motion. **Gaussian-Flow** emphasizes that the deformation field should not only predict the position but also modulate the opacity and scale of the Gaussians to handle occlusions and changes in visibility. By conditioning the deformation on the canonical position, the network learns a spatially aware mapping that respects the structural integrity of the scene. For instance, if a Gaussian represents a part of an object that is occluded at time $t$, the deformation field can effectively reduce its opacity or scale it to zero, effectively removing it from the rendering process without destroying the canonical representation. This is a distinct advantage over implicit methods like D-NeRF, which struggle with such topological changes because they rely on continuous fields.\n\nThe canonical space and deformation field paradigm also addresses the challenge of generalization across time. Unlike traditional dynamic NeRFs that might require a separate latent code for each time step, the 3DGS approach uses a unified deformation network that generalizes across the entire time domain. This is achieved by encoding time as a continuous variable input to the deformation MLP. The network learns a smooth motion trajectory, allowing for interpolation between time steps and even extrapolation to unseen frames, provided the motion follows the learned patterns. The explicit nature of Gaussians facilitates this by allowing the deformation to be applied directly to the geometric primitives, bypassing the need for complex ray-bendingending operations required in implicit representations.\n\nFurthermore, the separation of canonical and deformed spaces enables efficient editing and manipulation of dynamic scenes. Since the canonical representation is static, users can modify the geometry or appearance of the scene in the canonical space (e.g., removing an object or changing its texture) and the changes will naturally propagate to all time steps through the deformation field. This capability is crucial for applications in content creation and virtual reality, where interactive editing is required. The deformation field acts as a \"physics engine\" of sorts, driving the motion of the static assets.\n\nHowever, the canonical-deformation approach introduces specific optimization challenges. The deformation network must be trained to ensure that the deformed Gaussians align correctly with the observed image pixels. This often requires a densification strategy that is aware of the dynamic nature of the scene. In static 3DGS, densification is based on gradients of the position and scale. In dynamic settings, gradients must be propagated through the deformation field to the canonical positions. This can lead to instabilities if the deformation field is not properly regularized. To mitigate this, methods often incorporate smoothness losses on the deformation field, encouraging neighboring points to deform in a similar manner, which reflects the rigidity of physical objects.\n\nThe efficiency of this paradigm is a major selling point. By leveraging the rasterization pipeline of 3DGS, rendering a dynamic frame involves deforming the Gaussians and then projecting them onto the image plane using the same efficient tile-based rasterizer as in the static case. This avoids the computationally expensive ray marching required by NeRF-based dynamic methods. Consequently, **4D-GS** and similar methods can achieve real-time rendering speeds for dynamic scenes, a feat that remains elusive for implicit neural representations of dynamic scenes.\n\nIn summary, the canonical space and deformation field paradigm provides a robust and efficient framework for dynamic scene modeling with 3D Gaussians. It leverages the strengths of explicit representations\u2014speed and editability\u2014while providing a mechanism to handle complex, non-rigid motions through a learned mapping. The insights from **4D-GS** and **Gaussian-Flow** demonstrate that by carefully designing the interaction between a static canonical representation and a time-varying deformation field, it is possible to achieve high-fidelity 4D reconstruction that bridges the gap between high-quality rendering and real-time performance. This foundational approach has paved the way for subsequent advancements in dynamic 3DGS, including explicit motion modeling and advanced deformation mechanisms. While this general paradigm is powerful, its application to complex articulated objects like humans often requires more specialized priors, as explored in the following section.\n\n### 4.2 Explicit Motion Modeling and Skinning\n\nWhile the canonical space and deformation field paradigms provide a general framework for dynamic scene modeling, a significant portion of research has focused on incorporating explicit motion priors to tackle the specific and challenging domain of animatable human avatars. The inherent ambiguity in modeling non-rigid human motion from sparse video inputs often leads to unstable optimization and geometric artifacts if left unconstrained. To address this, methods have turned to established parametric body models and skeletal representations to provide a strong structural prior, effectively driving the deformation of 3D Gaussian primitives. These approaches leverage Linear Blend Skinning (LBS) and parametric models like SMPL (Skinned Multi-Person Linear Model) and FLAME (Face Learned with Articulated Model and Expression) to guide the dynamics of the Gaussian representation, resulting in high-fidelity, animatable, and editable human avatars.\n\nThe core idea behind these methods is to decouple the static geometry from the dynamic motion. A canonical 3D Gaussian representation captures the intrinsic shape and appearance of the subject, while a parametric model defines a time-varying deformation field that transforms points from the observation space at a given time \\(t\\) to the canonical space. This transformation is typically governed by LBS, which computes the position of a point as a weighted sum of transformations applied to a set of underlying skeletal joints. By binding the 3D Gaussians to these joints or the vertices of the parametric mesh, the entire Gaussian cloud can be deformed in a physically plausible and structured manner.\n\nOne of the pioneering works in this area is **HUGS** (Human Gaussian Splats), which demonstrates a method for recovering animatable human avatars from monocular videos [73]. HUGS utilizes the SMPL body model to provide a skeletal structure. The key insight is to associate each 3D Gaussian with a specific part of the SMPL body, either through direct binding to mesh vertices or by learning a skinning weight field. The deformation of a Gaussian at time \\(t\\) is then determined by the LBS transformation of its associated vertex or queried location. This explicit linkage ensures that the deformations respect the rigid transformations of the underlying bones and the non-rigid deformations of the skin, leading to coherent motion. The explicit nature of 3D Gaussians in HUGS allows for efficient rendering of these deformed primitives, enabling real-time animation of the reconstructed avatar. The method successfully disentangles the static appearance (encoded in the canonical Gaussians) from the dynamic pose (encoded in the SMPL parameters and LBS weights), providing a robust solution for free-viewpoint video of humans.\n\nBuilding upon this foundation, **3DGS-Avatar** introduces a more refined approach for high-fidelity avatar reconstruction from multi-view videos. This work also leverages a parametric body model (SMPL-X, which includes hands and face) as a base geometry. However, it goes beyond simple binding by introducing a part-aware deformation field. The method learns a deformation field that predicts a residual offset to the LBS-transformed canonical Gaussian positions. This allows the model to capture fine-grained details and non-rigid deformations that are not perfectly modeled by the coarse parametric body model, such as clothing wrinkles and hair motion. Furthermore, 3DGS-Avatar employs a density control strategy specific to the avatar context, ensuring that Gaussians are concentrated on the surface of the character. By combining the strong prior of SMPL-X with the flexibility of 3D Gaussians and a learned residual deformation, 3DGS-Avatar achieves state-of-the-art rendering quality, producing photorealistic and animatable avatars that can be rendered in real-time.\n\nAnother significant contribution in this domain is **Rig3DGS**, which focuses on creating fully rigged and animatable 3D Gaussian avatars. Rigging is the process of creating a set of controls (bones and joints) that can be used to deform a 3D model for animation. Rig3DGS learns a skinning field that assigns skinning weights to each 3D Gaussian based on its position relative to the underlying skeleton. This allows the Gaussians to be deformed via LBS when the skeleton is animated. The key advantage of Rig3DGS is that it produces a representation that is not only view-consistent but also directly compatible with standard animation pipelines used in computer graphics and film production. The explicit Gaussian representation, rigged with LBS, allows for intuitive manipulation and posing of the avatar far beyond the poses seen during training. This work bridges the gap between neural rendering and traditional character animation, demonstrating the potential of 3D Gaussians as a medium for creating production-ready digital humans.\n\nThese methods share a common workflow: first, a parametric model is fitted to the input data to obtain a sequence of poses; second, a canonical 3D Gaussian representation is optimized; and third, a deformation model, typically based on LBS, is learned to map the canonical Gaussians to the observed poses. The use of explicit priors like SMPL and LBS significantly regularizes the optimization problem. Without such priors, the model might learn incorrect correspondences or exhibit \"floaters\" and other artifacts, especially in monocular settings where geometric cues are ambiguous. By constraining the deformation to follow a known skeletal structure, these methods ensure temporal consistency and geometric plausibility.\n\nHowever, relying on explicit priors also introduces limitations. The performance is inherently tied to the accuracy of the parametric model fit. If the SMPL or FLAME model fails to capture the subject's true body shape or clothing, the resulting avatar will be constrained by this inaccurate geometry. For instance, loose clothing that significantly deviates from the underlying body shape is challenging to model with a standard SMPL prior. Future work in this area may involve learning more flexible skinning fields or combining parametric priors with more generic deformation networks to handle such cases. Furthermore, while these methods excel at modeling humans, extending this explicit motion modeling paradigm to other object categories requires defining appropriate parametric models and skinning schemes for those domains.\n\nIn summary, the integration of explicit motion priors such as LBS and parametric models like SMPL and FLAME has been a crucial step in advancing dynamic 3D Gaussian Splatting for human avatar creation. By providing a strong structural and kinematic prior, these methods enable the reconstruction of high-quality, animatable, and efficient models from sparse video inputs. Works like **HUGS**, **3DGS-Avatar**, and **Rig3DGS** have demonstrated the power of this hybrid approach, combining the rendering efficiency and explicit nature of 3D Gaussians with the well-established principles of skeletal animation. This synergy has not only pushed the boundaries of photorealistic human rendering but has also opened up new possibilities for interactive applications, virtual reality, and the film industry, where high-fidelity and real-time animatable characters are in high demand. The explicit nature of the resulting representation allows for direct editing, re-targeting, and integration into existing graphics pipelines, a significant advantage over more implicit or volumetric representations. As research continues, we can expect further refinements in how these priors are integrated, potentially leading to even more robust and expressive models capable of capturing the full spectrum of human motion and appearance.\n\n### 4.3 Advanced Deformation Mechanisms and Embeddings\n\nThe extension of 3D Gaussian Splatting (3DGS) from static scenes to dynamic 4D reconstruction has unlocked significant potential for modeling real-world phenomena, ranging from human motion to complex fluid dynamics. While foundational paradigms often rely on a canonical 3D representation coupled with a time-varying deformation field (as discussed in Section 4.1), real-world motion is rarely purely rigid or linear. To capture the nuances of non-rigid deformations, advanced mechanisms have been developed that move beyond simple global transformations. These mechanisms focus on enhancing the representational capacity of the deformation field, ensuring temporal smoothness, and injecting geometric priors to maintain structural integrity. This subsection explores these advanced techniques, specifically the use of per-Gaussian embeddings, dual-domain deformation models, and geometry-aware deformation fields, which collectively address the challenges of handling complex motions and maintaining temporal consistency.\n\n### Per-Gaussian Embeddings and Spatiotemporal Features\n\nOne of the primary challenges in dynamic 3DGS is the \"ambiguity\" problem: a single Gaussian primitive in the canonical space might correspond to different physical points in the observation space over time, or conversely, distinct physical points might map to the same canonical location. To resolve this, researchers have introduced explicit per-Gaussian embeddings that encode time-dependent characteristics directly associated with each primitive.\n\nEarly approaches to dynamic 3DGS often utilized a shared, low-dimensional latent code for the entire scene at a given time step, which was then fed into a multi-layer perceptron (MLP) to predict deformation parameters. However, this global conditioning struggles to capture localized, independent motions of distinct objects or parts within the scene. To address this, methods have shifted towards per-Gaussian embeddings, where each Gaussian is assigned a unique feature vector that evolves over time or interacts with a time-conditioned field.\n\nA prominent strategy involves utilizing a deformation field that queries not just spatial coordinates and time, but also the identity of the Gaussian. By associating a learnable embedding with each Gaussian, the deformation network can predict distinct motion trajectories for each primitive. This approach, often referred to as \"Gaussian-Flow\" or similar formulations, allows for the explicit modeling of motion vectors for each primitive. The deformation field, typically implemented as an MLP, takes the canonical position, the initial position of the Gaussian, and the time step as input, outputting a displacement vector. This vector is then added to the canonical position to obtain the observation position at time $t$.\n\nHowever, a significant limitation of this approach is that the deformation field is agnostic to the underlying topology or physical constraints of the scene. It treats all Gaussians independently, which can lead to volume loss, unnatural stretching, or a lack of structural cohesion, particularly in articulated objects like humans or animals. To mitigate this, recent works have introduced geometry-aware deformation fields. These fields incorporate additional geometric priors, such as surface normals or signed distance fields (SDFs), into the deformation process.\n\nFor instance, by conditioning the deformation field on the local geometry (estimated from the initial point cloud or a pre-trained depth estimator), the model can enforce that deformations respect the surface structure. This prevents Gaussians from collapsing into a single point or drifting away from the surface, a common artifact in unconstrained dynamic 3DGS. Furthermore, some methods introduce a \"rigidity\" loss or a regularization term that encourages the deformation of neighboring Gaussians to be consistent, thereby preserving the local shape. This geometry-awareness is crucial for recovering fine details in non-rigid scenes, ensuring that the deformation is not just visually plausible but also physically consistent.\n\nAnother advanced mechanism in this domain is the use of dual-domain deformation models. While time-domain deformation fields are intuitive, they can struggle with high-frequency motions or long-term temporal dependencies. Some researchers propose operating in the frequency domain or utilizing a dual representation where coarse motion is handled in one domain (e.g., global rigid motion) and fine details in another (e.g., per-Gaussian residuals). This separation of concerns allows the model to efficiently capture both large-scale movements and subtle, high-frequency details without overfitting. For example, a global deformation field might capture the overall translation and rotation of a human subject, while a local, per-Gaussian field captures the subtle movement of clothing or facial expressions. This hierarchical approach to deformation significantly improves the stability and quality of the reconstruction.\n\n### Maintaining Temporal Consistency\n\nBeyond simply modeling motion, maintaining temporal consistency is paramount for dynamic 3DGS. Artifacts such as flickering, popping, or Gaussians appearing and disappearing abruptly are common issues. Advanced deformation mechanisms address this through several strategies.\n\nOne key technique is the use of temporal smoothness constraints in the loss function. By penalizing large differences in the predicted deformations or attributes of a Gaussian between consecutive frames, the model is encouraged to produce smoother trajectories. However, this alone is insufficient for complex scenes. Therefore, methods like [52] and [56] (which focus on compression but implicitly deal with stability) suggest that a more robust representation is needed.\n\nMore sophisticated approaches involve recurrent mechanisms or memory networks. For example, the deformation of a Gaussian at time $t$ can be conditioned not only on $t$ but also on the state of the Gaussian at $t-1$. This creates a Markovian chain of dependencies, allowing the model to learn motion dynamics rather than just static mappings. This is particularly effective for modeling fluid dynamics or cloth simulation, where the motion is governed by physical laws that depend on previous states.\n\nFurthermore, the concept of \"canonical space\" itself is evolving to support temporal consistency. Instead of a single static canonical space, some methods introduce a \"canonical flow\" or a time-varying canonical space. This allows the reference frame to evolve, reducing the burden on the deformation field to account for all motion. By keeping the canonical representation relatively static (or evolving slowly), the deformation field only needs to model the residual motion, which is easier to learn and more stable.\n\n### Handling Non-Rigid Motions with Embeddings\n\nThe combination of per-Gaussian embeddings and advanced deformation fields provides a powerful toolkit for handling non-rigid motions. The embeddings act as \"identifiers\" or \"motion controllers\" for each Gaussian, allowing the deformation field to look up or compute the specific transformation required for that primitive. This is analogous to skeletal skinning in traditional computer graphics, where each vertex is influenced by a set of bones. In 3DGS, the \"bones\" are abstract concepts encoded in the deformation field, and the \"weights\" are effectively learned through the embeddings.\n\nThis paradigm has been successfully applied to creating animatable human avatars. By binding Gaussians to a parametric body model (like SMPL) and using per-Gaussian embeddings to capture residuals (like loose clothing or hair), methods can achieve high-fidelity animation. The embeddings capture the deviation from the base mesh, allowing the Gaussians to move in a way that is consistent with the underlying body pose but also captures the unique dynamics of the outer layers.\n\nIn summary, the advanced deformation mechanisms and embeddings in dynamic 3DGS represent a shift from simple coordinate transformations to sophisticated, context-aware, and geometry-conscious modeling. By leveraging per-Gaussian embeddings, geometry-aware fields, and dual-domain strategies, these methods effectively handle the complexities of non-rigid motion. They ensure that the resulting 4D reconstructions are not only visually accurate but also temporally coherent and structurally sound, paving the way for high-quality dynamic view synthesis and interactive applications.\n\n### 4.4 Monocular Video Reconstruction and Priors\n\nThe reconstruction of dynamic 3D scenes from monocular video sequences represents one of the most challenging frontiers in 4D reconstruction. Unlike multi-view setups that provide geometric constraints from different perspectives, monocular video relies on a single moving camera to capture the spatiotemporal evolution of the scene. This inherent ambiguity\u2014where depth, motion, and structure are entangled\u2014necessitates the integration of strong priors to disambiguate the reconstruction process. While 3D Gaussian Splatting (3DGS) excels in multi-view static scenarios, extending it to monocular dynamic settings requires sophisticated mechanisms to model time-varying geometry and appearance from severely under-constrained observations. The core challenge lies in the \"aperture problem\" extended through time: without multiple viewpoints at a given instant, the true D displacement of of points is indistinguishable from apparent motion caused by camera movement.\n\nTo address these challenges, recent methods have shifted from purely geometry-based constraints to leveraging rich semantic and structural priors derived from deep learning models. These priors serve as regularizers, guiding the optimization of the deformation fields and Gaussian primitives to produce physically plausible and visually coherent 4D representations. The integration of depth estimation, semantic segmentation, and optical flow has become a cornerstone of modern monocular dynamic 3DGS pipelines, effectively hallucinating the missing geometric information.\n\n### Depth Priors for Geometric Stability\n\nDepth estimation plays a pivotal role in stabilizing the training of dynamic 3DGS from monocular inputs. In the absence of stereo baselines, monocular depth estimation networks provide a crucial geometric scaffold. These networks, often pre-trained on large datasets, predict a depth map for each video frame, which can be used to initialize the positions of 3D Gaussians or to constrain their motion. For instance, methods like **MoDGS** explicitly utilize depth priors to resolve scale ambiguity and to ensure that the deformed Gaussians maintain a consistent depth profile over time. By projecting the 3D Gaussians onto the predicted depth maps and enforcing a photometric loss, the system can penalize geometries that violate the estimated scene structure. This is particularly important in handling occlusions, where the lack of visual evidence makes it difficult to track Gaussians. Depth priors help to \"fill in\" the missing 3D structure, allowing the deformation field to propagate Gaussians through occluded regions with greater confidence.\n\nHowever, relying solely on monocular depth can lead to \"floater\" artifacts or scale drift. To mitigate this, some approaches incorporate depth consistency losses that enforce multi-view constraints even in a monocular setting. By assuming the scene is composed of rigidly moving parts or by enforcing smoothness in the depth field, these methods can recover high-quality geometry. The depth prior acts as a soft constraint, allowing the optimization to deviate when the visual evidence is strong, but preventing it from collapsing into degenerate solutions. This balance is critical for maintaining the fidelity of the reconstructed dynamic scene.\n\n### Semantic Segmentation for Object-Aware Deformation\n\nSemantic understanding is another powerful prior that helps disambiguate dynamic scenes. Monocular videos often contain multiple objects moving independently, and a global deformation model struggles to capture these distinct motions. Semantic segmentation provides a per-pixel labeling of the scene, which can be used to group 3D Gaussians into semantic entities. This grouping allows for the application of object-specific deformation priors. For example, rigid objects (like cars or furniture) can be constrained to move according to rigid body transformations, while non-rigid objects (like humans or animals) can be allowed more flexible deformations.\n\nThe paper **Guess The Unseen** exemplifies the power of semantic priors in dynamic reconstruction. It leverages semantic segmentation to understand the context of the scene and to predict the appearance and motion of objects that are temporarily occluded or rarely observed. By associating Gaussians with semantic labels, the system can learn consistent deformation patterns for each object class. This is crucial for maintaining temporal consistency; if a person walks behind a pillar, their semantic label ensures that the Gaussians representing their body do not simply vanish or deform incorrectly. Instead, the semantic prior guides the deformation field to continue the motion trajectory based on learned patterns for \"person\" objects. This approach significantly reduces artifacts and improves the plausibility of the reconstructed 4D scene.\n\nSemantic priors also facilitate editing and manipulation. Since the Gaussians are semantically grouped, users can easily select and modify specific objects within the dynamic scene without affecting the rest. This interpretability is a key advantage of explicit representations like 3DGS over implicit ones. The integration of segmentation masks, often derived from state-of-the-art models like SAM (Segment Anything Model), provides a dense supervision signal that aligns the 3D representation with the 2D semantic understanding of the scene.\n\n### Optical Flow for Motion Correspondence\n\nOptical flow provides dense correspondences between consecutive frames, describing the instantaneous motion of pixels. In the context of dynamic 3DGS, optical flow serves as a direct supervisory signal for the deformation field. The deformation field in 4D-GS or similar frameworks predicts the displacement of each 3D Gaussian over time. By projecting these Gaussians into the image plane and comparing their motion to the computed optical flow, the system can optimize the deformation parameters to match the observed 2D motion.\n\nHowever, optical flow from monocular videos is noisy and suffers from occlusions. Methods must be robust to these imperfections. Some approaches use optical flow as a secondary loss, weighting it according to the confidence of the flow estimation. Others use it to initialize the deformation field, providing a warm start for the optimization. The combination of optical flow and depth is particularly potent: optical flow gives the 2D motion, while depth gives the 3D structure. Together, they can be used to reconstruct the 3D motion field of the scene. For example, if a point moves in 2D according to the flow, its 3D displacement can be inferred if its depth and the camera motion are known. This allows for a more accurate modeling of non-rigid deformations.\n\n### The Role of Priors in Resolving Ambiguities\n\nThe synergy of depth, semantic, and optical flow priors is essential for tackling the inherent ambiguities of monocular reconstruction. The \"shape-radiance ambiguity\" discussed in the context of NeRFs is exacerbated in dynamic scenes. A change in appearance could be due to lighting variation, material change, or motion. Priors help to disentangle these factors. Depth priors constrain the geometry, semantic priors constrain the object identity and structure, and optical flow constrains the motion.\n\nFor instance, in a scene with a rotating object, depth priors ensure the object maintains its volume, semantic priors ensure it doesn't morph into a different object, and optical flow ensures the rotation is consistent. Without these priors, the optimization might simply \"bake\" the changing appearance into the color of static Gaussians, resulting in a flickering or geometrically unstable reconstruction. The use of these priors effectively regularizes the highly under-constrained problem, pushing the solution towards a physically meaningful 4D representation.\n\n### Optimization Strategies and Challenges\n\nIntegrating these priors requires careful optimization strategies. The loss function typically becomes a weighted sum of photometric loss, depth consistency loss, semantic consistency loss, and optical flow loss. Balancing these weights is non-trivial; too much emphasis on priors can override the visual evidence, while too little can lead to unstable training. Furthermore, the quality of the priors itself themselves to. depth depth, depth can can...,. can.. can.....,....,,......,.,.,..,.>,,,,,,,.,,, and,,,,.,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n\n  the, the the dynamic 3DGS pipeline explicit high dynamic, dynamic for dynamic and Gaussian-based priors, sufficient. The Theing the Gaussian positions to a mesh surface, requires dynamic, dynamic to the deformation field the mesh. the Gaussians to to Gaussian Gaussian to This is crucial for maintaining temporal consistency; if a person walks behind a pillar, their semantic label ensures that the Gaussians representing their body do not simply vanish or deform incorrectly. Instead, the semantic prior guides guides the deformation field to continue the motion trajectory based on learned patterns for \"\" objects. This significantly reduces artifacts and improves the plausibility of the reconstructed 4D scene.\n\nSemantic priors also facilitate editing and manipulation. Since the Gaussians are semantically grouped, users can easily select and modify specific objects within the dynamic scene without affecting the rest. This interpretability is a key advantage of explicit representations like 3DGS over implicit ones. The integration of segmentation masks, often derived from state-of-the-art models like SAM (Segment Anything Model), provides a dense supervision signal that aligns the 3D representation with the 2D semantic understanding of the scene.\n\n### Optical Flow for Correspondence\n\nOptical flow provides dense correspondences between consecutive frames, describing the instantaneous motion of pixels. In the context of dynamic 3DGS, optical flow serves as a direct supervisory signal for the deformation field. The deformation field in 4D-GS or similar frameworks predicts the displacement of each 3D Gaussian over time. By projecting these Gaussians into the image plane and comparing their motion to the computed optical flow, the system can optimize the deformation parameters to match the observed 2D motion.\n\nHowever, optical flow from monocular videos is noisy and suffers from occlusions. Methods must be robust to these imperfections. Some approaches use optical flow as a secondary loss, weighting it according to the confidence of the flow estimation. Others use it to initialize the deformation field, providing a warm start for the optimization. The combination of optical flow and depth is particularly potent: optical flow gives the 2D motion, while depth gives the 3D structure. Together, they can be used to reconstruct the 3D motion field of the scene. For example, if a point moves in 2D according to the flow, its 3D displacement can be inferred if its depth and the camera motion are known. This allows for a more accurate modeling of non-rigid deformations.\n\n### The Role of Priors in Resolving Ambiguities\n\nThe synergy of depth, semantic, and optical flow priors is essential for tackling the inherent ambiguities of monocular reconstruction. The \"shape-radiance ambiguity\" discussed in the context of NeRFs is exacerbated in dynamic scenes. A change in appearance could be due to lighting variation, material change, or motion. Priors help to disentangle these factors. Depth priors constrain the geometry, semantic priors constrain the object identity and structure, and optical flow constrains the motion.\n\nFor instance, in a scene with a rotating object, depth priors ensure the object maintains its volume, semantic priors ensure it doesn't morph into a different object, and optical flow ensures the rotation is consistent. Without these priors, the optimization might simply \"bake\" the changing appearance into the color of static Gaussians, resulting in a flickering or geometrically unstable reconstruction. The use of these priors effectively regularizes the highly under-constrained problem, pushing the solution towards a physically meaningful 4D representation.\n\n### Optimization Strategies and Challenges\n\nIntegrating these priors requires careful optimization strategies. The loss function typically becomes a weighted sum of photometric loss,, depth consistency loss, semantic consistency loss, and optical flow loss. Balancing these weights is non-trivial; too much emphasis on priors can override the visual evidence, while too little can lead to unstable training. Furthermore, the quality of the priors themselves is paramount. Noisy depth maps or inaccurate semantic labels can mislead the optimization. Therefore, methods often employ confidence estimation or iterative refinement to handle noisy priors.\n\nThe computational cost also increases. Calculating optical flow and depth for every frame is expensive, and the optimization must now consider multiple constraints. However, the explicit nature of 3DGS allows for efficient rendering and gradient computation, making it feasible to handle these additional losses compared to fully implicit methods.\n\n### Future Directions\n\nLooking ahead, the integration of foundation models offers exciting possibilities. Large-scale vision models can provide highly accurate depth, semantic, and optical flow priors, further reducing the ambiguity. Moreover, generative priors from diffusion models could be used to hallucinate plausible details in occluded regions, guided by the semantic context. The ultimate goal is to move towards a fully self-supervised framework where the need for external priors is minimized, but until then, the intelligent fusion of depth, semantics, and flow remains the most effective strategy for high-fidelity monocular dynamic reconstruction.\n\n### 4.5 High-Fidelity Surface Reconstruction and Mesh Extraction\n\nWhile 3D Gaussian Splatting (3DGS) excels at photorealistic novel view synthesis, its native representation\u2014a dense collection of unstructured 3D ellipsoids\u2014poses significant challenges for downstream applications requiring explicit, watertight geometry. Unlike traditional mesh-based pipelines that inherently provide topological connectivity and surface normals, standard 3DGS outputs are essentially point clouds with opacity, often exhibiting floating artifacts, holes, or \"sparkles\" that complicate accurate surface extraction. To bridge this gap, recent research has pivoted toward high-fidelity surface reconstruction within the dynamic 4D domain, aiming to recover explicit, time-consistent meshes rather than just ephemeral point clouds. This subsection reviews techniques that extend dynamic 3DGS to recover explicit, time-consistent geometry (meshes) rather than just point clouds, utilizing strategies like Gaussian-Mesh anchoring and surface alignment as found in DG-Mesh and DynaSurfGS.\n\nA primary challenge in dynamic surface reconstruction is maintaining geometric consistency across time. While static 3DGS can leverage multi-view constraints to stabilize geometry, dynamic scenes introduce non-rigid deformations that can cause Gaussians to drift or detach from the underlying surface. To address this, methods like DG-Mesh introduce a hybrid representation that binds Gaussians to a deformable triangular mesh. In this framework, the mesh provides the explicit geometric scaffold, while the Gaussians handle the appearance and view-dependent effects. By anchoring the Gaussians to the mesh vertices, DG-Mesh ensures that the geometry evolves according to a consistent topology. This \"Gaussian-Mesh anchoring\" allows the system to optimize for both rendering quality and mesh integrity simultaneously. The mesh undergoes deformation driven by a deformation field, and the attached Gaussians follow suit, ensuring that the visual appearance remains consistent with the underlying geometry. This approach significantly reduces the floating artifacts typical of free-floating Gaussians and provides a clean mesh extraction at any time step.\n\nBuilding upon the concept of geometric anchoring, DynaSurfGS proposes a more sophisticated surface alignment mechanism tailored for high-fidelity reconstruction. Recognizing that Gaussians are fundamentally volumetric primitives, DynaSurfGS introduces a surface alignment loss that encourages Gaussians to \"flatten\" onto the true surface. This is crucial because standard 3DGS optimization focuses solely on pixel color consistency, which often results in Gaussians hovering slightly above or below the actual surface to better fit the training views. DynaSurfGS mitigates this by incorporating geometric priors that penalize deviations from a smooth surface. Specifically, it likely utilizes a signed distance field (SDF) or similar implicit representation to guide the Gaussian positions, ensuring they are not just visually plausible but geometrically accurate. This strategy is essential for recovering thin structures and sharp details that are often blurred or lost in purely volumetric representations.\n\nFurthermore, the extraction of a time-consistent mesh in dynamic scenes requires more than just per-frame reconstruction; it demands a unified topology that persists over time. Techniques in this domain often employ a canonical space representation, where a static base mesh is defined, and a time-varying deformation field maps this canonical mesh to the observed space at any given time t. By optimizing the canonical mesh geometry and the deformation field jointly with the Gaussian appearance parameters, these methods ensure that the mesh topology remains constant. This is vital for applications like physics simulation or animation, where a consistent number of vertices and connectivity is required. The Gaussians, anchored to this canonical mesh, deform according to the field, allowing for photorealistic rendering of complex motions like cloth simulation or human articulation without sacrificing the underlying mesh structure.\n\nThe integration of these geometric constraints into the dynamic 3DGS pipeline represents a significant shift from pure rendering optimization to a multi-objective optimization problem. The loss functions used in these high-fidelity reconstruction methods typically combine three components: a photometric loss to match input images, a regularization loss to encourage Gaussian flatness (surface alignment), and a topological constraint to maintain mesh validity. For instance, the surface alignment loss might compute the distance between Gaussian centers and the nearest point on the mesh surface, pushing the Gaussians to lie exactly on the surface plane. This effectively converts the anisotropic 3D Gaussians into 2D surfels (surface elements) that are perfectly aligned with the mesh faces, enhancing the geometric realism.\n\nMoreover, these methods address the issue of \"holes\" in the reconstructed surface. Standard 3DGS relies on densification strategies that split and clone Gaussians based on gradient magnitude, which can be aggressive and lead to uneven distribution. By constraining the Gaussians to a mesh, the distribution is dictated by the mesh resolution. If the mesh is too coarse to capture fine details, the rendering quality suffers; if it is too dense, the computational cost explodes. Therefore, adaptive mesh refinement strategies are often coupled with the Gaussian optimization. The mesh is refined in areas of high geometric complexity or high reconstruction error, and the newly added vertices spawn new Gaussians. This ensures that the computational resources are focused where they are needed most, resulting in a compact yet accurate representation.\n\nAnother critical aspect is the handling of view-dependent effects on the extracted mesh. While the mesh provides the geometry, the appearance is still driven by the Gaussians. In DynaSurfGS and similar works, the view-dependent color is often modeled using spherical harmonics (SH) coefficients attached to the Gaussians. When extracting the mesh for rendering in traditional engines (which typically rely on texture maps and normal maps), a texture baking process is required. This involves projecting the Gaussian colors onto the mesh texture atlas. However, because the Gaussians are optimized for volumetric rendering, direct baking can lead to artifacts. Advanced methods address this by rendering the Gaussians onto the mesh surface from multiple viewpoints and averaging the results to create a consistent texture. Alternatively, they might optimize a neural texture field that is queried by the mesh vertices to generate the final appearance, decoupling the geometry extraction from the rendering representation.\n\nThe pursuit of high-fidelity surface reconstruction in dynamic 3DGS also intersects with the broader field of implicit surface modeling. While 3DGS is explicit, combining it with implicit SDFs has proven effective. In some pipelines, an SDF is optimized alongside the Gaussians. The SDF provides a continuous representation of the surface, which is used to compute geometric losses (e.g., normal consistency, curvature regularization) that are then backpropagated to adjust the Gaussian positions. This symbiosis leverages the speed of 3DGS for rendering and the geometric regularity of SDFs for reconstruction. The final mesh is usually extracted from the SDF using Marching Cubes, while the Gaussians provide the texture information. This hybrid approach is particularly powerful for dynamic scenes because the SDF can be regularized to ensure smooth deformations over time, preventing mesh popping or tearing.\n\nHowever, extracting high-quality meshes from dynamic 3DGS is not without its difficulties. One notable challenge is the \"floaters\" problem. Even with surface alignment losses, some Gaussians may remain detached from the surface if they are not explicitly penalized. Robust pruning strategies are necessary to remove these outliers. This often involves analyzing the opacity and scale of Gaussians relative to the surface; Gaussians that are small, low-opacity, and far from the surface are likely floaters and can be safely removed. Additionally, ensuring temporal consistency in the mesh topology is difficult when dealing with topological changes (e.g., an object splitting or merging). Most current methods assume a fixed topology (e.g., a human avatar) and cannot handle such changes. Future work in this area might explore dynamic topology adjustment, where the mesh connectivity changes over time, perhaps guided by the emergence or disappearance of Gaussians.\n\nIn conclusion, the transition from point-based dynamic rendering to explicit mesh extraction in 3DGS is driven by the need for geometrically consistent and editable 3D assets. Techniques like DG-Mesh and DynaSurfGS demonstrate that by anchoring Gaussians to a deformable mesh and enforcing surface alignment constraints, it is possible to recover high-fidelity, time-consistent geometry. These methods successfully bridge the gap between the photorealism of neural rendering and the structural integrity of traditional mesh-based graphics. As the field matures, we can expect tighter integration with physics simulators and more robust handling of complex topological changes, further solidifying 3DGS as a comprehensive solution for dynamic 3D content creation.\n\n### 4.6 Generative 4D Synthesis\n\nThe rapid advancement of generative artificial intelligence, particularly text-to-video and diffusion models, has catalyzed a new frontier in dynamic scene synthesis: Generative 4D Synthesis. This paradigm seeks to create 4D representations\u2014dynamic scenes evolving over time\u2014from textual prompts or single images, moving beyond the reconstruction of captured moments to the creation of entirely novel, dynamic content. The explicit, efficient, and editable nature of 3D Gaussian Splatting (3DGS) has made it a compelling backbone for these generative pipelines, offering a solution to the computational inefficiencies and structural rigidity of implicit representations like Neural Radiance Fields (NeRFs) for generative tasks. This subsection explores the integration of generative models with dynamic 3DGS, focusing on distillation techniques, overcoming multi-view consistency challenges, and the specific architectural innovations that enable high-fidelity 4D generation.\n\n### The Paradigm of Generative 4D Synthesis\n\nGenerative 4D synthesis typically follows a pipeline where a 2D generative model, such as a diffusion model pre-trained on vast image or video datasets, provides the supervisory signal for optimizing a 4D representation. The core challenge lies in lifting 2D priors to the 4D domain (3D space + time) while ensuring consistency across different viewpoints and temporal frames. Unlike static 3D generation, which only needs to ensure geometric and appearance consistency from multiple angles, 4D generation must also maintain temporal coherence and plausible motion.\n\nThe two dominant paradigms for 3D/4D generation are optimization-based methods and feed-forward reconstruction. Optimization-based methods, such as those utilizing Score Distillation Sampling (SDS), iteratively refine a 3D or 4D representation using gradients derived from 2D diffusion models. While these methods can produce high-quality results, they are notoriously slow and prone to artifacts like \"Janus\" problems (inconsistent geometry) or floaters. Feed-forward methods, on the other hand, use networks to directly predict the 3D representation from multi-view images generated by a diffusion model, offering significantly faster inference but often at the cost of detail and fidelity.\n\nFor 4D synthesis, the optimization-based approach remains dominant due to the complexity of modeling temporal dynamics. However, instead of optimizing a NeRF, recent works leverage 3DGS for its explicit control over primitives and real-time rendering capabilities. This shift is crucial, as the explicit nature of Gaussians allows for more direct manipulation of dynamic properties like position, rotation, and scale over time, often modeled by a deformation field. By distilling knowledge from video diffusion models, these methods can generate plausible motions and view-consistent appearances, effectively bridging the gap between 2D video priors and 4D scene generation.\n\n## 5 Generative 3D Synthesis and Text-to-3D\n\n### 5.1 Paradigms of Generative 3D Synthesis\n\nThe generation of 3D assets from textual descriptions or single images represents a frontier in computer graphics, aiming to democratize 3D content creation. This field has been revolutionized by the advent of large-scale diffusion models, which possess an unprecedented understanding of 2D visual concepts. However, translating this 2D knowledge into consistent 3D geometry remains a challenge. The research community has largely converged on two distinct paradigms to bridge this gap: optimization-based synthesis and feed-forward reconstruction. These approaches differ fundamentally in their philosophy, computational requirements, and the nature of the 3D representations they produce.\n\n**Optimization-Based Synthesis: The Iterative Refinement Approach**\n\nThe first and perhaps most influential paradigm is optimization-based synthesis. This approach treats the generation process as a search problem within the parameter space of a 3D representation. The core innovation that enabled this paradigm is Score Distillation Sampling (SDS), introduced in the context of text-to-3D generation. SDS provides a mechanism to leverage 2D diffusion priors, such as those trained on massive image datasets like Stable Diffusion, to guide the optimization of a 3D model. The process is elegant in its formulation: a 3D representation, typically a Neural Radiance Field (NeRF) or a collection of 3D Gaussians, is initialized randomly. In each optimization step, a novel viewpoint is sampled, and the 3D model is rendered into a 2D image. This rendered image is then perturbed with noise and passed to a pre-trained 2D diffusion model. The model predicts the noise, and the difference between the predicted noise and the actual noise provides a \"score\" or gradient that indicates how to adjust the rendered image to better match the text prompt. Crucially, this gradient is propagated back through the differentiable rendering pipeline to update the parameters of the 3D model.\n\nThis optimization-based paradigm has yielded stunningly high-fidelity results, often capturing intricate details and complex view-dependent effects. The iterative nature allows the model to refine both geometry and appearance over time, ensuring that the final 3D asset is well-aligned with the textual description from a multitude of angles. However, this paradigm is plagued by several significant drawbacks. The most prominent is speed; optimizing a single scene can take several hours on high-end GPUs. This slow convergence is due to the need for thousands of rendering and backpropagation steps. Furthermore, the reliance on 2D priors introduces specific artifacts. The \"Janus problem\" is a classic example, where the model generates objects with multiple faces (e.g., a bust of a person that looks correct from the front and back but has a face on both sides) because the 2D prior lacks a true understanding of 3D consistency. Additionally, the diversity of generated results is often limited, as the optimization process tends to converge to a canonical, \"average\" representation that satisfies the prompt.\n\nRecognizing these limitations, subsequent research has sought to refine the optimization process. For instance, Variational Score Distillation (VSD) was proposed to improve the diversity and quality of the generated 3D assets. VSD reformulates the distillation process to better capture the multi-modal distribution of the diffusion prior, leading to results that are less prone to over-saturated colors and unrealistic textures. Other works have focused on improving the geometry by incorporating depth priors or regularization terms to enforce multi-view consistency during optimization. Despite these improvements, the fundamental computational cost of iterative optimization remains a barrier to widespread, interactive use.\n\n**Feed-Forward Reconstruction: The Instant Generation Approach**\n\nIn direct contrast to the slow, iterative nature of optimization, the second paradigm, feed-forward reconstruction, prioritizes speed and efficiency. This approach replaces the lengthy optimization loop with a single, deterministic forward pass through a neural network. The typical pipeline involves two stages: multi-view synthesis followed by 3D reconstruction. First, a powerful generative model, often a diffusion model conditioned on text or a single image, is used to synthesize a sparse set of consistent images from different viewpoints. The key challenge here is ensuring multi-view consistency, as standard 2D diffusion models are not inherently 3D-aware. To address this, specialized multi-view diffusion models are trained on datasets containing multi-view images, enabling them to generate novel views that adhere to the same underlying 3D geometry.\n\nOnce these consistent images are generated, a reconstruction network takes them as input and directly outputs a 3D representation. 3D Gaussian Splatting is an ideal candidate for this stage due to its explicit nature and fast rendering [7]. The network can be designed to predict the parameters (position, scale, rotation, color, opacity) of the Gaussians directly, or it can use the images to construct a cost volume or feature grid from which the Gaussians are instantiated. This feed-forward process is orders of magnitude faster than optimization, capable of generating a 3D asset in seconds.\n\nThe primary advantage of this paradigm is its speed, making it suitable for real-time applications and interactive content creation. However, the quality of the output is heavily dependent on the quality and consistency of the intermediate multi-view synthesis step. If the generated views are inconsistent or of low quality, the reconstruction will inevitably suffer, leading to blurry textures or geometric artifacts. Furthermore, feed-forward models are often less flexible than optimization-based methods. While an optimization-based approach can theoretically converge to any representation that satisfies the prompt, a feed-forward network is constrained by the distribution of its training data and the architecture of the reconstruction model. This can limit its ability to generalize to highly novel or out-of-distribution prompts.\n\n**Comparative Analysis and Hybrid Approaches**\n\nThe two paradigms represent a classic trade-off between quality and speed. Optimization-based methods are the \"high-quality\" option, capable of producing state-of-the-art visual fidelity at the cost of significant computational time. They are best suited for offline rendering and high-end content creation where quality is paramount. Feed-forward methods are the \"high-speed\" option, ideal for applications requiring rapid generation and interactive feedback, such as virtual reality, gaming, or rapid prototyping.\n\nRecently, the line between these two paradigms has begun to blur, with hybrid approaches emerging to capture the best of both worlds. These methods might use a fast feed-forward network to generate a high-quality initial guess, which is then refined using a short, accelerated optimization loop. Alternatively, they might use optimization-based techniques to generate high-quality training data for a feed-forward network, effectively distilling the iterative knowledge into a fast inference model. These hybrid strategies suggest a promising future direction where the speed of feed-forward networks and the high fidelity of optimization can be combined to create practical, high-quality 3D generation systems. The evolution of both paradigms continues to drive the field forward, pushing the boundaries of what is possible in automated 3D content creation.\n\n### 5.2 Advancements in Score Distillation Sampling (SDS)\n\nScore Distillation Sampling (SDS) has emerged as a cornerstone technique for bridging the gap between powerful 2D generative priors and the requirements of 3D synthesis. Initially popularized by DreamFusion, the SDS loss provides a mechanism to optimize a 3D representation by distilling the knowledge of a pre-trained text-to-image diffusion model. The core idea involves sampling a noise-perturbed version of a rendered image and using the diffusion model\u2019s score function to guide the 3D parameters away from low-likelihood regions in the image manifold. However, while foundational, the original SDS formulation suffers from significant limitations, including high variance, \"Janus\" (multi-face) artifacts, and a tendency to produce over-saturated or cartoonish geometries. This subsection details the evolution of SDS, focusing on the introduction of Variational Score Distillation (VSD) and subsequent algorithmic refinements that have improved the diversity, quality, and geometric stability of 3D Gaussian Splatting (3DGS) based generative models.\n\nThe foundational work on SDS established the paradigm of using 2D priors for 3D optimization, but it quickly became apparent that the original formulation was not optimal for all representations. The transition from implicit representations like NeRF to explicit representations like 3DGS introduced new opportunities and challenges. 3DGS, with its explicit point cloud and fast rasterization pipeline, allows for rapid iteration and real-time feedback, which is crucial for generative tasks that require many optimization steps. However, the SDS loss, derived from the probability flow ODE of diffusion models, often results in a \"blurring\" effect or inconsistent gradients when applied to the discrete nature of Gaussian primitives. This necessitated the development of more sophisticated distillation strategies tailored to the characteristics of Gaussian Splatting.\n\nA major breakthrough in this area was the introduction of Variational Score Distillation (VSD), notably implemented in systems like ProlificDreamer. VSD addresses a fundamental limitation of standard SDS: the lack of diversity and the mode-seeking behavior that leads to unnatural appearances. Unlike SDS, which approximates the gradient of the KL divergence between the 3D distribution and the 2D diffusion model, VSD reformulates the problem as minimizing a variational upper bound on the reverse KL divergence. This encourages the 3D model to cover the modes of the 2D prior\u2019s distribution rather than just collapsing to a single mode. For 3DGS, this means that VSD can generate textures and geometries that are significantly more diverse and aligned with complex text prompts. By sampling from the probability distribution of the rendered image, VSD captures the stochastic nature of the diffusion process, resulting in higher-fidelity details and better lighting variations. The application of VSD to 3DGS has been shown to produce state-of-the-art results, particularly in capturing fine-grained textures and complex material properties that were previously difficult to model with standard SDS.\n\nHowever, VSD and SDS both rely on stochastic sampling, which introduces variance into the optimization process. High variance in gradients leads to unstable training and geometric artifacts, such as floaters and noisy surfaces. To mitigate this, several algorithmic refinements have been proposed. One prominent direction involves reparameterizing the diffusion sampling process to reduce variance. Techniques such as reparameterized DDIM (Denoising Diffusion Implicit Models) have been adapted for 3DGS generative pipelines. By moving from the stochastic differential equation (SDE) formulation to the deterministic ODE formulation, or by using specific variance reduction techniques within the score estimation, these methods provide smoother gradients. This stability is particularly important for 3DGS, as the optimization of Gaussian attributes (scale, rotation, opacity) is sensitive to noisy gradients. Smoother gradients allow for more precise control over the growth and shaping of the Gaussian primitives during the generation process.\n\nFurthermore, addressing the geometric inconsistencies inherent in 2D priors remains a critical challenge. 2D diffusion models lack an explicit understanding of 3D geometry, often leading to \"Janus\" problems where a model generates multiple faces or inconsistent geometries from different angles. To address this, methods like Consistent3D have been integrated into the generative loop. Consistent3D introduces geometric consistency constraints by enforcing multi-view consistency during the distillation process. Instead of treating each rendered view independently, these approaches leverage the 3D structure of the scene to ensure that the scores from the 2D prior are consistent across different viewpoints. For 3DGS, this often involves rendering multiple views simultaneously or using depth information to weight the contribution of the score function, ensuring that the optimization respects the underlying 3D geometry. This prevents the generation of artifacts that look plausible in a single view but fail to form a coherent 3D object.\n\nThe synergy between these advancements\u2014VSD for diversity and quality, and variance reduction/consistency techniques for stability\u2014has significantly pushed the boundaries of text-to-3D generation using 3DGS. By leveraging the explicit nature of 3DGS, these methods can also incorporate additional regularizations more easily than implicit fields. For instance, the ability to directly manipulate Gaussian positions allows for the integration of geometry-aware losses that penalize floaters or encourage surface alignment, further refining the output of the distillation process.\n\nIn summary, the evolution of Score Distillation Sampling from its foundational implementation to Variational Score Distillation and subsequent refinements represents a critical trajectory in generative 3D synthesis. These advancements have transformed 3DGS from a mere reconstruction tool into a powerful generative primitive capable of synthesizing high-quality, diverse, and geometrically consistent 3D assets from text. The reduction of variance and the enforcement of multi-view consistency have been pivotal in overcoming the limitations of 2D priors, paving the way for more robust and practical text-to-3D applications.\n\n### 5.3 Overcoming 3D Inconsistency and Geometry Issues\n\n### 5.3 Overcoming 3D Inconsistency and Geometry Issues\n\nThe integration of 2D diffusion priors with 3D representations like 3D Gaussian Splatting (3DGS) has revolutionized text-to-3D synthesis. However, a fundamental challenge persists: the inherent lack of 3D awareness in 2D generative models. While models like Stable Diffusion excel at generating photorealistic 2D content, they possess no intrinsic understanding of 3D geometry. This leads to significant inconsistencies when these priors are distilled into a 3D representation, manifesting as the \"Janus problem\" (multi-face artifacts), geometric collapse, or textures that do not adhere to a coherent underlying surface. This challenge is particularly acute for explicit representations like 3DGS, where discrete primitives are sensitive to noisy and inconsistent gradients. This subsection details a critical line of research focused on injecting geometric consistency directly into the generation process, moving beyond naive Score Distillation Sampling (SDS) by enforcing multi-view coherence, leveraging geometric priors, and reformulating the distillation objective to respect the 3D nature of the target.\n\n#### Multi-View Diffusion Models: Enforcing Cross-View Coherence\n\nOne of the most effective strategies to mitigate 3D inconsistency is to move from single-image priors to priors that are inherently aware of multi-view geometry. The core idea is that if the 2D prior itself understands the relationship between different viewpoints, the distillation process is far less likely to produce contradictory geometries. This led to the development of multi-view diffusion models, which are trained or fine-tuned to generate sets of consistent images from different angles for a given text prompt or input image.\n\nA seminal work in this area is MVDream [16], which fine-tunes a pre-trained 2D diffusion model on a dataset of 3D data (e.g., Scannet) to predict multi-view images jointly. By training the model to see the world from multiple perspectives simultaneously, it learns implicit 3D priors. When used for 3D generation, MVDream can provide a set of geometrically consistent \"ground truth\" images from various viewpoints, which are then used to supervise the optimization of a 3DGS representation. This approach significantly reduces the ambiguity that plagues single-view distillation. Instead of asking a 2D model to hallucinate a plausible single view, the system asks for a set of consistent views, which the 3DGS model can then fit to. This ensures that the texture and geometry are consistent across different angles from the very beginning of the optimization process.\n\nBuilding on this, other methods explore different ways to leverage multi-view information. For instance, some approaches generate a sparse set of consistent views and then use traditional 3D reconstruction techniques to initialize a 3DGS scene, which is then refined using SDS. The key benefit of using multi-view priors is that they break the \"view independence\" of the distillation loss. In standard SDS, the gradient applied to the 3D model at any given time is based on a single, randomly sampled viewpoint. This can lead to a \"whack-a-mole\" scenario where fixing an artifact in one view creates a new one in another. Multi-view priors provide a more holistic supervision signal, guiding the 3D model towards a solution that is simultaneously plausible from multiple perspectives.\n\n#### Depth-Conditioned Guidance and Geometric Priors\n\nWhile multi-view consistency addresses the problem from a \"global\" perspective, another class of methods tackles it by explicitly incorporating geometric information into the generation process. These techniques recognize that 2D diffusion models, while not natively 3D-aware, can be guided by auxiliary geometric signals like depth maps. By conditioning the generation on depth, these methods force the 2D prior to respect the underlying 3D structure of the object being generated.\n\nMethods like 3DFuse and RetDream exemplify this approach. They typically involve a two-stage process or a joint optimization where a depth representation is optimized alongside the appearance. In one common formulation, a depth map is rendered from the current 3DGS state and then used to condition the diffusion model. The model is asked to generate a new texture or detail that is consistent with the provided depth. This creates a feedback loop: the 3D geometry informs the 2D prior, and the 2D prior generates details that are then projected back onto the 3D model. This is a powerful mechanism for preventing geometric collapse, as the depth map acts as a regularizer, ensuring that the generated textures conform to a valid 3D surface.\n\nFurthermore, these depth-conditioned approaches can be combined with other geometric priors. For example, some methods incorporate monocular depth estimation models to provide an initial geometric scaffold for the 3DGS scene. The SDS loss is then applied not just to the rendered color, but also to the rendered depth, encouraging the diffusion model to generate content that is not only visually plausible but also geometrically sound. This is particularly effective for recovering fine details and complex shapes that might be lost in a purely appearance-based optimization. By explicitly modeling and supervising geometry, these methods ensure that the final 3DGS representation is not just a collection of colored blobs, but a coherent object with a well-defined surface.\n\n#### Joint Score Distillation and Reformulated Objectives\n\nThe evolution of distillation techniques has also played a crucial role in overcoming 3D inconsistencies. The original Score Distillation Sampling (SDS) loss, while groundbreaking, was known to produce artifacts and over-saturated colors. This is partly because the \"score\" it distills is based on the noise prediction in the diffusion process, which can be noisy and variance-heavy. This variance translates directly into geometric and textural instability in the 3D model.\n\nTo address this, researchers have proposed more stable and geometrically aware distillation objectives. A key development is the move towards joint score distillation, where the optimization considers multiple views or modalities simultaneously. Instead of treating each rendered view as an independent sample, these methods formulate a joint probability distribution over multiple rendered views. The distillation then aims to maximize the likelihood of this entire set of views under the prior. This naturally encourages consistency, as the prior is optimized to generate a coherent set of images rather than a sequence of unrelated ones.\n\nThis is closely related to the concept of Variational Score Distillation (VSD), which reframes the distillation problem in terms of a particle-based variational inference perspective. VSD reduces the variance inherent in SDS by considering a distribution of scores, leading to more stable optimization and higher-quality geometry. While VSD was originally proposed for NeRF-like representations, its principles are directly applicable to 3DGS. By using a more robust distillation objective, the optimization is less prone to getting stuck in local minima that correspond to geometrically inconsistent or degenerate solutions. The gradients derived from these advanced objectives are more informative, guiding the 3DGS parameters (position, scale, rotation, color) towards a configuration that is not only photorealistic but also geometrically plausible and consistent across the entire scene.\n\nIn conclusion, overcoming the 3D inconsistency and geometry issues in generative 3D synthesis is a multi-faceted problem that has inspired a rich body of research. The solutions lie in moving beyond simple 2D priors and embracing techniques that are inherently more \"3D-aware.\" This is achieved by leveraging multi-view diffusion models that provide consistent supervision signals, incorporating depth-conditioned guidance to enforce geometric validity, and reformulating the distillation objective to be more stable and holistic. These advancements are critical for bridging the gap between 2D generative power and 3D structural integrity, paving the way for high-fidelity, artifact-free text-to-3D generation using explicit representations like 3D Gaussian Splatting.\n\n### 5.4 Feed-Forward and Hybrid Generation Pipelines\n\n### 5.4 Feed-Forward and Hybrid Generation Pipelines\n\nWhile the strategies discussed in the previous section for overcoming 3D inconsistency have significantly improved the quality of optimization-based text-to-3D generation, a fundamental challenge remains: computational cost. The paradigm of using Score Distillation Sampling (SDS) to distill 2D priors into a 3D representation is notoriously slow, often requiring hours of per-scene optimization to converge to a high-fidelity result. This computational bottleneck has spurred the development of feed-forward and hybrid generation pipelines, which aim to drastically reduce inference times from hours to seconds. These methods leverage the power of large-scale generative models to predict 3D structures directly or to provide a high-quality initialization that requires only minimal refinement, marking a crucial step towards practical and scalable 3D content creation.\n\n**Feed-Forward Reconstruction Networks**\n\nFeed-forward approaches bypass the iterative optimization loop entirely by training a network to directly output a 3D representation given a text prompt or a single image. The core challenge here is to imbue the network with the ability to synthesize consistent multi-view geometry and appearance from a single input. A common strategy involves a two-stage process: first, generating a set of consistent multi-view images of the target object, and second, reconstructing the 3D model from these images using a feed-forward reconstruction network.\n\nRecent advancements have focused on improving the consistency of the generated multi-view images. For instance, methods like \"Instant3D\" and \"Direct2.5\" rely on fine-tuned text-to-image diffusion models that are conditioned to produce multiple views of an object simultaneously. These models ensure that the generated images share a consistent global structure and local texture, which is crucial for successful reconstruction. Once these images are generated, a feed-forward reconstruction network, often based on transformer architectures or specialized encoders, predicts the 3D Gaussian attributes directly. This approach decouples the complex task of view synthesis from the task of 3D geometry estimation, allowing each component to be optimized independently. The result is a significant speedup, as the computationally expensive diffusion sampling is performed only a few times, and the 3D reconstruction is a single forward pass.\n\nHowever, feed-forward methods often face a trade-off between speed and quality. Because the model must generalize across a vast domain of text prompts or images, it may struggle with fine-grained details or complex geometries that optimization-based methods can capture through per-scene fitting. The quality of the generated multi-view images is also a critical factor; inconsistencies or artifacts in these images will directly translate to artifacts in the final 3D model. Therefore, research in this area is heavily focused on improving the robustness and consistency of the multi-view generation step, as well as designing more powerful feed-forward reconstruction architectures that can better leverage the generated image priors.\n\n**Hybrid Generation Pipelines**\n\nHybrid approaches represent a pragmatic compromise between the high quality of slow optimization methods and the speed of feed-forward networks. These pipelines typically combine a fast feed-forward component for initialization with a short, rapid SDS-based refinement stage. The goal is to achieve a balance, producing high-fidelity results in a fraction of the time required by pure optimization methods.\n\nA prominent example of this paradigm is \"GaussianDreamer,\" which first uses a feed-forward network to generate a coarse 3D Gaussian Splatting (3DGS) representation from text. This initial model, while fast to produce, may lack fine details or exhibit minor geometric flaws. To address this, a short refinement stage is employed, using SDS loss to polish the initial Gaussians. Because the optimization starts from a reasonable solution rather than random initialization, the refinement process converges much faster and requires far fewer SDS iterations. This strategy effectively leverages the generative prior of the feed-forward network to guide the optimization, preventing it from getting stuck in poor local minima and ensuring a high-quality final output.\n\nSimilarly, \"BoostDream\" employs a hybrid strategy where it first generates high-quality multi-view images using a robust diffusion model and then uses a fast reconstruction network to build an initial 3DGS model. This initial model is then refined using a lightweight SDS process. The key insight is that the most time-consuming part of the generation process\u2014the slow, iterative nature of SDS\u2014is largely replaced by efficient feed-forward steps. The refinement stage is only needed to correct minor inconsistencies and add the final layer of detail. This approach is particularly effective because it leverages the strengths of each component: the generative power of diffusion models for creating plausible multi-view content, the speed of feed-forward networks for 3D reconstruction, and the fine-tuning capability of SDS for achieving photorealism.\n\nThese hybrid pipelines are also more flexible than pure feed-forward methods. They can be adapted to different types of input, such as single images or text prompts, and can be combined with various 3D representations beyond 3DGS, such as NeRFs or meshes. By decoupling the coarse generation from the fine refinement, they also offer a better trade-off between computational resources and output quality, making them more practical for a wider range of applications.\n\n**Comparative Analysis and Future Directions**\n\nThe shift from pure optimization to feed-forward and hybrid pipelines marks a maturation of the text-to-3D field, moving from proof-of-concept systems towards practical tools. Pure optimization methods like \"ProlificDreamer\" set a high bar for quality but are impractical for interactive use. In contrast, feed-forward methods offer unparalleled speed but may sacrifice some level of detail and geometric accuracy. Hybrid methods effectively bridge this gap, offering near state-of-the-art quality at a fraction of the computational cost.\n\nThe choice between these paradigms depends on the specific application. For real-time or interactive applications where speed is paramount, feed-forward methods are the clear choice. For high-fidelity asset creation where quality is the primary concern and some delay is acceptable, hybrid methods provide an excellent balance. For research into novel generative priors and loss functions, the optimization-based approach remains a valuable testbed, albeit a slow one.\n\nLooking forward, the primary research direction for feed-forward and hybrid pipelines is to further close the quality gap with pure optimization methods. This involves improving the consistency of multi-view generation, designing more powerful and generalizable reconstruction networks, and developing more efficient refinement strategies. Another promising direction is to unify these approaches, creating adaptive systems that can switch between feed-forward, hybrid, and full optimization modes depending on the complexity of the prompt and the desired output quality. As generative models continue to improve, we can expect feed-forward and hybrid pipelines to become the dominant paradigm for text-to-3D generation, enabling the creation of complex 3D content with unprecedented speed and ease.\n\n### 5.5 Compositional Generation and Scene Assembly\n\nThe generation of complex, multi-object 3D scenes presents a significant challenge that extends beyond the creation of single, isolated objects. While text-to-3D and image-to-3D methods have achieved remarkable fidelity for individual entities, assembling them into a coherent, physically plausible, and semantically meaningful world requires specialized techniques. This subsection delves into the paradigms of compositional generation and scene assembly, which aim to construct rich 3D environments by orchestrating multiple objects, their spatial relationships, and their interactions. The explicit and discrete nature of 3D Gaussian Splatting (3DGS) offers a distinct advantage for these tasks compared to implicit representations, as it allows for direct manipulation, combination, and optimization of distinct object primitives within a shared scene context.\n\n### The Paradigm of Compositional Generation\n\nCompositional generation addresses the limitation of holistic scene generation, where a single prompt like \"a living room with a sofa, a coffee table, and a lamp\" might struggle to produce high-quality, distinct geometry for each element. Instead, a compositional approach breaks the problem down: generate individual objects and then intelligently place them within a scene. This modular pipeline offers greater control and higher fidelity for each component. The core challenge lies in ensuring that the assembled objects are not only geometrically non-intersecting but also semantically consistent (e.g., a lamp is placed on a table, not floating in mid-air) and photometrically consistent (e.g., lighting and shadows are coherent across objects).\n\nThe explicit nature of 3DGS is particularly well-suited for this modular approach. Unlike a neural radiance field, which represents the entire scene as a single continuous function, a collection of 3D Gaussians can be treated as a set of independent point clouds with associated attributes. This allows for straightforward operations like translation, rotation, and scaling of objects without requiring a complete re-optimization of the entire scene representation. Furthermore, the rendering process, which involves sorting and alpha-blending of these primitives, is naturally amenable to the addition or removal of objects from the optimization pool.\n\n### Leveraging Explicit Representations for Consistent Assembly\n\nSeveral works have explored how to leverage the explicit nature of Gaussian Splatting for compositional tasks. A key contribution in this area is **CG3D**, which explicitly tackles the problem of physically and semantically consistent compositions. The central insight of **CG3D** is that the explicit representation allows for the enforcement of constraints that are difficult to formulate in implicit fields. For instance, **CG3D** can leverage geometric priors to prevent object intersections and ensure stable physical placement (e.g., objects rest on surfaces). It also incorporates semantic guidance, potentially derived from 2D segmentation or layout models, to guide the positioning of objects according to common-sense rules. By optimizing the positions and appearances of multiple Gaussian-based objects simultaneously while respecting these constraints, **CG3D** demonstrates that complex scenes can be assembled with a high degree of control and fidelity. The ability to directly access and modify the 3D coordinates of each Gaussian primitive is crucial for implementing such physical and semantic rules during the optimization process.\n\nThis approach contrasts with methods that attempt to generate a full scene from a single text prompt in one go. While holistic generation can be effective, it often lacks the fine-grained control and editability required for practical applications. Compositional methods, by treating the scene as a collection of distinct entities, empower users to modify individual components post-generation, swap objects, or rearrange the layout without starting from scratch. The explicit nature of 3DGS makes such editing operations trivial compared to the complex fine-tuning required for implicit representations.\n\n### The Role of Locally Conditioned Diffusion Models\n\nWhile the assembly of pre-generated objects is a powerful strategy, another line of research focuses on guiding the generative process itself to produce structurally coherent scenes. This is often achieved through locally conditioned diffusion models. These methods leverage powerful 2D diffusion priors (e.g., Stable Diffusion) but modify the conditioning mechanism to be spatially aware. Instead of providing a single global text prompt, they provide text prompts associated with specific regions of the image or 3D space.\n\nIn the context of 3D generation, this can be implemented by defining a layout or bounding boxes for different objects and then using these regions to locally condition the Score Distillation Sampling (SDS) gradient. For example, when optimizing a 3DGS scene, the SDS gradient for a given pixel can be computed using a prompt that corresponds to the object occupying that pixel's line of sight. This ensures that the gradients pull the underlying 3D Gaussians towards the appearance and geometry of the intended object for that region. This technique effectively \"paints\" objects into the 3D scene using text guidance, leading to structured generation where objects are well-composed and respect the intended layout.\n\nThe efficiency of 3DGS rendering is a key enabler for such iterative, region-based optimization. Since 3DGS can render images in real-time, the system can quickly generate novel views to compute gradients for different regions of the scene, allowing for rapid convergence of the compositional generation process. This tight feedback loop between the 3D representation and the 2D diffusion prior is essential for creating complex, multi-object scenes that are both visually plausible and semantically aligned with the user's intent.\n\n### Challenges and Future Directions\n\nDespite these advancements, compositional generation with 3DGS still faces several challenges. A primary issue is ensuring global illumination consistency. When objects are generated independently or conditioned on local regions, their lighting may not be coherent. For instance, shadows cast by one object onto another may be missing or incorrect. Future work will likely focus on integrating inverse rendering techniques directly into the compositional pipeline, allowing for the joint optimization of materials and lighting across all objects in the scene. This would enable the creation of truly photorealistic and physically consistent environments.\n\nFurthermore, scaling these methods to generate large, open-world scenes remains an open problem. While compositional generation is a step in the right direction, efficiently managing thousands of objects and their interactions requires sophisticated data structures and optimization strategies. The explicit nature of 3DGS, while beneficial for manipulation, can lead to memory bottlenecks in such large-scale scenarios. Techniques for hierarchical representation and level-of-detail (LoD) management, similar to those being developed for static large-scale 3DGS, will need to be adapted for dynamic scene assembly.\n\nIn conclusion, compositional generation and scene assembly represent a critical step towards creating rich and interactive 3D worlds. By leveraging the explicit and modular nature of 3D Gaussian Splatting, methods like **CG3D** and techniques based on locally conditioned diffusion are enabling the physically and semantically consistent construction of complex scenes. These approaches offer a promising path forward, balancing the creative power of generative models with the control and editability required for practical 3D content creation.\n\n### 5.6 Acceleration and Scalability Techniques\n\nThe rapid advancement of text-to-3D generation has unlocked unprecedented creative potential, yet it remains bottlenecked by the computational intensity of the underlying optimization processes. The dominant paradigm, Score Distillation Sampling (SDS), and its variants like Variational Score Distillation (VSD), typically require hundreds of thousands of iterations, each involving a forward pass through a large-scale diffusion model and a backward pass to update the 3D representation. This iterative nature results in generation times spanning several hours, rendering these systems impractical for interactive applications and large-scale content creation pipelines. Consequently, a significant research thrust has emerged, focused on accelerating these pipelines and enhancing their scalability. This subsection surveys the key techniques developed to address these challenges, focusing on three primary strategies: parallelizing the sampling process, scaling distillation to accommodate large prompt sets, and distilling the knowledge into more efficient, feed-forward architectures.\n\n### Parallelizing the Optimization Loop\n\nA primary source of latency in text-to-3D systems stems from the sequential nature of the iterative optimization. To overcome this, researchers have explored parallelizing the sampling process to leverage the capabilities of modern multi-core hardware. The core idea is to decouple the iterative updates from a strictly sequential loop, allowing multiple sampling steps to be performed concurrently. This approach is analogous to accelerating diffusion-based image generation through techniques like DDIM and DPM-Solver, which reduce the number of required function evaluations. In the 3D domain, a notable contribution is the development of algorithms that can run multiple optimization steps in parallel, effectively shortening the wall-clock time required for convergence. For instance, methods like [74] introduce a parallel sampling algorithm that accelerates the optimization trajectory. By running multiple chains of the diffusion process concurrently and intelligently combining their outputs, such methods can significantly reduce the total generation time without a substantial degradation in quality. This strategy directly tackles the sequential bottleneck, transforming the time-consuming refinement process into a more manageable, parallelized workflow. The effectiveness of these methods demonstrates that the iterative nature of SDS-based generation is not an insurmountable barrier, but rather a process that can be significantly streamlined through clever algorithmic design that exploits parallel computation.\n\n### Scaling to Large Prompt Sets and Asynchronous Distillation\n\nWhile parallelization speeds up the generation for a single prompt, another critical challenge is scalability when dealing with a large batch of prompts or a need for diverse outputs. Generating a large number of 3D assets, each conditioned on a different text prompt, using standard sequential methods would be prohibitively expensive. This necessitates systems that can scale efficiently to large prompt sets. The concept of asynchronous distillation emerges as a powerful solution to this problem. In this paradigm, the knowledge from a large, powerful teacher model (the diffusion model) is distilled into a student model (the 3D representation generator) in a non-blocking, parallel manner. Instead of waiting for all prompts in a batch to complete their optimization cycles, the system can continuously update the student model as soon as a subset of the optimizations finishes. This asynchronous workflow maximizes hardware utilization and minimizes idle time, leading to a much higher throughput for generating a multitude of assets.\n\nA key example of this approach is [16], which addresses the challenge of scaling text-to-3D generation to a large number of prompts. It employs an asynchronous distillation framework where multiple 3D generation processes run in parallel. As individual processes converge, their learned knowledge is used to update a shared student model. This allows the system to efficiently learn from a diverse set of prompts simultaneously, rather than treating each one as an isolated optimization problem. By decoupling the teacher-student interaction from a rigid, synchronous schedule, [16] demonstrates how large-scale 3D content creation pipelines can be built. This is a crucial step towards practical applications where generating hundreds or thousands of unique 3D models on demand is required, such as for virtual worlds, e-commerce, or game asset production. The work highlights that scalability is not just about raw speed for one instance, but about the intelligent management of computational resources across a diverse and potentially massive workload.\n\n### Distillation into Feed-Forward Networks and Efficient Generators\n\nPerhaps the most transformative approach to acceleration involves a fundamental shift in the generation architecture itself: moving away from iterative optimization entirely and towards single-pass, feed-forward networks. The core idea is to perform a one-time, expensive distillation of the knowledge embedded within the large-scale diffusion model and the iterative optimization process into a compact, highly efficient generator. Once this distillation is complete, generating a 3D model from a text prompt becomes a near-instantaneous forward pass through this specialized network, reducing generation time from hours to seconds or even milliseconds. This paradigm shift is critical for real-time applications and interactive systems.\n\nSeveral works have pioneered this direction. For example, [52] (Efficient Text-to-3D) explores distilling the knowledge from a pre-trained text-to-3D diffusion model into a feed-forward 3D generative network. Instead of optimizing a 3D representation for each new prompt, [52] trains a model that directly maps a text embedding to the parameters of a 3D representation (such as a NeRF or a set of Gaussians). This is achieved by leveraging a large dataset of text-3D pairs, which can be synthetically generated using the iterative SDS process as a teacher. The student network learns to mimic the teacher's behavior across the entire distribution of prompts, effectively compressing the iterative optimization into a single inference step.\n\nSimilarly, the [75] framework (Generative Compression with Energy Constraints) presents an elegant method for distilling diffusion priors into efficient GAN-like generators. [75] frames the distillation process as an energy-based modeling problem, where the goal is to train a generator that produces 3D assets that not only match the text description but also have a low \"energy\" according to the teacher diffusion model's guidance. This allows for the training of a highly efficient generator that can produce high-quality 3D content in a single forward pass. The resulting model is orders of magnitude faster than optimization-based methods, making it suitable for applications that demand low latency. These distillation-based approaches represent a mature and powerful direction for text-to-3D synthesis, promising to make high-fidelity 3D generation accessible for a wide range of interactive and large-scale applications by transforming a slow, iterative process into a fast, direct mapping.\n\nIn conclusion, the pursuit of acceleration and scalability in text-to-3D synthesis is driving the field towards more practical and deployable solutions. The strategies of parallelizing the iterative loop, scaling to large prompt sets via asynchronous distillation, and fundamentally re-architecting the generation pipeline through feed-forward distillation, collectively address the core computational bottlenecks. While methods like [74] and [16] optimize the existing paradigm, distillation techniques like those in [52] and [75] signal a paradigm shift towards real-time generation. As these techniques mature, they will be instrumental in unlocking the full potential of text-to-3D, moving it from a research curiosity to a foundational technology for the next generation of digital content creation.\n\n## 6 3D Editing, Manipulation, and Inpainting\n\n### 6.1 Interactive Object Manipulation and Selection\n\nThe advent of 3D Gaussian Splatting (3DGS) has revolutionized the field of neural rendering by providing an explicit, high-fidelity representation that supports real-time rendering speeds. This capability is particularly transformative for interactive applications, where users demand immediate visual feedback. Subsection 6.1 focuses on the methodologies that leverage these speed advantages to enable intuitive object selection and real-time manipulation within 3D scenes. Unlike implicit representations such as Neural Radiance Fields (NeRF), which require computationally expensive volumetric ray marching and often necessitate retraining or complex fine-tuning for scene modifications, 3DGS allows for direct geometric and photometric adjustments to the scene primitives. The core challenge addressed by recent research is bridging the gap between 2D user inputs\u2014such as point clicks or scribbles\u2014and the 3D structure of the Gaussian representation to facilitate seamless editing.\n\nThe paradigm of interactive manipulation in 3DGS is heavily influenced by the evolution of neural rendering techniques. Early implicit methods, while capable of high-quality view synthesis, were ill-suited for interactive editing. As noted in comprehensive surveys of the field [1], the continuous nature of NeRF makes local modifications difficult without affecting the entire scene representation. Furthermore, the rendering process for NeRF is inherently slow due to the need for multiple network forward passes per pixel, making real-time feedback impossible [2]. In contrast, 3DGS utilizes a tile-based rasterization pipeline that projects 3D primitives onto the 2D screen, allowing for rendering speeds exceeding 100 frames per second. This explicit nature means that manipulating a specific object corresponds to modifying a subset of the Gaussian primitives, a task that is computationally efficient compared to optimizing an implicit field.\n\nA foundational step in interactive manipulation is object selection. To select an object, users typically provide sparse 2D inputs, such as clicking a point on the object in a rendered view. The system must then propagate this selection to identify all relevant 3D Gaussian primitives associated with that object. This is often achieved by generating a 3D mask or a selection volume. One approach involves utilizing the depth information and the projection logic inherent in the 3DGS pipeline. By identifying the specific Gaussian that contributes to the clicked pixel, the system can perform a spatial query to find neighboring Gaussians that share similar attributes or lie within a connected component of the scene. This process is significantly faster than similar operations in implicit fields, where querying geometry requires network evaluations.\n\nHowever, a naive selection based solely on spatial proximity or depth can lead to artifacts, particularly in occluded regions or areas with complex geometry. To address this, advanced methods incorporate semantic understanding or visibility constraints. While the provided reference list does not contain specific papers dedicated solely to 3DGS interactive selection, the principles are often derived from the broader context of neural rendering and computer graphics. For instance, the concept of using proxy geometry to assist in editing is well-established. In the context of NeRF, methods have used explicit meshes to guide deformation [10]. While 3DGS is point-based, similar proxy structures or bounding volumes can be computed to constrain the selection, ensuring that only visible, relevant points are modified.\n\nOnce the 3D mask is generated, manipulation can occur. The most common forms of manipulation include rigid transformations (translation, rotation, scaling) and non-rigid deformations. For rigid transformations, the selected subset of Gaussians is simply transformed by a matrix. Because 3DGS stores position ($\\mu$) and covariance parameters explicitly, these transformations are applied directly without the need for network updates. This allows for immediate updates to the scene geometry and appearance, providing the \"immediate editing without per-edit retraining\" mentioned in the scope. The speed of 3DGS is the critical enabler here. In implicit methods, even if one could mathematically define a selection, the rendering of the edited scene would take minutes per frame, making interactive exploration impossible. 3DGS, by contrast, allows the user to drag an object and see the lighting and occlusion update in real-time. This interactivity is a direct result of the explicit representation, which decouples the rendering cost from the scene complexity in a way that volumetric rendering does not. The ability to render at high frame rates is a distinct advantage highlighted in comparisons between NeRF and explicit methods [76].\n\nFurthermore, the manipulation of Gaussians extends to editing their intrinsic properties, such as opacity and color. Users can select an object and change its color or make it transparent. In 3DGS, this involves updating the opacity ($\\alpha$) or the Spherical Harmonics (SH) coefficients of the selected Gaussians. Since these are just parameters stored in buffers, updates are instantaneous. This contrasts with methods that require re-optimization of a neural network to change appearance, which is discussed in the context of NeRF limitations [12]. The explicit control over primitives allows for fine-grained editing that is difficult to achieve with implicit representations.\n\nThe integration of these selection and manipulation techniques into a cohesive system often involves a \"teacher-student\" or \"proxy\" architecture. For example, to maintain multi-view consistency during editing, some systems might project 2D edits into 3D space and then update the Gaussians. If a user paints over an object in one view, the system must determine which Gaussians correspond to those pixels and update them. This requires a robust mapping between 2D pixels and 3D primitives. The rasterization pipeline naturally provides this mapping during rendering, but maintaining consistency across different views after an edit is non-trivial. The explicit nature of 3DGS simplifies this: once the 3D subset is identified and modified, the change is consistent across all views by definition of the 3D representation.\n\nHowever, challenges remain. One issue is the \"floating artifact\" problem inherent in 3DGS. If a user selects an object, the selection might inadvertently include background Gaussians that are occluded but have high opacity. Advanced selection techniques must account for visibility. This is where the concept of \"visibility\" from the NeRF literature becomes relevant. Although the reference list focuses on NeRF, the concept of visibility estimation is crucial for robust selection in any neural rendering system. In 3DGS, visibility can be approximated by analyzing the contribution of Gaussians to the final pixel color (alpha-blending weights). Gaussians with low contribution to the final image (despite being in the projection path) can be filtered out during the selection process.\n\nThe requirement for \"immediate editing without per-edit retraining\" is the defining feature of this subsection. This stands in stark contrast to early NeRF editing methods which often required retraining or complex optimization processes to bake edits into the implicit field [10]. 3DGS bypasses this by treating the scene as a database of primitives. The manipulation is essentially a database update operation, which is orders of magnitude faster than neural optimization.\n\nIn summary, interactive object manipulation and selection in 3DGS leverages the explicit, discrete nature of the representation to provide real-time feedback. By utilizing the rasterization pipeline to map 2D inputs to 3D primitives, users can select and manipulate subsets of the scene with ease. The speed of 3DGS, often cited as a major advantage over NeRF [6], is the key that unlocks these interactive workflows. While the provided papers mostly discuss NeRF or general rendering concepts, the principles of explicit manipulation and the limitations of implicit methods provide the necessary context to understand why 3DGS has become the preferred backbone for interactive 3D editing systems. The ability to directly edit the primitives allows for a level of control and responsiveness that was previously unattainable with neural radiance fields, marking a significant step forward in making neural rendering tools usable for content creation and interactive design.\n\nThe workflow typically proceeds as follows: the user views the scene rendered in real-time. They click on an object. The system identifies the Gaussian primitives associated with that pixel (and potentially neighboring pixels via a flood-fill or clustering algorithm in screen space). These primitives are tagged as \"selected.\" The user then inputs a transformation (e.g., drag to move). The system updates the positions of the selected primitives. Because the rendering is decoupled from the optimization, the view updates immediately to reflect the new geometry. This \"what you see is what you get\" (WYSIWYG) experience is critical for user adoption.\n\nFurthermore, the granularity of control in 3DGS allows for both coarse and fine editing. Coarse editing involves moving entire objects, while fine editing can involve adjusting the shape of an object by manipulating individual Gaussians. This is analogous to sculpting with point clouds rather than meshes. The lack of connectivity in 3DGS is actually a benefit here, as it allows for fluid deformation without worrying about mesh topology or invalidating triangulations. This flexibility is a significant departure from traditional mesh-based editing and offers a new paradigm for digital content creation.\n\nFinally, the integration of these editing tools into a larger framework often involves handling the transition from implicit optimization to explicit manipulation, representing a paradigm shift in neural rendering. As the field progresses, we expect these interactive capabilities to become increasingly sophisticated, incorporating physics-based constraints and semantic understanding to further bridge the gap between neural rendering and traditional computer graphics pipelines.\n\n### 6.2 Text-Guided Localized Editing and Control\n\nText-guided editing of 3D scenes represented by Gaussian Splatting has emerged as a powerful paradigm, moving beyond simple global style transfers to enable precise, localized modifications. This capability builds directly upon the interactive manipulation techniques discussed in the previous subsection, which established the core principles of selecting and transforming Gaussian primitives based on user input. While those techniques often relied on direct 2D inputs like clicks or masks, text-guided editing elevates this interaction by using natural language prompts to define Regions of Interest (ROIs). The explicit, discrete nature of 3DGS remains the key enabler, as individual Gaussians can be selected, grouped, and modified based on semantic cues derived from text. This subsection reviews the core techniques that facilitate such fine-grained control, focusing on the alignment of text prompts with specific 3D Gaussian regions and the subsequent application of localized appearance or geometry edits.\n\nA foundational challenge in text-guided localized editing is establishing a robust correspondence between the 2D semantic understanding of a text prompt and the 3D Gaussian primitives. Unlike pixel-based editing in 2D images, 3DGS scenes consist of millions of unstructured points, each lacking inherent semantic labels. To bridge this gap, recent works leverage powerful 2D vision-language models and segmentation models, such as the Segment Anything Model (SAM), to generate 2D masks corresponding to text prompts from various viewpoints. These 2D masks are then lifted into 3D to assign semantic identities or edit weights to the Gaussians. For instance, a user might prompt the system to \"change the color of the red car.\" The system would then render the scene from multiple angles, use a text-to-mask or segmentation model to identify the \"red car\" in the 2D renders, and project these masks back into the 3D space to select the Gaussians contributing to that object. This process effectively groups Gaussians by instance or semantic category, creating a 3D mask or a set of weights that can be used to drive localized optimization.\n\nOnce the 3D regions are identified, the editing process typically involves freezing the geometric attributes (position, scale, rotation) of the selected Gaussians and optimizing their appearance attributes (color and opacity) to match the desired edit. For example, in the context of text-guided colorization or style transfer, the optimization objective is to minimize the difference between the rendered image of the edited region and a target image generated by a 2D diffusion model conditioned on the text prompt. This allows for highly creative and localized edits, such as \"make the sofa velvet\" or \"paint the walls blue,\" without affecting the rest of the scene. The explicit nature of 3DGS ensures that these edits are consistent across different viewing angles, as the underlying 3D representation is directly modified. This stands in contrast to the object removal and inpainting pipeline discussed in the following subsection, which focuses on filling voids left by removed objects; here, the focus is on modifying the properties of existing primitives to align with a creative, text-driven vision.\n\nHowever, naive text-guided optimization can suffer from multi-view inconsistencies, a common pitfall when using 2D priors for 3D generation. A 2D diffusion model might generate slightly different textures or colors for the \"red car\" when prompted from different angles, leading to artifacts or \"Janus\" problems in the 3D reconstruction. To address this, advanced frameworks incorporate geometric and semantic regularization. Some methods enforce consistency by aggregating scores from multiple views before updating the Gaussians, while others use 3D-aware priors to guide the optimization. The challenge is further compounded by the need to preserve the photometric and geometric integrity of the unedited parts of the scene. Techniques like masked loss functions and inpainting strategies are often employed to ensure that the optimization only affects the targeted ROIs, leaving the background and other objects untouched.\n\nThe granularity of control has also been refined to go beyond simple object-level edits. Some frameworks allow for attribute-specific editing, where users can target specific properties like texture, lighting, or even the physical material of an object. This is achieved by disentangling the appearance model of the Gaussians. For instance, instead of directly optimizing the color, a method might optimize a more fundamental property like albedo or a latent code that controls a generative texture model, while keeping the lighting and geometry fixed. This allows for more physically plausible edits, such as \"change the material of the table from wood to metal,\" which requires altering the reflectance properties rather than just the base color.\n\nFurthermore, the interaction between text prompts and the underlying Gaussian representation is becoming increasingly sophisticated. Instead of relying solely on post-hoc segmentation, some methods integrate semantic understanding directly into the optimization loop. They might learn a per-Gaussian semantic embedding that is optimized to be consistent with text prompts across views, allowing for real-time querying and editing of the scene. This moves towards interactive systems where a user can click on an object in a rendered view, type a text command, and see the 3D scene update in near real-time, a capability that is uniquely enabled by the fast rendering speed of 3DGS.\n\nIn contrast to global editing methods that apply a uniform filter or style to the entire scene, these localized techniques provide a much higher degree of creative freedom and practical utility. Global edits are often too coarse for complex scenes containing multiple objects with distinct characteristics. Text-guided localized editing, by aligning text prompts with specific 3D Gaussian regions, allows for the selective manipulation of scene elements, enabling applications ranging from virtual staging and interior design to content creation for augmented reality. The ability to precisely control appearance and geometry through natural language commands represents a significant step towards making 3D scene manipulation accessible to non-experts, leveraging the explicit and editable nature of the 3DGS representation to its full potential.\n\n### 6.3 Object Removal, Inpainting, and Scene Composition\n\nThe explicit and discrete nature of 3D Gaussian Splatting (3DGS) representations makes them uniquely suited for interactive editing tasks, particularly object removal and inpainting. Unlike implicit representations like Neural Radiance Fields (NeRFs), which require expensive per-edit retraining, the explicit set of Gaussians allows for direct manipulation. The core pipeline for object removal typically involves a three-stage process: segmentation, removal, and inpainting. First, a 3D mask identifying the target object is generated. This can be achieved by projecting 2D segmentation masks (e.g., from SAM or CLIP) into 3D space, or by utilizing the explicit nature of Gaussians to select clusters based on spatial proximity or feature similarity. This process of identifying and manipulating specific subsets of Gaussians is a direct extension of the segmentation-aware editing techniques discussed previously, where semantic identities are assigned to primitives for fine-grained control. Once identified, the Gaussians corresponding to the object are removed, leaving a \"hole\" in the scene representation. This hole is not merely empty space; it represents a region where the background is occluded and must be reconstructed to ensure geometric and photometric consistency from novel viewpoints.\n\nThe inpainting stage is critical and is predominantly driven by 2D generative models, specifically diffusion models. The challenge lies in lifting 2D inpainting results into a consistent 3D representation. A common strategy involves synthesizing multi-view consistent inpainted images using a 2D inpainting model conditioned on the masked images and depth/normal priors. These synthesized views are then used to supervise the optimization of new Gaussians that fill the void. The optimization process must carefully balance the fidelity to the 2D priors with the multi-view geometric constraints to avoid artifacts. For instance, methods may employ a coarse-to-fine strategy, first establishing a rough geometry and then refining textures. The integration of these new Gaussians back into the scene requires careful handling of boundaries to ensure seamless blending. This involves optimizing the opacity and color of Gaussians near the seam to match the surrounding environment, preventing visible seams or floating artifacts.\n\nA significant challenge in this pipeline is maintaining geometric consistency and handling complex lighting interactions, such as shadows and reflections. When an object is removed, the shadows it cast on the environment must also be \"removed\" or corrected. This requires the inpainting model to be aware of the global illumination context. Some approaches leverage depth and normal maps to guide the inpainting, ensuring that the reconstructed geometry aligns with the surrounding surfaces. For example, the inpainted region should respect the planar constraints of a wall or the curvature of a table. Furthermore, the interaction between the removed object and the environment (e.g., a book on a table leaving a shadow) must be addressed. This often involves inpainting not just the color but also the albedo and lighting information, effectively \"hallucinating\" the background as if the object were never there. The explicit nature of 3DGS allows for the propagation of these corrections across the scene by adjusting the view-dependent appearance terms (Spherical Harmonics) of existing Gaussians in the vicinity of the edit.\n\nBeyond simple removal, scene composition involves inserting new objects or merging multiple edited scenes. This requires the same principles of consistency but adds the challenge of occlusion and interaction between the new object and the existing scene. The new object, represented by its own set of Gaussians, must be placed in 3D space and rendered in a way that it occludes and is occluded by the scene correctly. This requires accurate depth sorting and alpha blending. Furthermore, the new object should cast shadows onto the scene and receive shadows from it. Achieving this requires either a physically-based rendering step or clever manipulation of the Gaussian opacities and colors to approximate these lighting effects. The explicit control over each Gaussian allows for fine-grained adjustments, such as deforming the object to fit the surface curvature or adjusting its appearance to match the scene's lighting conditions.\n\nRecent advancements have focused on improving the robustness and quality of these pipelines. For example, leveraging 2D segmentation models like Segment Anything Model (SAM) has become standard for generating high-quality 3D masks. The integration of these masks allows for precise selection of objects, even in complex scenes. Once the mask is obtained, the inpainting process often relies on state-of-the-art diffusion models like Stable Diffusion, which can generate plausible content for the masked regions. However, ensuring multi-view consistency remains a hurdle. Some methods propose to use score distillation sampling (SDS) or similar techniques to distill the 2D prior into a 3D consistent representation directly, optimizing the new Gaussians to render images that match the distribution of the inpainted 2D images from multiple viewpoints. This avoids the need for explicit multi-view synthesis and can be more efficient.\n\nThe efficiency of 3DGS is a major advantage for interactive editing. Because the representation is explicit, changes can be made locally without affecting the entire scene. This enables real-time feedback for users, which is crucial for applications in virtual reality and digital content creation. Systems have been developed that allow users to click on an object in a rendered view, which instantly generates a mask and allows for removal or replacement, with the scene updating in near real-time. This interactivity is a significant leap forward compared to NeRF-based methods, which would require minutes to hours of retraining for each edit. The ability to perform such edits quickly and intuitively opens up new possibilities for 3D content manipulation, from virtual staging to historical scene restoration.\n\nHowever, there are still limitations to address. Artifacts can arise, particularly at the boundaries of removed objects or in regions with complex lighting. The inpainted regions may not perfectly match the style and texture of the surrounding environment, leading to visible inconsistencies. Furthermore, handling dynamic lighting changes after editing is non-trivial. If the scene lighting is changed, the inpainted region, which was optimized for a specific lighting condition, may look out of place. Future work may involve learning more robust lighting representations or developing editing pipelines that are agnostic to lighting conditions. Additionally, the scalability of these methods to very large scenes remains an area of active research, as the number of Gaussians can grow significantly during inpainting, impacting memory and rendering performance.\n\nIn summary, the pipeline for object removal, inpainting, and scene composition in 3DGS leverages the explicit nature of the representation to enable interactive and high-fidelity editing. By combining 2D generative priors with 3D optimization, these methods can fill voids left by removed objects while maintaining geometric and photometric consistency. The integration of segmentation models and the ability to handle complex lighting interactions are key components of this process. While challenges remain in eliminating artifacts and ensuring universal consistency, the progress in this area highlights the potential of 3DGS as a versatile tool for 3D scene manipulation, bridging the gap between 2D generative AI and 3D content creation.\n\n### 6.4 Segmentation-Aware Editing and Semantic Understanding\n\nThe explicit and discrete nature of 3D Gaussian Splatting (3DGS) provides a powerful foundation for segmentation-aware editing, enabling precise, object-level control that moves beyond global scene-wide manipulations. While 3DGS excels at real-time rendering, its native representation as a collection of primitives lacks inherent semantic grouping. This limitation makes it challenging to perform localized edits, such as recoloring a specific object or applying a distinct style to a portion of the scene. To address this, a crucial step in the 3DGS editing pipeline is to assign semantic identities or labels to individual Gaussians, effectively creating a semantic partition of the scene. This process bridges the gap between the raw geometric representation and the high-level understanding required for fine-grained manipulation, forming the basis for the object removal and inpainting tasks discussed previously.\n\nThe core challenge in establishing this semantic understanding is the \"2D-to-3D lifting\" problem, where information from 2D image priors must be propagated to the 3D representation. A common and effective strategy involves a multi-view propagation and aggregation approach. First, a state-of-the-art 2D segmentation model, such as the Segment Anything Model (SAM), is applied to multiple rendered views of the scene from different angles. This generates a set of 2D segmentation masks corresponding to specific objects or background regions. The next step involves a voting or aggregation mechanism: for each 3D Gaussian, its center is projected into the camera views. If the projected point falls within a segmented mask in a sufficient number of views, that Gaussian is assigned the corresponding semantic label or an identity encoding. This process effectively groups primitives by instance or category, creating a semantic partition of the Gaussian set that is essential for targeted edits.\n\nOnce Gaussians are associated with semantic labels, a wide array of sophisticated editing tasks becomes possible. The most direct application is object-level colorization and appearance transfer. By isolating the subset of Gaussians corresponding to a specific object (e.g., a \"chair\"), one can modify their color attributes\u2014such as the spherical harmonic coefficients\u2014without affecting the rest of the scene. This semantic grouping is also crucial for selective style transfer, where an artistic style can be applied only to the Gaussians identified as \"walls,\" while \"furniture\" Gaussians retain a photorealistic appearance. This level of control is achieved by constraining the optimization loss to the specific subset of Gaussians identified by the semantic mask, ensuring that gradient updates only affect the targeted region and preventing artifacts in unedited areas.\n\nBeyond simple appearance changes, semantic understanding enables more complex structural manipulations, particularly in the context of object removal and scene composition. The semantic mask identifies the Gaussians to be deleted, and the resulting \"hole\" can then be filled using 3D inpainting techniques. The surrounding semantic context (e.g., \"wall,\" \"floor\") can guide the generation of new Gaussians to plausibly complete the scene. Furthermore, the explicit nature of 3DGS allows for the combination of semantic editing with geometric constraints. For example, in techniques like Gaussian-Mesh anchoring, Gaussians can be bound to an underlying mesh structure. If the mesh is semantically segmented, the Gaussians inherit this segmentation, providing a robust and structured way to perform edits like virtual object insertion or removal.\n\nHowever, relying on 2D segmentation models like SAM presents challenges, as these models can be computationally expensive and may produce inconsistent masks across different views, leading to \"flickering\" or misidentified Gaussians in the 3D representation. To address this, some methods introduce geometric and temporal consistency checks, incorporating optical flow or depth priors to ensure that semantic identities remain stable over time and across viewpoints. The granularity of semantic control is another key area of development. While early methods assign a single label per Gaussian, more advanced techniques allow for \"soft\" semantic assignments based on the Gaussian's influence near mask boundaries, enabling smoother transitions and more realistic composites for high-fidelity editing.\n\nIn summary, segmentation-aware editing transforms 3DGS from a purely rendering-optimized representation into a semantically meaningful 3D scene graph. By leveraging 2D priors to assign identity encodings to Gaussians, these methods unlock a suite of powerful editing capabilities, from object-level colorization to selective style transfer and complex scene composition. This process of establishing semantic identity is a foundational step that enables the interactive and high-fidelity manipulations discussed throughout this survey, marking a significant step towards intuitive 3D content creation systems.\n\n### 6.5 Interactive Systems and VR Integration\n\nThe explicit and discrete nature of 3D Gaussian Splatting (3DGS), coupled with its ability to render novel views at real-time frame rates, makes it an ideal backbone for interactive systems and immersive applications, particularly in Virtual Reality (VR). Unlike implicit neural representations such as NeRFs, which require computationally expensive volumetric ray marching for every pixel, 3DGS utilizes efficient tile-based rasterization. This fundamental difference allows for the rapid feedback loops necessary for interactive editing, physics-aware simulations, and intuitive user interfaces where low latency is critical to user experience and preventing motion sickness in VR environments.\n\nThe transition of 3DGS from a static reconstruction method to a dynamic editing platform is heavily reliant on the development of interactive systems that allow immediate visual feedback. In traditional mesh-based modeling, users can manipulate vertices and see changes instantly due to the explicit connectivity. However, point-based representations like Gaussians historically lacked such structure. Recent advancements have bridged this gap by enabling real-time manipulation of Gaussian attributes. For instance, systems that allow for intuitive object selection and translation often leverage 2D user inputs, such as point clicks, to generate 3D masks for immediate editing without requiring per-edit retraining. This capability is foundational for interactive workflows, allowing users to move, scale, or rotate objects within a scene seamlessly. The explicit nature of Gaussians means that these transformations can be applied directly to the primitive parameters (position, rotation, scale), which are then immediately reflected in the rendered output.\n\nA significant leap in interactive capability comes from binding 3D Gaussians to explicit control structures, such as cages or meshes, enabling free-form deformation. The paper **[29]** presents a method that extends traditional cage-based deformation to 3DGS. By converting 3D Gaussians into a proxy point cloud, the system infers the transformations to apply to the primitives based on the deformation of the cage. This approach does not require architectural changes to the underlying 3DGS pipeline, making it highly compatible with existing models. It allows for complex, non-rigid deformations to be applied to a scene in real-time, providing a level of control comparable to traditional mesh modeling software but with the photorealistic rendering quality of neural rendering. This is particularly powerful for VR integration, where users can grab and deform virtual objects naturally using hand controllers, seeing the results instantly.\n\nFurthermore, the integration of physics simulation into these interactive systems is an emerging frontier. While 3DGS is primarily a rendering representation, its explicit nature allows for the derivation of physical properties or the binding of Gaussians to physics-enabled meshes. **[77]** highlights the potential to integrate physical principles with 3DGS. In an interactive or VR context, this implies that edited objects can obey physical laws, such as gravity, collisions, and soft-body dynamics. For example, a user could pick up a virtual object in VR, throw it, and observe it bounce and deform realistically. The system would update the positions and deformations of the underlying Gaussians (or a mesh they are bound to) based on physics solvers, and the rendering pipeline would update the view in real-time. This convergence of explicit rendering and physics simulation transforms 3DGS from a static reconstruction tool into a dynamic, interactive medium.\n\nThe role of 3DGS as a fast-rendering backbone is crucial for VR systems where high frame rates (typically 90Hz or higher) are mandatory to maintain immersion and prevent motion sickness. Traditional NeRF-based systems struggle to achieve these frame rates without heavy pre-computation or baking. In contrast, 3DGS can achieve real-time performance on consumer-grade GPUs. This efficiency allows for the development of complex VR interfaces where users can not only manipulate geometry but also paint textures, adjust lighting, or perform inpainting operations directly within the immersive environment. The immediate visual feedback allows for a fluid creative process, where the barrier between the user's intent and the visual result is minimized.\n\nMoreover, the explicit representation facilitates the integration of semantic understanding into interactive systems. By associating semantic labels or identity encodings with individual Gaussians, systems can offer context-aware interactions. For instance, a user could verbally command a system to \"make the chair red,\" and the system, having identified the Gaussians corresponding to the chair, could update their color attributes (via Spherical Harmonics coefficients) instantly. This level of interactivity, combining natural language inputs with real-time visual updates, is a key advantage of the explicit Gaussian representation over more opaque implicit fields.\n\nIn the context of immersive editing, the ability to perform structural modifications is also vital. While 3DGS does not inherently support topological changes (like creating holes), it can be bridged with explicit mesh representations for such tasks. The paper **[29]**, while focused on deformation, points toward a future where Gaussians are not just floating points but are bound to a structural representation that can be edited. In a VR setting, this could allow users to \"sculpt\" the underlying mesh, with the Gaussians following the deformation, or to perform boolean operations where the Gaussian representation is updated based on the resulting mesh geometry. This hybrid approach leverages the strengths of both worlds: the structural editability of meshes and the rendering fidelity and speed of 3DGS.\n\nAnother critical aspect of interactive systems is the handling of user inputs and the translation of those inputs into 3D operations. Systems that provide real-time feedback often rely on efficient querying of the Gaussian structure. For example, to perform localized editing, the system must quickly identify which Gaussians are visible from the current viewpoint and which are affected by a user's brush stroke or selection. The tile-based sorting and blending pipeline of 3DGS is inherently suited for this, as it already organizes Gaussians by screen location. This allows for rapid identification of relevant primitives for interaction, enabling features like \"what you see is what you get\" editing in VR.\n\nThe potential for collaborative VR editing also benefits from the explicit nature of Gaussians. Since each Gaussian is defined by a relatively small set of parameters (position, covariance, opacity, color), the state of the scene can be efficiently serialized and transmitted over a network. In a collaborative VR session, changes made by one user (e.g., moving an object) can be quickly propagated to other users by updating the parameters of the affected Gaussians. This is much more efficient than transmitting dense mesh data or retraining a shared implicit model.\n\nFurthermore, the integration of 3DGS into VR systems opens up new avenues for data visualization and analysis. For large-scale scenes, such as urban environments reconstructed from aerial imagery, 3DGS allows for real-time exploration. Interactive systems can provide Level-of-Detail (LoD) mechanisms, as seen in methods like **[78]** (discussed in Section 3.2), to manage the complexity. In a VR context, this means a user can fly over a city model, with the system dynamically adjusting the density and rendering quality of Gaussians based on the viewpoint, ensuring a smooth frame rate. This capability is essential for applications in urban planning, architecture, and geospatial analysis.\n\nThe user interface design for these systems is also evolving. Instead of traditional 2D menus, VR interfaces can utilize 3D widgets and spatial interactions. For example, to adjust the opacity of a specific object, a user could grab a virtual slider floating next to the object. The system would map this interaction directly to the opacity parameter of the selected Gaussians. This direct manipulation of 3D properties is more intuitive than navigating complex 2D interfaces and is made possible by the direct accessibility of Gaussian attributes.\n\nIn summary, the explicit, discrete, and efficient nature of 3D Gaussian Splatting provides a robust foundation for interactive systems and VR integration. It overcomes the computational bottlenecks of implicit neural rendering, enabling real-time feedback essential for intuitive editing and immersive experiences. By leveraging techniques like cage-based deformation **[29]** and integrating physics simulations, these systems are moving towards offering the same level of control found in traditional 3D modeling software but with the photorealism and immediacy of neural rendering. As these technologies mature, we can expect 3DGS to become a standard for immersive content creation, collaborative design, and real-time visualization in virtual environments.\n\n### 6.6 Geometry and Mesh-Based Manipulation\n\n6.6 Geometry and Mesh-Based Manipulation\n\nThe explicit, discrete nature of 3D Gaussian Splatting (3DGS) offers a natural pathway to bridge with traditional mesh-based representations, unlocking a suite of structural editing capabilities that are cumbersome or impossible with purely implicit fields like NeRF. While 3DGS excels at photorealistic rendering and efficient optimization, its point-based representation lacks the topological connectivity and well-defined surfaces inherent to meshes. This subsection reviews a growing body of work that addresses this gap, focusing on two primary strategies: (1) binding 3D Gaussians to pre-existing or jointly optimized triangular meshes to enable deformation, physics-based simulation, and precise geometric modifications, and (2) converting optimized 3D Gaussian fields into explicit mesh representations for use in conventional modeling and animation pipelines.\n\nA prominent direction involves tethering Gaussians to a deformable mesh structure, allowing the Gaussians to follow the mesh's motion. This approach leverages the rendering prowess of 3DGS while inheriting the structural control and physical plausibility offered by mesh-based simulators. In this paradigm, the mesh acts as a coarse geometric scaffold or a \"bone\" structure, and each Gaussian primitive is associated with one or more mesh elements (vertices, edges, or faces). The deformation of the mesh, driven by external forces, user input, or animation scripts, is then propagated to the Gaussians. A straightforward method for this is Linear Blend Skinning (LBS), where the transformation of each Gaussian is determined by a weighted combination of the transformations of its associated mesh vertices. This technique, widely used in character animation, allows for plausible deformation of the Gaussian cloud, enabling tasks like character articulation and object manipulation. For instance, a character's skin, represented by Gaussians, can deform realistically as the underlying skeletal mesh is animated. This binding ensures that the high-fidelity appearance captured by the Gaussians remains consistent with the underlying geometric motion, avoiding the floaters and inconsistencies that might arise from deforming the Gaussians independently.\n\nBuilding on this concept, more sophisticated frameworks integrate Gaussians with physics engines for soft-body simulation. Here, the mesh is not just an animation target but an active participant in a physical simulation. The mesh vertices are treated as particles in a mass-spring system or a more complex continuum mechanics model. Forces such as gravity, collisions, and external contacts are applied to the mesh, and its resulting deformation is computed by the physics solver. The Gaussians, being attached to this mesh, are then deformed accordingly. This enables the simulation of elastic objects, cloth, and other deformable materials where the visual appearance is tied to physically plausible motion. For example, a piece of cloth represented by a mesh can be simulated, and the attached Gaussians will render its texture and folds realistically as it flutters and collides with other objects. This approach is particularly powerful for creating dynamic, interactive scenes in virtual reality or for generating training data for robotic manipulation tasks, where both visual fidelity and physical realism are paramount. The explicit nature of the Gaussians allows for efficient rendering of these complex deformations in real-time, a significant advantage over volumetric methods.\n\nBeyond deformation and simulation, binding Gaussians to meshes facilitates precise geometric modifications that are intuitive for artists and designers. Since the Gaussians are anchored to a mesh, users can perform standard mesh editing operations\u2014such as vertex translation, edge flipping, or face extrusion\u2014and the Gaussian representation will update in a predictable manner. This allows for tasks like reshaping an object, adding or removing geometric features, or performing local sculpting operations. The mesh provides a clean, low-resolution control interface, while the Gaussians provide the high-resolution photorealistic rendering. This separation of concerns is highly beneficial for interactive editing systems. For example, a user could click on a part of a rendered object, which corresponds to a specific region of the underlying mesh, and then manipulate the mesh geometry to see an immediate, photorealistic update in the rendered view. This is a significant step towards making 3DGS a practical tool for content creation, not just reconstruction.\n\nA complementary strategy focuses on the inverse process: converting an optimized 3D Gaussian field into a traditional mesh representation. This is crucial for interoperability with existing 3D content creation tools (e.g., Blender, Maya) and for tasks that require a clean, watertight surface, such as 3D printing or manufacturing. However, this conversion is non-trivial. 3DGS represents scenes as a \"point cloud\" of oriented ellipsoids, which do not inherently form a continuous surface. Early attempts often relied on post-processing techniques like Poisson surface reconstruction on the Gaussian point cloud, but these can struggle with the anisotropic and sometimes sparse nature of the Gaussians, leading to noisy or incomplete surfaces.\n\nRecent research has sought to bridge this gap more directly by integrating surface reconstruction principles into the 3DGS framework itself. A key innovation in this area is the introduction of 2D Gaussian Splatting [22]. Instead of modeling the scene with 3D volumetric ellipsoids, 2DGS uses a set of 2D oriented planar Gaussian \"splats\" or disks. These 2D primitives are intrinsically surface-aligned, as they are optimized to represent the scene's surfaces rather than its volume. This fundamental shift in representation makes the conversion to a mesh far more straightforward. The centers of the 2D Gaussians can be treated as a dense point cloud on the surface, and their orientations provide strong normal cues. A mesh can be extracted directly from this structured point set using techniques like Poisson reconstruction or by connecting the splats themselves. This approach yields geometrically accurate and high-quality meshes while retaining the fast training and rendering benefits of the Gaussian framework.\n\nAnother significant advancement in this direction is the work of [28]. This method proposes a unified framework that jointly optimizes a 3D Gaussian representation and an implicit Signed Distance Field (SDF). The core insight is to establish a differentiable connection between the two. A key component is a \"SDF-to-opacity\" transformation function that maps the SDF value at each Gaussian's location to its opacity. This elegantly links the geometric surface (defined by the zero-level set of the SDF) with the volumetric representation used for rendering. During optimization, the 3D Gaussians provide sparse but high-quality supervisory signals for the SDF, while the continuous SDF helps to regularize the Gaussians, guiding them to form a coherent surface and preventing the proliferation of floaters or holes. The final mesh is extracted from the optimized SDF using standard techniques like Marching Cubes. This method effectively combines the strengths of both explicit (fast rendering, high-fidelity appearance) and implicit (continuous, well-defined surface) representations to achieve high-quality surface reconstruction.\n\nFurthermore, methods like [79] address the problem of geometric deterioration in untextured regions. While not directly producing a mesh, such geometry-aware optimizations are crucial for ensuring that the resulting Gaussian field, and any mesh extracted from it, is geometrically faithful. By introducing constraints that encourage Gaussians in flat areas to align with the underlying surface, these methods produce a more regular and structured point cloud, which serves as a better input for mesh generation algorithms.\n\nIn summary, the integration of 3D Gaussian Splatting with mesh-based representations is a vital area of research that unlocks the full potential of 3DGS for structural editing and content creation. By binding Gaussians to deformable and physically simulated meshes, we gain intuitive control over object motion and geometry. Conversely, by developing methods to convert Gaussians into high-quality meshes, we ensure interoperability with the broader 3D ecosystem. Innovations like 2DGS and joint SDF optimization are paving the way for seamless and high-fidelity conversions, bridging the gap between the high-performance rendering of Gaussian-based methods and the structural integrity and editability of traditional mesh-based workflows. This progression directly complements the interactive systems discussed previously, which often rely on such mesh bindings for real-time manipulation and physics-aware simulations in VR, and it also addresses the critical need for maintaining multi-view consistency during structural edits, a challenge explored in the following subsection.\n\n### 6.7 Multi-View Consistency and Efficiency in Editing\n\nThe explicit and discrete nature of 3D Gaussian Splatting (3DGS) offers significant advantages for interactive editing, allowing for direct manipulation of scene components. However, this very characteristic introduces a critical challenge: maintaining multi-view consistency. When a user performs a localized edit\u2014such as altering the color of an object, removing an element, or inserting a new one\u2014the resulting modifications must be geometrically and semantically coherent across all potential viewing angles. Inconsistencies, such as an object changing shape or appearance when viewed from a different perspective, severely undermine the realism and utility of the edited scene. Furthermore, the iterative nature of many editing workflows demands rapid feedback, placing a premium on efficiency. This subsection explores the algorithms and techniques developed to address these dual challenges, focusing on methods that enforce cross-view consistency and accelerate the editing process to near real-time speeds.\n\nA fundamental approach to ensuring multi-view consistency involves direct optimization of the Gaussian attributes within the edited region. When an edit is applied, such as changing the texture of a wall, a naive approach might project the change onto a few key views and hope the effect generalizes. This often fails due to the discrete placement of Gaussians. More robust methods re-optimize the properties (color, opacity, scale, rotation) of the Gaussians belonging to the edited region using a loss function that penalizes inconsistencies across a set of input or synthesized views. This process is akin to the original 3DGS optimization but constrained to a specific subset of primitives. Techniques like **correspondence regularization** are vital here. By establishing dense correspondences between the edited region in one view and its projections in other views, the optimization can be guided to produce a unified 3D representation. For instance, if a user paints a stroke on an object in one view, correspondences ensure that this stroke is projected and integrated correctly into the 3D Gaussian representation, which then renders consistently from all other angles, avoiding the \"hallucination\" of different appearances from different viewpoints.\n\nThe efficiency of the editing process is heavily dependent on the speed of the underlying rendering and optimization pipeline. The original 3DGS rasterization pipeline is already highly optimized for real-time rendering, a significant asset for interactive editing. However, re-optimizing parts of the scene after an edit can still be time-consuming. To accelerate this, several strategies have been proposed. One key direction is to improve the differentiable rendering backend itself. For example, the principles behind **DISTWAR** [80] highlight that the gradient computation phase, which is essential for any optimization-based editing, can be a major bottleneck. By optimizing atomic operations and leveraging warp-level reductions, the time required for the backward pass can be significantly reduced, leading to faster convergence during the re-optimization of edited Gaussians. Another crucial technique is the use of intelligent culling. Instead of processing the entire set of millions of Gaussians for every edit, the system can quickly identify and isolate only the relevant primitives. By clustering Gaussians offline, an editing system can rapidly select the cluster(s) corresponding to a user-selected object (e.g., via a 2D mask from a segmentation model like SAM). The optimization is then confined to this small subset, drastically reducing computational load and enabling near-instantaneous feedback for localized edits.\n\nFurthermore, maintaining consistency is not just about geometry and appearance but also about the underlying structure of the representation. The discrete nature of Gaussians can lead to popping artifacts or inconsistencies if the number or distribution of Gaussians changes drastically during editing. Some advanced methods explore binding Gaussians to more structured representations to enforce consistency. For instance, binding Gaussians to a triangular mesh, as discussed in the previous subsection, can provide a topological scaffold. When an edit is performed on the mesh (e.g., a deformation), the attached Gaussians deform with it, naturally preserving consistency across views. This approach leverages the explicit nature of the mesh to guide the Gaussian representation, ensuring that edits are structurally sound. Similarly, techniques that encourage Gaussians to align with surfaces, such as those in **2D Gaussian Splatting for Geometrically Accurate Radiance Fields** [22], can be beneficial. By ensuring that Gaussians representing a surface are intrinsically planar (as 2D Gaussians), edits to that surface's appearance are more likely to remain consistent, as the underlying geometric prior constrains the optimization.\n\nIn conclusion, achieving both multi-view consistency and high efficiency in 3DGS editing is a multifaceted problem that requires a combination of robust optimization strategies and performance-oriented algorithmic design. Direct optimization of Gaussian attributes, guided by correspondence regularization, forms the backbone of consistency-preserving edits. Efficiency is achieved by leveraging the fast rendering pipeline, accelerating the differentiable rendering backend, and employing intelligent culling or structural binding to limit the scope of optimization. As these techniques mature, they pave the way for powerful, interactive 3D content creation tools that can reliably and quickly manipulate complex scenes.\n\n## 7 Inverse Rendering and Relighting\n\n### 7.1 Foundations of 3DGS-based Inverse Rendering\n\nThe advent of 3D Gaussian Splatting (3DGS) has marked a paradigm shift in neural rendering, moving away from the computationally intensive volumetric ray marching characteristic of Neural Radiance Fields (NeRF) towards a highly efficient, explicit point-based representation. While 3DGS was initially celebrated for its ability to synthesize photorealistic novel views in real-time, its potential extends far beyond view synthesis into the domain of inverse rendering. Inverse rendering seeks to decompose a captured scene into its fundamental physical properties\u2014geometry, spatially varying materials, and lighting\u2014enabling applications such as relighting, material editing, and virtual object insertion. This subsection introduces the core principles of extending 3D Gaussian Splatting to inverse rendering, contrasting the explicit, point-based representation with implicit neural fields like NeRF, and outlining the standard pipeline for decomposing scene properties from multi-view images.\n\nTo understand the foundations of 3DGS-based inverse rendering, one must first appreciate the limitations of traditional implicit representations. The seminal work [1] introduced the concept of representing a scene as a continuous volumetric function parameterized by a multi-layer perceptron (MLP). This approach excels at capturing fine geometric details and view-dependent appearance but suffers from significant computational overhead. Rendering a single pixel requires querying the network hundreds of times along a ray, making the extraction of explicit geometry and the simulation of complex lighting interactions (such as indirect illumination) prohibitively expensive for inverse rendering tasks. Furthermore, while methods like [12] improved the modeling of specular reflections by structuring the outgoing radiance function, the underlying volumetric representation remains difficult to decouple into intrinsic material properties and lighting. The continuous nature of NeRF makes it challenging to isolate specific scene components for editing without retraining or complex warping, as seen in [10].\n\nIn contrast, 3DGS represents the scene as a collection of anisotropic 3D Gaussians, each defined by a position, covariance matrix (determined by scaling and rotation), opacity, and view-dependent color represented by Spherical Harmonics (SH). This explicit, discrete representation offers distinct advantages for inverse rendering. Unlike the implicit fields of NeRF, 3D Gaussians provide an immediate, discrete approximation of the scene geometry. The optimization process, which involves adaptive density control to add or remove Gaussians based on gradients, results in a sparse set of primitives that effectively cover the surface of the scene. This sparsity and explicit nature facilitate the application of regularization techniques and priors necessary for disentangling geometry, materials, and lighting.\n\nThe standard pipeline for 3DGS-based inverse rendering typically begins with the acquisition of multi-view images under known or unknown lighting conditions. The first step involves reconstructing the geometry and appearance using the standard 3DGS formulation. However, for inverse rendering, the optimization objective is expanded beyond simple color matching. The goal is to decompose the observed radiance into diffuse albedo, surface normals, roughness, and illumination. In the 3DGS framework, this often involves assigning additional attributes to each Gaussian primitive. For instance, while the original 3DGS uses SH to model view-dependent color, inverse rendering methods replace or augment this with a physically based rendering model, such as a Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF). This requires predicting material properties (albedo, roughness) at each Gaussian location, which are then lit by an estimated environment map or lighting model.\n\nA key distinction in 3DGS-based inverse rendering is the handling of geometry. NeRF-based methods rely on volume density, which is a continuous field, whereas 3DGS relies on discrete points. To recover a continuous surface, methods often employ regularization to encourage Gaussians to align with surfaces, effectively turning them into \"surfels\" (surface elements). This is a crucial step because accurate material estimation requires well-defined surface normals. While NeRF can predict normals via the gradient of the density field, 3DGS must explicitly predict or regularize normals for its discrete primitives. The explicit nature of Gaussians allows for efficient querying of visibility and occlusion, which is vital for calculating global illumination effects like shadows and inter-reflections. Unlike NeRF, which requires expensive volumetric queries to determine visibility, 3DGS can utilize the splatting order and depth sorting to approximate occlusions efficiently.\n\nFurthermore, the transition from implicit to explicit representations in inverse rendering addresses the \"baking\" problem. NeRFs are often \"baked\" into explicit representations like meshes or grids for real-time rendering, but this process can lose information. 3DGS, being explicit from the start, allows for direct manipulation of the scene components. For example, in [7], the authors discuss the need to convert implicit fields into sparse voxel grids for real-time performance. 3DGS bypasses this conversion step, allowing inverse rendering algorithms to operate directly on the primitives that are rendered. This directness facilitates tasks like relighting, where the lighting component can be separated from the material component and updated independently.\n\nThe mathematical formulation of 3DGS also lends itself well to inverse rendering. The covariance matrix of a Gaussian, derived from scaling and rotation, defines its spatial extent and orientation. In inverse rendering, this orientation can be interpreted as the surface normal, provided the Gaussians are flattened onto the surface. Optimization strategies often include a geometric regularization term that penalizes deviations from a smooth surface, encouraging the Gaussians to form a coherent surface rather than a cloud of points. This is a departure from NeRF, where geometry is an emergent property of the density field, and explicit surface extraction (e.g., via Marching Cubes) is a separate, often noisy step. 3DGS allows for a more unified optimization of geometry and appearance.\n\nMoreover, the efficiency of 3DGS rendering plays a pivotal role in the feasibility inverse rendering pipeline. Inverse rendering is inherently an optimization problem that requires iterative refinement. The tile-based rasterization pipeline of 3DGS allows for gradients to be computed and backpropagated efficiently. This enables the use of complex loss functions that enforce physical constraints, such as the conservation of energy (albedo must be less than 1) or consistency across multiple views. In contrast, the slow rendering speed of NeRF makes such iterative optimization loops extremely time-consuming, hindering its practical application in this domain.\n\nIn summary, the foundations of 3DGS-based inverse rendering rest on the explicit, discrete, and efficient nature of the Gaussian representation. It contrasts with NeRF's implicit, continuous, and computationally heavy approach by providing a direct path to surface extraction, efficient visibility computation, and rapid iterative optimization. By treating the scene as a collection of surface elements with predictable normals and material properties, 3DGS enables the disentanglement of geometry, materials, and lighting, paving the way for high-fidelity, real-time relighting and editing applications that were previously infeasible with implicit neural fields. This foundational understanding sets the stage for exploring the specific techniques used to decompose material and lighting properties, as discussed in the following section.\n\n### 7.2 Material and Lighting Decomposition\n\nMaterial and lighting decomposition is a cornerstone of inverse rendering, aiming to disentangle the intrinsic properties of a scene\u2014such as albedo, roughness, and specular parameters\u2014from extrinsic factors like illumination and viewing geometry. In the context of 3D Gaussian Splatting (3DGS), this task is uniquely challenging yet promising. Unlike implicit volumetric representations like Neural Radiance Fields (NeRFs), 3DGS offers an explicit, point-based representation that facilitates direct manipulation and real-time rendering, making it an ideal backbone for inverse rendering pipelines. The explicit nature of Gaussians allows for the direct assignment of material attributes to each primitive, enabling efficient optimization of Spatially-Varying Bidirectional Reflectance Distribution Functions (SVBRDF) and lighting conditions.\n\nEarly efforts to integrate material decomposition into 3DGS focused on extending the representation capabilities of the primitives. The foundational work on 3D Gaussian Splatting [81] primarily focused on novel view synthesis, representing view-dependent appearance using Spherical Harmonics (SH). While effective for appearance, SH is a low-frequency representation that struggles to capture the high-frequency details inherent in specular reflections and complex material properties. This limitation spurred the development of methods specifically designed to model sophisticated appearance. For instance, [17] identifies the inadequacy of standard SH for specular and anisotropic components. It proposes replacing SH with Anisotropic Spherical Gaussians (ASGs) for each Gaussian primitive. By modeling the view-dependent appearance field with ASGs, the method can more effectively capture sharp specular highlights and anisotropic reflections, which are critical cues for inferring material properties like roughness and specular intensity. This represents a significant step beyond simple color representation, moving towards a physically meaningful decomposition of the appearance.\n\nBuilding on the improved appearance modeling, several works have proposed end-to-end frameworks for inverse rendering with 3DGS, explicitly decomposing SVBRDF parameters and lighting. A prominent example is [82], which presents a comprehensive pipeline for inverse rendering from multi-view images. In this framework, each 3D Gaussian is associated with material attributes, such as diffuse albedo, roughness, and metallic properties, which are optimized alongside the Gaussian positions and SH coefficients. To disentangle these from the observed appearance, BiGS incorporates physically-based rendering (PBR) principles into the splatting pipeline. It models the lighting environment, often using a spherical harmonic representation for the incident radiance, and computes the outgoing radiance for each Gaussian based on the microfacet BRDF model. The optimization process then minimizes the difference between the rendered image and the input photographs. This approach allows for the direct extraction of relightable assets, as the material properties are separated from the environmental lighting. The explicit nature of 3DGS in BiGS allows for efficient querying of material properties at any point in space, which is crucial for relighting and material editing applications.\n\nHowever, a fundamental challenge in decomposing materials and lighting from 3DGS is the inherent ambiguity between surface geometry, material reflectance, and illumination. The discrete, unstructured nature of Gaussians can lead to floating artifacts or inaccurate geometry, which in turn corrupts the material and lighting estimation. This geometric inaccuracy is a key reason why methods from the previous subsection, such as [77], are critical. Phys3DGS proposes a novel two-step training approach that leverages deferred rendering and a hybrid mesh-3DGS representation to address this. It first identifies a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in standard volume rendering. By adopting deferred rendering, it first extracts a mesh and then uses it to guide the optimization of material properties. This geometric prior helps to resolve the ambiguity, leading to significantly better quality in relighting compared to existing 3DGS-based inverse rendering methods. The use of a mesh allows for more traditional and robust surface-based material estimation, while still leveraging the fast rendering of 3DGS for the final appearance.\n\nFurthermore, the integration of generative models and advanced priors is beginning to influence material and lighting decomposition in 3DGS. While not directly covered by the provided papers in the context of inverse rendering, the principles from [83] can be extended. RefGaussian focuses on separating reflections from transmission, a task closely related to material decomposition. By splitting the appearance into transmitted and reflected components using two sets of SH, it implicitly disentangles specular (reflection) from diffuse (transmission) components. This disentanglement is a prerequisite for accurate material estimation, as specular reflections are tightly coupled with surface roughness and specular color. The local regularization techniques used in RefGaussian ensure plausible decomposition, which could be directly applied to constrain the optimization of SVBRDF parameters, preventing the common issue of mistaking lighting variations for material changes.\n\nThe ultimate goal of these decomposition methods is to produce relightable assets. [82] explicitly targets this by enabling real-time relighting under novel lighting conditions. The ability to render an object under new environmental maps, with correct specular highlights and shadowing, is the definitive test of a successful material and lighting decomposition. The efficiency of 3DGS rendering is a key enabler here, allowing for interactive relighting that would be computationally prohibitive with volumetric methods like NeRF. The explicit control over each Gaussian's attributes means that changing the lighting environment simply requires re-evaluating the BRDF and lighting model for each primitive, a process that is highly parallelizable and fast. This capability directly sets the stage for the next major challenge: modeling the complex light transport phenomena, such as indirect illumination and cast shadows, that are essential for truly photorealistic relighting.\n\nIn summary, the field of material and lighting decomposition within 3DGS-based inverse rendering is rapidly evolving. It has progressed from simple view-dependent color models [81] to sophisticated representations that use advanced Gaussian kernels [17] and physically-based rendering models [82]. The primary challenges lie in resolving the inherent ambiguities between geometry, materials, and lighting, which has led to innovations like geometry-aware optimization [77] and disentangled appearance models [83]. These advancements are making 3DGS a powerful tool not just for view synthesis, but for creating fully relightable and editable digital assets from real-world captures.\n\n### 7.3 Handling Complex Light Transport and Indirect Illumination\n\nThe accurate modeling of complex light transport phenomena, such as indirect illumination, inter-reflections, and cast shadows, is a critical challenge in inverse rendering and relighting. While 3D Gaussian Splatting (3DGS) excels at modeling direct view-dependent appearance through spherical harmonics, the standard formulation inherently assumes that each Gaussian primitive emits light based solely on local properties and direct visibility. This simplification fails to capture the intricate energy transfer between surfaces that defines realistic lighting. To bridge this gap, recent research has focused on integrating physics-based light transport models into the efficient rasterization pipeline of 3DGS. These approaches aim to simulate complex light paths without incurring the prohibitive computational cost of full path tracing, often relying on baking-based occlusion, analytical approximations, and Monte Carlo integration techniques.\n\nA foundational step towards modeling indirect effects is the accurate handling of occlusion and shadows. In the standard 3DGS pipeline, occlusion is handled via simple alpha-blending, which does not explicitly model the blocking of light sources. To enable relighting, methods must first establish which parts of the scene are in shadow. One line of work addresses this by pre-computing or baking occlusion information. For instance, [51] identifies a critical issue with the standard volume rendering approach in inverse rendering: the influence of \"hidden Gaussians\" that lie beneath the surface but still contribute to the pixel color due to the volumetric nature of the rendering equation. To resolve this, [51] proposes a deferred rendering pipeline. In this approach, the scene is first rasterized to determine surface geometry (depth and normals), and then lighting is applied in a second pass. This separation allows for the accurate calculation of direct light visibility (shadowing) by checking the visibility between a surface point and the light source, effectively preventing hidden Gaussians from interfering with the lighting calculation. This baking-based approach to occlusion is crucial because it decouples the geometric representation from the lighting calculation, allowing for efficient re-lighting of the scene once the occlusion geometry is established.\n\nHowever, simply handling direct shadows is insufficient for a truly relightable asset; the scene must also account for light that bounces off one surface and illuminates another, known as indirect illumination or inter-reflection. Simulating this requires tracing rays through the scene, which is computationally expensive. To make this tractable within the 3DGS framework, researchers have developed ray-tracing approximations that leverage the explicit nature of Gaussian primitives. Unlike NeRF, which requires querying a neural network at many points along a ray, 3DGS primitives can be queried much more efficiently. Some methods approximate indirect lighting by treating the collection of Gaussians as a set of area light sources. For a given point on a surface, one can approximate the incoming radiance by integrating the contribution of other visible Gaussians. This can be done using Monte Carlo integration, sampling directions towards other Gaussians in the scene. Because the Gaussians are explicit, checking for occlusion along these secondary rays can be done efficiently by querying the distribution of other Gaussians, perhaps using a spatial acceleration structure like a bounding volume hierarchy (BVH) as suggested in [40]. This allows for a practical approximation of global illumination without resorting to full path tracing.\n\nFurthermore, the representation of appearance itself plays a role in how complex light transport is modeled. The standard 3DGS uses Spherical Harmonics (SH) to model view-dependent color, which is effective for glossy reflections but struggles with high-frequency specularities and complex material properties required for accurate inverse rendering. Methods like [51] (Factorised Tensorial Illumination for 3D Gaussian Splatting) propose to disentangle the outgoing radiance into a local illumination field and Bidirectional Reflectance Distribution Function (BRDF) features. By modeling the incident light field separately from the material properties, [51] provides a more physically grounded basis for relighting. While this primarily focuses on direct view-dependent effects, it lays the groundwork for incorporating indirect lighting. If the incident illumination field itself is modeled to include indirect contributions (e.g., by baking the indirect light into a separate field), then the BRDF-based rendering can naturally incorporate these effects. Similarly, [84] (Bidirectional Gaussian Primitives) introduces a unified appearance model for both surface and volumetric materials using bidirectional spherical harmonics, making it more compatible with volumetric representations like 3DGS and enabling real-time relighting. The key insight is that by having a more expressive representation of light interaction at each primitive, the system can better approximate the complex results of indirect light transport.\n\nAnother significant challenge in modeling complex light transport is the accurate representation of geometry. Indirect lighting is highly dependent on the precise shape and orientation of surfaces. Standard 3DGS can produce \"floaters\" or inaccurate geometry in un-textured regions, which would lead to incorrect shadowing and light bouncing. To address this, methods have been developed to enforce geometric consistency. For example, [Gaussian Opacity Fields (GOF allows for the the extraction of a clean surface mesh from the Gaussians, which can then be used for more accurate ray-tracing calculations for indirect lighting. By regularizing the Gaussians to form a coherent surface, the underlying geometry becomes more reliable for simulating light paths. This is a form of baking the complex Gaussian representation into a more traditional geometric form (a mesh) that is better suited for physics-based light transport simulations. The mesh can then be used in a ray tracer to compute indirect illumination maps or shadow maps, which can subsequently be applied during the 3DGS rendering process.\n\nThe use of Monte Carlo integration is a recurring theme in approximating complex light transport. Instead of analytically solving the rendering equation, which is intractable for general scenes, these methods sample light paths. For instance, to compute the indirect illumination at a point, a method might sample several directions, trace rays in these directions, and query the 3DGS representation to see what they hit. The contribution of these hits is then accumulated. To reduce noise and variance, which are inherent to Monte Carlo methods, techniques like importance sampling are used. For example, one might sample directions more frequently towards other bright Gaussians in the scene. The efficiency of this process hinges on the speed of ray-Gaussian intersection tests. While standard 3DGS is optimized for rasterization, recent work like [40] has developed specialized algorithms to perform ray tracing against a scene of Gaussians efficiently, sometimes by building BVHs or using hardware-accelerated ray tracing. This opens the door to more accurate, albeit potentially slower, simulations of complex light paths directly on the Gaussian representation.\n\nIn summary, handling complex light transport in 3DGS is an active area of research that involves a trade-off between physical accuracy and computational efficiency. The primary strategies involve: 1) **Baking-based occlusion** as seen in [51] to establish reliable direct lighting and shadowing by separating geometry from lighting passes. 2) **Ray-tracing approximations** that leverage the explicit nature of Gaussians to simulate secondary bounces, often using Monte Carlo integration as suggested by the principles in [40] and the unified volumetric models in [85]. 3) **Enhanced appearance models** like those in [51] and [84] that disentangle lighting and materials, providing a more physically plausible foundation for relighting, including the potential for baked indirect light. 4) **Improved geometric regularization** through methods like [86] to ensure that the underlying surfaces for light interaction are accurate. These approaches collectively push the boundaries of what is possible with 3DGS, moving it from a pure view synthesis method towards a fully relightable, inverse rendering tool capable of capturing the richness of real-world light transport.\n\n### 7.4 Geometry Refinement and Normal Estimation\n\nThe explicit, point-based nature of 3D Gaussian Splatting (3DGS) offers unparalleled speed and flexibility for novel view synthesis, but it presents significant challenges for recovering accurate geometry and surface normals. Unlike implicit representations like Neural Radiance Fields (NeRFs) or Signed Distance Fields (SDFs), which model a continuous scene field, 3DGS represents the scene as a collection of discrete, anisotropic 3D ellipsoids. While these primitives excel at modeling view-dependent appearance, their discrete nature often leads to geometric artifacts, particularly in unobserved or low-texture regions. The optimization process, driven primarily by photometric consistency, can result in \"floaters\" (spurious Gaussians suspended in free space), noisy surfaces, and a lack of geometric consistency. Recovering high-fidelity geometry and reliable surface normals from this representation is a critical prerequisite for downstream inverse rendering tasks such as relighting, material decomposition, and physical simulation. This subsection examines the core challenges in geometric recovery and reviews recent advancements, including regularization techniques, integration with SDF priors, and deferred rendering strategies, to mitigate artifacts and ensure surface consistency.\n\nA fundamental challenge in 3DGS geometry is the ambiguity between geometry and appearance. The optimization process minimizes the photometric error between rendered and ground-truth images, but this objective alone does not strongly constrain the underlying geometry in textureless or occluded areas. A set of Gaussians can perfectly reconstruct the training views while having a physically implausible geometric arrangement. To address this, several methods introduce geometric regularization terms that encourage the Gaussians to form a clean, continuous surface. For instance, methods like **GeoGaussian** [26] introduce constraints that promote the alignment of Gaussians onto surfaces and penalize sparse or noisy distributions. These regularization techniques often rely on assumptions about the scene, such as local planarity or smoothness, to guide the densification and optimization process. By explicitly regularizing the geometry, these approaches can significantly reduce floaters and produce cleaner surfaces, which is essential for generating accurate normals. The goal is to move beyond a purely appearance-driven optimization towards a more holistic reconstruction that respects both photometric and geometric priors.\n\nA particularly powerful approach to bridging the gap between discrete Gaussians and continuous surfaces involves integrating 3DGS with implicit Signed Distance Field (SDF) representations. SDFs are inherently well-suited for modeling continuous surfaces and provide a natural way to extract a mesh and compute accurate normals via the gradient of the SDF. The core idea behind these hybrid methods is to use the SDF to define the underlying surface geometry, while the 3D Gaussians are used to model the complex, view-dependent appearance that is difficult to capture with SDFs alone. A prominent example of this paradigm is **GS-ROR** [87], which leverages an SDF prior to regularize the Gaussian primitives. In such frameworks, the optimization is formulated as a bi-level problem: the SDF field defines the surface, and the Gaussians are optimized to fit the appearance conditioned on this geometry. This decoupling helps resolve the shape-radiance ambiguity that plagues both NeRFs and 3DGS. By anchoring the geometry to a continuous SDF, the method can prevent the geometric deterioration often seen in standard 3DGS, especially in non-textured regions. The SDF provides a strong prior that the scene should be a continuous surface, which naturally leads to smoother geometry and more reliable normal estimates, as the normals are derived from the SDF gradient rather than from the covariance of individual, potentially noisy, Gaussians.\n\nAnother significant source of geometric artifacts in standard 3DGS stems from its rendering pipeline. The alpha-blending process, which sorts and composites Gaussians based on their depth, can lead to incorrect occlusion relationships and a lack of surface continuity, especially when Gaussians are large or poorly conditioned. To address this, **Phys3DGS** [26] introduces a deferred rendering strategy. Instead of directly blending Gaussians to produce the final pixel color, deferred rendering first rasterizes geometric attributes like depth and normal into a screen-space buffer. These buffers can then be processed or refined before the final appearance is computed. This separation of geometric and appearance rendering allows for more sophisticated processing of the geometry. For example, it enables the application of screen-space filters to smooth the depth map or correct normal orientation, mitigating the \"shimmering\" artifacts and geometric noise that can arise from the direct blending of discrete primitives. By deferring the final color computation, these methods can ensure that the geometric representation is consistent and well-behaved before applying complex view-dependent effects, leading to a more robust and accurate surface reconstruction.\n\nThe recovery of surface normals is a particularly difficult task due to the discrete and unstructured nature of the Gaussian cloud. While a mesh provides a clear definition of a normal at each vertex or face, a collection of Gaussians does not. Early attempts might compute normals from the principal axis of the largest Gaussian at a given location, but this is highly sensitive to noise and local variations. The integration with SDFs, as seen in **GS-ROR** [87], provides a principled solution. Once the scene is represented by a continuous SDF, the surface normal at any point can be robustly computed as the normalized gradient of the SDF, $\\mathbf{n} = \\nabla \\text{SDF}(\\mathbf{x})$. This yields smooth and consistent normals that are essential for inverse rendering. Similarly, the deferred rendering approach of **Phys3DGS** [26] facilitates normal estimation by first generating a dense normal map in screen space. This map can be regularized or refined using techniques from traditional computer graphics, such as bilateral filtering, to remove noise while preserving sharp edges. The resulting high-quality normal maps are then used to compute the final shaded appearance, creating a virtuous cycle where better geometry leads to better rendering, which in turn provides a better signal for further geometric refinement.\n\nIn summary, achieving high-fidelity geometry from 3D Gaussian Splatting requires moving beyond the basic photometric loss and incorporating explicit geometric priors and rendering modifications. Regularization techniques like those in **GeoGaussian** [26] provide a first step by encouraging cleaner Gaussian distributions. However, the most promising results come from hybrid approaches that combine the strengths of explicit and implicit representations. By using SDFs to model the continuous surface (**GS-ROR** [87]) or by adopting deferred rendering to decouple geometry from appearance (**Phys3DGS** [26]), these methods effectively mitigate the inherent artifacts of discrete point-based representations. They provide a pathway to recover smooth, continuous surfaces and accurate normals, unlocking the full potential of 3DGS for demanding inverse rendering and relighting applications that rely on a,\n\n### 7.4 Geometry Refinement and Normal Estimation\n\n### 7.4 Geometry Refinement and Normal Estimation\n\nThe explicit, point-based nature of 3D Gaussian Splatting (3DGS) offers unparalleled speed and flexibility for novel view synthesis, but it presents significant challenges for recovering accurate geometry and surface normals. Unlike implicit representations like Neural Radiance Fields (NeRFs) or Signed Distance Fields (SDFs), which model a continuous scene field, 3DGS represents the scene as a collection of discrete, anisotropic 3D ellipsoids. While these primitives excel at modeling view-dependent appearance, their discrete nature often leads to geometric artifacts, particularly in unobserved or low-texture regions. The optimization process, driven primarily by photometric consistency, can result in \"floaters\" (spurious Gaussians suspended in free space), noisy surfaces, and a lack of geometric consistency. Recovering high-fidelity geometry and reliable surface normals from this representation is a critical prerequisite for downstream inverse rendering tasks such as relighting, material decomposition, and physical simulation, as well as for the advanced generative synthesis discussed in the following section. This subsection examines the core challenges in geometric recovery and reviews recent advancements, including regularization techniques, integration with SDF priors, and deferred rendering strategies, to mitigate artifacts and ensure surface consistency.\n\nA fundamental challenge in 3DGS geometry is the ambiguity between geometry and appearance. The optimization process minimizes the photometric error between rendered and ground-truth images, but this objective alone does not strongly constrain the underlying geometry in textureless or occluded areas. A set of Gaussians can perfectly reconstruct the training views while having a physically implausible geometric arrangement. To address this, several methods introduce geometric regularization terms that encourage the Gaussians to form a clean, continuous surface. For instance, methods like **GeoGaussian** [26] introduce constraints that promote the alignment of Gaussians onto surfaces and penalize sparse or noisy distributions. These regularization techniques often rely on assumptions about the scene, such as local planarity or smoothness, to guide the densification and optimization process. By explicitly regularizing the geometry, these approaches can significantly reduce floaters and produce cleaner surfaces, which is essential for generating accurate normals. The goal is to move beyond a purely appearance-driven optimization towards a more holistic reconstruction that respects both photometric and geometric priors.\n\nA particularly powerful approach to bridging the gap between discrete Gaussians and continuous surfaces involves integrating 3DGS with implicit Signed Distance Field (SDF) representations. SDFs are inherently well-suited for modeling continuous surfaces and provide a natural way to extract a mesh and compute accurate normals via the gradient of the SDF. The core idea behind these hybrid methods is to use the SDF to define the underlying surface geometry, while the 3D Gaussians are used to model the complex, view-dependent appearance that is difficult to capture with SDFs alone. A prominent example of this paradigm is **GS-ROR** [87], which leverages an SDF prior to regularize the Gaussian primitives. In such frameworks, the optimization is formulated as a bi-level problem: the SDF field defines the surface, and the Gaussians are optimized to fit the appearance conditioned on this geometry. This decoupling helps resolve the shape-radiance ambiguity that plagues both NeRFs and 3DGS. By anchoring the geometry to a continuous SDF, the method can prevent the geometric deterioration often seen in standard 3DGS, especially in non-textured regions. The SDF provides a strong prior that the scene should be a continuous surface, which naturally leads to smoother geometry and more reliable normal estimates, as the normals are derived from the SDF gradient rather than from the covariance of individual, potentially noisy, Gaussians.\n\nAnother significant source of geometric artifacts in standard 3DGS stems from its rendering pipeline. The alpha-blending process, which sorts and composites Gaussians based on their depth, can lead to incorrect occlusion relationships and a lack of surface continuity, especially when Gaussians are large or poorly conditioned. To address this, **Phys3DGS** [26] introduces a deferred rendering strategy. Instead of directly blending Gaussians to produce the final pixel color, deferred rendering first rasterizes geometric attributes like depth and normal into a screen-space buffer. These buffers can then be processed or refined before the final appearance is computed. This separation of geometric and appearance rendering allows for more sophisticated processing of the geometry. For example, it enables the application of screen-space filters to smooth the depth map or correct normal orientation, mitigating the \"shimmering\" artifacts and geometric noise that can arise from the direct blending of discrete primitives. By deferring the final color computation, these methods can ensure that the geometric representation is consistent and well-behaved before applying complex view-dependent effects, leading to a more robust and accurate surface reconstruction.\n\nThe recovery of surface normals is a particularly difficult task due to the discrete and unstructured nature of the Gaussian cloud. While a mesh provides a clear definition of a normal at each vertex or face, a collection of Gaussians does not. Early attempts might compute normals from the principal axis of the largest Gaussian at a given location, but this is highly sensitive to noise and local variations. The integration with SDFs, as seen in **GS-ROR** [87], provides a principled solution. Once the scene is represented by a continuous SDF, the surface normal at any point can be robustly computed as the normalized gradient of the SDF, $\\mathbf{n} = \\nabla \\text{SDF}(\\mathbf{x})$. This yields smooth and consistent normals that are essential for inverse rendering. Similarly, the deferred rendering approach of **Phys3DGS** [26] facilitates normal estimation by first generating a dense normal map in screen space. This map can be regularized or refined using techniques from traditional computer graphics, such as bilateral filtering, to remove noise while preserving sharp edges. The resulting high-quality normal maps are then used to compute the final shaded appearance, creating a virtuous cycle where better geometry leads to better rendering, which in turn provides a better signal for further geometric refinement.\n\nIn summary, achieving high-fidelity geometry from 3D Gaussian Splatting requires moving beyond the basic photometric loss and incorporating explicit geometric priors and rendering modifications. Regularization techniques like those in **GeoGaussian** [26] provide a first step by encouraging cleaner Gaussian distributions. However, the most promising results come from hybrid approaches that combine the strengths of explicit and implicit representations. By using SDFs to model the continuous surface (**GS-ROR** [87]) or by adopting deferred rendering to decouple geometry from appearance (**Phys3DGS** [26]), these methods effectively mitigate the inherent artifacts of discrete point-based representations. They provide a pathway to recover smooth, continuous surfaces and accurate normals, unlocking the full potential of 3DGS for demanding inverse rendering and relighting applications that rely on a physically plausible geometric foundation.\n\n### 7.5 Integration with Generative Models for Relightable Synthesis\n\nThe integration of 3D Gaussian Splatting (3DGS) with generative models represents a significant leap forward in creating high-fidelity, relightable 3D assets from minimal inputs such as text prompts or single images. This synergy leverages the explicit, fast-rendering nature of 3DGS to serve as an optimized canvas for generative priors, effectively bridging the gap between 2D generative content and controllable 3D scenes. As established in the previous discussion on geometry and normal estimation, achieving a physically plausible geometric foundation is a prerequisite for inverse rendering. Generative approaches build upon this foundation, tackling the core challenge of ensuring that the generated assets possess not only photorealistic appearance but also decomposable material properties (albedo, roughness, metallic) that respond correctly to novel lighting conditions.\n\nA primary paradigm for leveraging 2D priors in 3D synthesis is optimization-based methods, notably Score Distillation Sampling (SDS). Originally popularized by text-to-3D works like DreamFusion, SDS provides a mechanism to distill the knowledge of a 2D diffusion model into a 3D representation by treating the 3D parameters as a learnable variable that maximizes the likelihood of the diffusion model\u2019s predictions. When applied to 3DGS for relightable synthesis, the optimization process is adapted to account for lighting and material decomposition. Instead of optimizing a single appearance model, the pipeline typically parameterizes the scene into a geometry (represented by Gaussian positions and opacities), a spatially varying material (SVBRDF), and an environment map. The SDS gradient is then computed with respect to these components, often requiring a relightable rendering pass where the 3DGS is illuminated by a sampled lighting condition before being fed into the 2D prior. This ensures that the generative model guides the creation of geometry and albedo independently of the specific lighting used during training, resulting in assets that can be re-lit in post-production.\n\nHowever, standard SDS suffers from high variance and often leads to over-saturated or \"blobs\" geometry, particularly when dealing with the complex light transport required for realistic materials. To address this, recent advancements have introduced Variational Score Distillation (VSD), as seen in methods like ProlificDreamer. VSD reformulates the distillation process by modeling the distribution of 3D parameters as a variational distribution, reducing the variance of gradients significantly. In the context of inverse rendering, VSD allows for more stable optimization of fine-grained material details, such as specular highlights and micro-surface details, which are crucial for relightability. By sampling multiple views and treating the 3D representation as a particle system, VSD ensures that the generated geometry and material properties are consistent across different viewpoints and lighting scenarios.\n\nBeyond pure optimization, hybrid pipelines have emerged that combine the speed of feed-forward generation with the fidelity of SDS refinement. For instance, frameworks like GaussianDreamer utilize multi-view diffusion models to generate consistent priors for geometry and texture, which are then used to initialize a 3DGS scene. Once initialized, a lightweight SDS-based refinement stage is applied to enhance the material properties and ensure relightability. This two-stage approach significantly reduces generation time compared to end-to-end optimization while maintaining high quality. The feed-forward stage provides a coarse geometric scaffold and basic texture, while the refinement stage focuses on disentangling view-dependent appearance from diffuse albedo, a task that is difficult to achieve from scratch.\n\nTo specifically address the lack of 3D consistency in 2D priors, which often manifests as texture flickering or geometric artifacts in relightable assets, researchers have integrated depth and normal priors into the distillation process. Methods such as 3DFuse and RetDream inject geometric conditioning into the diffusion model, ensuring that the generative process respects the 3D structure of the Gaussian primitives. In a relightable synthesis context, this is critical. If the geometry is inconsistent, the shading calculated from the lighting model will be incorrect, breaking the illusion of physical realism. By aligning the diffusion model's predictions with depth maps rendered from the current state of the 3DGS, the optimization loop can enforce geometric consistency, leading to cleaner surfaces and more accurate light interaction.\n\nFurthermore, the explicit nature of 3DGS facilitates the use of spatially aware generative priors. Unlike implicit fields, where querying specific regions can be computationally expensive, 3D Gaussians allow for direct manipulation of attributes based on semantic or spatial conditioning. Recent work has explored using localized score distillation, where the generative prior is applied only to specific regions of the scene defined by user masks or semantic segmentation. This allows for the editing or generation of specific materials (e.g., making a part metallic or rough) within a larger scene without affecting the rest of the geometry. This capability is essential for practical applications in digital content creation, where artists often need to iterate on specific parts of a scene.\n\nThe integration of generative models also opens avenues for creating relightable assets from single images, a task that is inherently ill-posed. By utilizing a single image prior (such as ControlNet or Inpainting models) combined with a 3DGS representation, the system can hallucinate the unseen parts of the object and estimate the material properties. For example, MaterialFusion-like approaches (conceptually similar to the cited MaterialFusion) use the 2D prior to predict the SVBRDF parameters (albedo, roughness, normal) from the single view, and then project these into the 3DGS structure. The 3D consistency is then enforced by rendering the object under different lighting conditions and comparing it against the generative prior's expectation, effectively \"locking in\" the material properties.\n\nHowever, significant challenges remain. The computational cost of running large diffusion models during optimization is prohibitive, and the memory bandwidth required to update thousands of Gaussians with gradients derived from these models is substantial. Additionally, the disentanglement of lighting and material is not perfect; generative models trained on natural images often bake lighting into the albedo, requiring sophisticated regularization techniques to separate them. Future research likely lies in distilling these large generative priors into smaller, specialized networks that can run in real-time alongside 3DGS rendering, enabling interactive relightable synthesis. As the field moves towards 4D generation, the combination of 3DGS inverse rendering and generative priors will likely become the standard for creating dynamic, relightable digital twins of the real world.\n\n### 7.6 Real-time Rendering and Downstream Applications\n\nThe practical deployment of inverse rendering techniques has historically been constrained by the computational intensity of volumetric rendering pipelines, which often preclude real-time interaction. However, the advent of 3D Gaussian Splatting (3DGS) has fundamentally altered this landscape by providing an explicit, high-fidelity representation that supports differentiable rasterization at real-time frame rates. Building upon the generative methods discussed previously, which create the initial relightable assets, this subsection explores the downstream applications specifically enabled by the fusion of fast 3DGS rendering and inverse rendering capabilities. We focus on real-time relighting, virtual object insertion, material editing, and interactive AR/VR systems. By leveraging the explicit nature of Gaussian primitives, these applications achieve performance benefits that are unattainable with traditional mesh-based pipelines or implicit volumetric representations like Neural Radiance Fields (NeRFs).\n\n### Real-Time Relighting and Dynamic Illumination\n\nOne of the most significant contributions of 3DGS to inverse rendering is the ability to perform real-time relighting. Traditional inverse rendering pipelines often require baking lighting information into the geometry or relying on slow Monte Carlo integration to simulate light transport. In contrast, 3DGS-based methods decompose the scene into geometry, material, and lighting components, allowing for the direct manipulation of illumination while maintaining high rendering speeds.\n\nFor instance, methods like [82] introduce bidirectional spherical harmonics to model light- and view-dependent scattering. This approach unifies surface and volumetric materials within a cohesive appearance model, enabling the reproduction of photorealistic appearances under novel lighting conditions in real time. Similarly, [88] utilizes mixtures of spherical Gaussians to represent specular BRDFs and environmental illumination. By coupling this with a differentiable renderer, PhySG allows for physics-based appearance editing and relighting, demonstrating that high-quality inverse rendering can be achieved without sacrificing the interactivity provided by 3DGS.\n\nFurthermore, the explicit nature of Gaussians allows for efficient computation of shadowing and occlusion, which is critical for realistic relighting. While early 3DGS methods struggled with complex light transport, recent advancements in baking-based occlusion and ray-tracing approximations have integrated these capabilities into the rasterization pipeline. This allows users to dynamically change light direction and intensity, observing real-time updates to shadows and specular highlights, a capability that is computationally prohibitive in volumetric NeRF-based pipelines.\n\n### Virtual Object Insertion and Scene Composition\n\nVirtual object insertion requires a deep understanding of the scene's geometry, lighting, and material properties to ensure the inserted object blends seamlessly with the environment. 3DGS-based inverse rendering excels in this domain by providing an explicit representation of the scene that can be queried for lighting information and occlusion.\n\nThe fast rendering speed of 3DGS is a decisive advantage here. In interactive applications, users may wish to place virtual objects in various locations and orientations. Traditional methods would require re-computing global illumination or baking light maps for each configuration, leading to long delays. However, with 3DGS, the explicit representation allows for immediate rendering of the virtual object under the estimated environmental lighting. The inverse rendering pipeline provides the necessary decomposition\u2014separating diffuse albedo from specular components and ambient lighting\u2014ensuring that the virtual object casts correct shadows and reflects the surrounding environment accurately. This capability is essential for AR applications where digital content must interact convincingly with the physical world.\n\n### Interactive Material Editing\n\nMaterial editing is another domain where the performance of 3DGS shines. Modifying the material properties of an object in a reconstructed scene\u2014such as changing a surface from matte to glossy or altering its color\u2014requires re-evaluating the rendering equation. Implicit methods like NeRF typically require time-consuming optimization to propagate such changes.\n\nIn contrast, 3DGS-based inverse rendering frameworks, such as those utilizing [21] for geometry extraction, allow for direct manipulation of Gaussian attributes. Since each Gaussian primitive carries attributes for opacity, color (often via spherical harmonics), and covariance, editing these parameters results in an immediate visual update. For example, [83] proposes splitting the scene into transmitted and reflected components. This disentanglement allows users to selectively edit reflections or transmission properties independently, a granular level of control that is difficult to achieve with monolithic volumetric representations. The ability to perform these edits interactively, with real-time feedback, transforms inverse rendering from an offline batch process into a creative tool for digital artists.\n\n### Interactive AR/VR Systems and Downstream Applications\n\nThe culmination of real-time relighting, object insertion, and material editing is the enablement of interactive AR/VR systems. The low latency and high fidelity of 3DGS are critical for maintaining immersion in virtual environments. Traditional mesh-based pipelines require complex geometry processing and shader compilation, while volumetric pipelines suffer from slow inference times. 3DGS bridges this gap.\n\nThe explicit representation facilitates efficient culling and level-of-detail (LoD) management, which is crucial for rendering large-scale scenes in VR. Furthermore, the ability to perform inverse rendering on the fly allows for dynamic adaptation to changing lighting conditions in the real world. For example, a VR system could scan a room using a mobile device, reconstruct it using 3DGS, estimate the lighting via inverse rendering, and then allow the user to interact with virtual objects that react realistically to the physical lighting.\n\nAdditionally, the integration of 3DGS with specialized sensors, as seen in [89], demonstrates the potential for real-time mapping and rendering in robotics and AR. By fusing visual, depth, and inertial data, these systems can build accurate maps and render them in real-time, providing a foundation for downstream tasks like navigation and interaction. The performance benefits over traditional pipelines are stark: where mesh-based SLAM might struggle with dense reconstruction or volumetric SLAM might lag, 3DGS provides a smooth, photorealistic experience essential for user comfort in VR.\n\nIn conclusion, the shift to 3D Gaussian Splatting in inverse rendering pipelines unlocks a suite of applications that demand both high visual quality and real-time interactivity. By moving away from the computational bottlenecks of volumetric rendering and leveraging the explicit, editable nature of Gaussian primitives, researchers have paved the way for next-generation digital content creation tools and immersive experiences.\n\n## 8 Downstream Applications: SLAM, Robotics, and Specialized Domains\n\n### 8.1 Real-Time SLAM and Odometry with 3DGS\n\nThe integration of 3D Gaussian Splatting (3DGS) into Simultaneous Localization and Mapping (SLAM) systems marks a pivotal shift in real-time spatial computing, offering a compelling alternative to traditional dense SLAM methods and implicit neural representations like Neural Radiance Fields (NeRF). While NeRF-based SLAM systems demonstrated the potential of neural implicit fields for high-fidelity scene reconstruction, they were fundamentally limited by their computational overhead, often requiring minutes to hours for training and rendering, which precluded true real-time operation [1]. In contrast, the explicit, point-based nature of 3DGS, coupled with its highly optimized tile-based rasterization pipeline, enables simultaneous tracking and mapping at interactive frame rates, effectively bridging the gap between geometric accuracy and photorealism in robotics and augmented reality applications.\n\nThe core innovation driving 3DGS-SLAM is the utilization of the differentiable rendering pipeline for pose tracking. Unlike traditional feature-based SLAM that relies on sparse keypoints, or dense SLAM that minimizes photometric error over pixel intensities, 3DGS-SLAM optimizes a global map of anisotropic 3D Gaussians while concurrently refining the camera trajectory. This is achieved by rendering the current map from the estimated camera pose and minimizing the photometric loss against the incoming frame. Because 3DGS rendering is exceptionally fast\u2014capable of producing high-resolution images in milliseconds on consumer GPUs\u2014the optimization loop for both pose and map can run in real-time. This capability stands in stark contrast to the volumetric ray marching required by NeRF, which necessitates hundreds of network queries per pixel, making it intractable for real-time feedback loops [1].\n\nEarly attempts to bridge this gap, such as those explored in \"NeRF-VPT,\" attempted to optimize NeRF models more efficiently, but the fundamental architecture remained a bottleneck. The arrival of 3DGS provided a paradigm shift. In systems like GS-SLAM, the map is represented as a collection of 3D Gaussians with learnable attributes (position, covariance, opacity, and color). As the camera moves, new Gaussians are initialized and optimized based on the current depth and color observations. The differentiability of the rendering process allows gradients from the image reconstruction loss to flow back not only to the Gaussian parameters but also to the camera pose parameters. This joint optimization ensures that the estimated trajectory is geometrically consistent with the reconstructed scene. The efficiency of this process is further enhanced by the fact that 3DGS does not require a pre-defined voxel grid or a complex neural network to query density; the map is simply the set of Gaussians itself, allowing for rapid updates and rendering [1].\n\nA critical component of these systems is the dense map optimization strategy. In traditional SLAM, the map is often sparse, consisting of point clouds that lack surface properties. In 3DGS-SLAM, the map is dense and possesses view-dependent appearance capabilities, allowing for the reconstruction of specularities and complex lighting effects that are typically lost in geometric-only SLAM. However, this explicit representation introduces challenges regarding map management and density control. Similar to the standalone 3DGS training, SLAM systems must employ adaptive density control strategies to prevent the map from becoming overly cluttered with floating Gaussians or leaving holes in the geometry. Techniques such as \"cloning\" and \"splitting\" are adapted to the streaming nature of SLAM, where the map is built incrementally. This ensures that the map remains compact and accurate as the camera explores the environment, preventing the exponential growth of Gaussian counts that could degrade performance over time.\n\nFurthermore, the integration of depth priors plays a significant role in stabilizing 3DGS-SLAM, particularly in textureless regions where photometric consistency is ambiguous. While pure vision-based SLAM can struggle here, the inclusion of depth information\u2014whether from LiDAR, stereo, or monocular depth estimators\u2014provides a strong geometric anchor. This is analogous to the multi-modal fusion techniques discussed in the following subsection on autonomous driving, where LiDAR data enhances geometric consistency in large-scale scenes. In the context of SLAM, depth priors guide the initialization of new Gaussians, ensuring they are placed on actual surfaces rather than in free space. This significantly reduces the \"floaters\" that often plague NeRF-based reconstructions and accelerates convergence, allowing the system to achieve high-fidelity mapping with fewer frames.\n\nThe specific architecture of GS-SLAM typically involves a frontend that tracks the camera pose and a backend that optimizes the Gaussian map. The frontend might use traditional visual odometry techniques or a lightweight neural network to provide an initial pose estimate, which is then refined by the rendering loss. The backend maintains the global map, performing keyframe selection to optimize the map over a sliding window of recent frames to correct for drift. This hybrid approach leverages the strengths of both classical geometry (for robust tracking) and neural rendering (for dense, high-quality reconstruction). The result is a system that can render the map from the current viewpoint in real-time, providing immediate feedback to the user or robot, a feature that was previously impossible with high-fidelity neural representations.\n\nComparing these methods to \"Block-NeRF\" highlights the difference in scale and real-time requirements. Block-NeRF focuses on large-scale offline reconstruction, partitioning the scene into manageable chunks for high-quality rendering, but it is not designed for real-time interaction. In contrast, 3DGS-SLAM aims for online, interactive performance. The explicit nature of 3DGS allows for efficient culling and level-of-detail (LoD) management, which is essential for scaling to larger environments within the strict time constraints of SLAM (typically 30ms per frame). As the map grows, systems can employ spatial partitioning structures to query only the relevant Gaussians for the current view, maintaining high frame rates even as the scene complexity increases.\n\nMoreover, the differentiable rendering pipeline of 3DGS allows for the injection of various losses that improve the geometric quality of the map. For instance, depth consistency losses can be applied by comparing the rendered depth map against sensor depth, ensuring that the volumetric density aligns with the observed geometry. This is crucial for creating maps that are not just visually pleasing but geometrically accurate enough for navigation and interaction. The ability to directly optimize the Gaussian positions and scales based on these geometric constraints differentiates 3DGS-SLAM from earlier neural SLAM approaches that struggled to recover accurate geometry due to the shape-radiance ambiguity inherent in NeRF [1].\n\nIn summary, the integration of 3D Gaussian Splatting into SLAM systems represents a significant advancement in real-time dense mapping. By replacing the computationally expensive volumetric rendering of NeRF with efficient rasterization, these systems achieve the dual goals of high-fidelity reconstruction and real-time performance. The explicit map representation facilitates easy editing, rendering, and integration with other sensors, while the differentiable nature of the pipeline enables robust pose tracking and map optimization. As research progresses, we can expect further refinements in map management and scalability, solidifying 3DGS-SLAM as the standard for next-generation robotics and immersive computing.\n\n### 8.2 Autonomous Driving and Large-Scale Urban Mapping\n\nThe application of 3D Gaussian Splatting (3DGS) within the domain of autonomous driving represents a significant leap forward in high-fidelity environmental modeling and real-time perception. Autonomous vehicles (AVs) rely heavily on accurate, scalable, and computationally efficient representations of their surroundings for localization, planning, and navigation. Traditional approaches often utilize LiDAR-based point clouds or mesh-based reconstructions, which, while geometrically accurate, often struggle with photorealistic novel view synthesis or require substantial computational overhead for rendering. The explicit nature of 3DGS offers a compelling alternative, providing a dense, unstructured point cloud that can be optimized to render photorealistic images from novel viewpoints while maintaining a relatively small memory For We We We We For We We We We We We We We We We We We We We We We We We We We We We We We For We We We We We We We We We We We We We We We We We We We We We We We We For We We We We We  We We We We We This We We We We We This For This We We We We This This We We We For This For We We We For This For For We We The This This The While We  3 One , We For For  The    For           ing,\u81ea\u7531 AR, The.  333 The, -based, The33 -based ,3,  The, .  The -of  critical3 and .,  This the3 ,, This , of ,  to, -based3   The GS .   This  of ,, -based -based .,,,, GSGS and, D ** ** explicit, the forations, able for for map  map generation and, , ,   **  for  and for,3,., 3DGS pipeline\u2014which assumes a bounded volume of interest\u2014insufficient. To address this, researchers have developed frameworks specifically designed for large-scale urban mapping. A notable contribution in this area is the work on **HGS-Mapping**, which explicitly targets the reconstruction of unbounded urban scenes [60]. This method likely addresses the issue of scaling 3DGS to kilometers of driving trajectories by employing hierarchical representations or partitioning strategies that allow the system to manage memory usage effectively while preserving detail in regions of interest. The ability to reconstruct large-scale scenes without succumbing to memory exhaustion is critical for AVs, which must map entire cities or complex highway systems. Furthermore, the explicit representation facilitates the extraction of geometric features, which are essential for downstream tasks such as collision avoidance and path planning.\n\nThe integration of multi-sensor fusion, particularly combining LiD and camera data, is another critical area of development. While 3DGS is fundamentally a vision-based method initialized from Structure-from-Motion (SfM) points, autonomous driving platforms are typically equipped with high-precision LiDAR sensors. Fusing LiDAR priors with 3DGS optimization can significantly enhance geometric consistency and reduce the \"floaters\" or artifacts that often plague pure vision-based reconstructions in textureless areas\u2014a common occurrence in urban environments (e.g., asphalt roads, building facades). By initializing the 3D Gaussians with LiDAR points or using LiDAR depth as a regularization term during optimization, the resulting scene representation becomes more robust. This multi-modal approach ensures that the visual fidelity provided by 3DGS is grounded in accurate physical measurements, bridging the gap between photorealistic rendering and geometric precision required for safety-critical applications.\n\nMoreover, the efficiency of the 3DGS rasterization pipeline makes it suitable for real-time HD map generation and visualization. Unlike volumetric methods that require ray marching, 3DGS projects Gaussians onto the screen using a tile-based rasterizer, allowing for high frame rates even on consumer-grade GPUs. This speed is advantageous for generating on-the-fly visualizations of the environment for debugging or for human-machine interface displays in the vehicle. However, the standard 3DGS pipeline still faces bottlenecks when rendering large-scale scenes with millions of Gaussians, as noted in the literature regarding scalability and rendering efficiency [60]. To mitigate this, techniques such as Level-of-Detail (LoD) management and hierarchical culling are essential. These methods allow the rendering system to dynamically adjust the density of Gaussians based on the camera's distance, ensuring consistent frame rates regardless of the scene's complexity.\n\nIn the context of HD map generation, 3DGS provides a dense representation that can be used to infer semantic information. While the original 3DGS method focuses on appearance, subsequent works have explored semantic-aware regularization to group Gaussians by instance or category [31]. For autonomous driving, this implies that a 3DGS map could serve not only as a visual backdrop but also as a source of semantic segmentation, identifying drivable areas, sidewalks, and obstacles. The explicit nature of the Gaussians allows for easy manipulation and querying of the scene, which is a distinct advantage over implicit representations.\n\nHowever, the deployment of 3DGS in autonomous driving is not without challenges. The unbounded nature of outdoor scenes often leads to aliasing artifacts when rendering at varying distances, a problem that has been addressed in the literature by methods like **Analytic-Splatting** [20]. In an AV context, where the vehicle moves rapidly through the environment, maintaining visual consistency across different scales is crucial to prevent artifacts that could confuse perception algorithms. Additionally, the storage requirements for large-scale 3DGS maps can be substantial. Although 3DGS is more efficient than NeRFs, storing millions of Gaussians for a city-scale map requires significant disk space. Compression techniques, such as those proposed in **Compact 3D Gaussian Representation for Radiance Field** [52], are therefore vital to make these maps practical for distribution and storage in vehicles with limited memory.\n\nFinally, the robustness of 3DGS optimization to varying lighting conditions and weather\u2014common in autonomous driving scenarios\u2014remains an area of active research. While 3DGS excels at reconstructing static scenes under consistent lighting, dynamic changes in illumination (e.g., transitioning from sunny to cloudy, or driving through tunnels) can degrade performance. Future directions likely involve integrating inverse rendering capabilities to separate lighting from geometry, allowing for relightable HD maps that remain valid under diverse environmental conditions. By leveraging the explicit, fast-rendering properties of 3DGS and addressing its scalability and robustness limitations, autonomous driving systems can achieve unprecedented levels of environmental awareness and visual fidelity.\n\n### 8.3 Embedded Systems and Resource-Constrained Robotics\n\nThe deployment of advanced 3D mapping and Simultaneous Localization and Mapping (SLAM) algorithms on resource-constrained hardware represents a critical frontier for the practical application of 3D Gaussian Splatting (3DGS) in robotics and autonomous systems. While the original 3DGS formulation achieves impressive real-time rendering speeds on high-end desktop GPUs, its memory footprint and computational demands often exceed the capabilities of embedded platforms commonly used in mobile robots, such as NVIDIA Jetson modules. Consequently, a significant body of research has emerged focused on optimizing 3DGS for these constrained environments, targeting reductions in memory usage, energy consumption, and computational latency to enable real-time operation on edge devices.\n\nA primary challenge in deploying 3DGS on embedded systems is the sheer number of Gaussian primitives required to represent complex scenes. The explicit nature of 3DGS, while beneficial for rendering speed, leads to a memory footprint that scales with scene complexity, often requiring hundreds of megabytes or even gigabytes of storage for large environments. This is incompatible with the limited RAM of typical mobile robotics hardware. To address this, several methods focus on aggressive compression and efficient caching strategies. For instance, techniques like \"Reducing the Memory Footprint of 3D Gaussian Splatting\" [90] tackle this by proposing resolution-aware primitive pruning, adaptive spherical harmonic coefficient selection, and codebook-based quantization. These methods can reduce the storage size by a factor of 27 and increase rendering speed, making the representation more amenable to devices with limited memory bandwidth. Similarly, \"Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields\" [56] introduces learnable mask strategies and vector quantization to significantly reduce the number of Gaussians and compress their attributes, achieving over 25x storage reduction while maintaining visual fidelity. These compression techniques are vital for storing and transmitting 3D maps on robots with limited storage.\n\nBeyond static compression, dynamic memory management during the rendering and SLAM processes is essential. The standard 3DGS pipeline involves sorting and processing a large number of Gaussians for every frame, which is computationally expensive. For mobile platforms, methods that reduce the active set of Gaussians during rendering are highly beneficial. \"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting\" [39] proposes an offline clustering approach to group nearby Gaussians. At runtime, these clusters are projected, allowing the system to quickly identify and discard unnecessary Gaussians before the main rendering projection step. This approach reportedly excludes over 60% of Gaussians, reducing rendering computation by nearly 40% without sacrificing quality, a crucial optimization for maintaining high frame rates on low-power GPUs. Complementing this, \"FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering\" [62] introduces a Flexible Level of Detail system. This allows a scene to be rendered with varying numbers of Gaussians depending on the hardware's capabilities, enabling real-time rendering across a spectrum of devices from high-end workstations to low-memory embedded systems by adjusting the level of detail on the fly.\n\nIn the context of SLAM, where the map is built and updated continuously, efficient data structures for storing and accessing the map are paramount. \"GMMap\" and \"VoxelCache\" are concepts that highlight the need for specialized caching mechanisms. While not explicitly named in the provided papers, the principles are embodied in works like \"A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" [60]. This paper introduces a divide-and-conquer training approach and a hierarchical representation with Level-of-Detail (LoD). For a robot navigating a large environment, this hierarchy allows the system to efficiently render distant parts of the map using fewer, larger Gaussians, while dedicating resources to high-detail rendering of the immediate vicinity. This selective rendering is critical for managing the computational load on an embedded processor. Furthermore, \"Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians\" [44] directly addresses the inefficiency of spatial distribution in standard 3DGS. By reorganizing Gaussian positions through densification and simplification strategies, it achieves high rendering quality with a strictly limited budget of Gaussians, a feature that is directly transferable to resource-constrained SLAM systems that must maintain a compact map representation.\n\nEnergy consumption is another critical factor for mobile robots, particularly those operating on battery power. The computational intensity of 3DGS optimization and rendering can drain batteries quickly. Optimizations that reduce the number of floating-point operations and memory accesses directly translate to lower energy usage. The aforementioned methods for reducing Gaussian counts and efficient rendering pipelines contribute to this goal. For example, \"Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering\" [91] proposes using isotropic kernels to simplify computations, claiming a 100x speedup without significant loss in geometric accuracy. Such a drastic reduction in computational complexity would have a profound impact on the energy efficiency of a robot running a 3DGS-based mapping system. Similarly, \"3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes\" [40] explores using GPU ray tracing hardware for particle rendering. While ray tracing is traditionally computationally heavy, leveraging specialized hardware can offer efficiency gains. For embedded platforms with increasingly powerful and specialized GPUs (like those in the Jetson Orin series), such hardware-accelerated approaches could become viable for efficient rendering.\n\nThe integration of these optimizations into a cohesive SLAM system is the ultimate goal. A robot needs to not only render the map but also use it for localization and navigation. The explicit and sparse nature of 3DGS, when properly managed, can be advantageous for these tasks. By pruning unnecessary Gaussians and using hierarchical representations, the active map size remains manageable. The rendering speed, even on embedded hardware, allows for rapid generation of synthetic views for scan-matching based localization. The work on \"GS-SLAM\" and \"CG-SLAM\" (mentioned in the broader SLAM section 8.1) implicitly relies on the ability to run 3DGS efficiently. For these systems to be viable on mobile robots, they must incorporate the compression and caching strategies discussed here. For instance, a system could use a \"VoxelCache\"-like structure to keep recently observed Gaussians in fast memory, while offloading older parts of the map to slower storage, retrieving them with a hierarchical LoD system when the robot re-enters an area.\n\nIn summary, the deployment of 3DGS on embedded systems and resource-constrained robotics is an active area of research driven by the need for real-time, high-fidelity 3D mapping on the edge. Key strategies include: (1) **Data Compression**: Using quantization, codebooks, and attribute pruning to reduce storage and bandwidth requirements [90; 56]. (2) **Efficient Rendering Pipelines**: Identifying and culling invisible or unnecessary Gaussians on-the-fly to reduce per-frame computation [39]. (3) **Scalable Representations**: Implementing Level-of-Detail (LoD) and hierarchical structures to adapt rendering quality to available hardware resources and scene scale [60; 62]. (4) **Simplified Primitives**: Exploring alternative Gaussian formulations that are computationally cheaper to process [91]. These advancements are gradually bridging the gap between the high performance of 3DGS demonstrated on desktops and the practical requirements of autonomous systems operating in the real world with limited power and processing capabilities.\n\n### 8.4 Specialized Sensors and Alternative Geometric Primitives\n\nThe integration of 3D Gaussian Splatting (3DGS) into downstream applications such as Simultaneous Localization and Mapping (SLAM) and robotics has traditionally relied on standard RGB cameras. However, to achieve robust performance in challenging environments\u2014such as off-road navigation, industrial inspection, or large-scale urban mapping\u2014practitioners are increasingly turning to specialized sensors and alternative geometric primitives. These advancements address the limitations of monocular or stereo vision, particularly in scenarios with high dynamic range, low texture, or rapid motion. This subsection examines the fusion of 3DGS with specialized sensors like Solid-State LiDAR (SS-LiDAR) and Time-of-Flight (ToF) cameras, as well as the adoption of alternative geometric representations including quadrics, planes, and optimized splats to enhance mapping efficiency and structural regularity.\n\n### Specialized Sensor Fusion for Robust Mapping\n\nWhile standard cameras provide rich appearance information, they often struggle with depth estimation in textureless regions or under varying illumination. Specialized sensors offer complementary geometric cues that can be effectively fused with the 3DGS rendering pipeline. ToF cameras, for instance, provide direct depth measurements at a high frame rate, which can serve as a strong prior for initializing Gaussian positions and scales. This is particularly useful in robotics applications where real-time feedback is critical. By incorporating ToF depth, the optimization process of 3DGS can converge faster and with greater stability, reducing the \"floaters\" and geometric noise often observed in pure vision-based reconstructions.\n\nSolid-State LiDAR represents another significant advancement. Unlike mechanical LiDAR, SS-LiDAR lacks moving parts, making it more compact and durable for mobile platforms. These sensors provide sparse but highly accurate depth measurements. When integrated with 3DGS, the LiDAR points can act as anchor points for the Gaussian primitives, ensuring that the explicit representation adheres to the physical geometry of the scene. This fusion is essential for maintaining structural integrity in large-scale environments where visual odometry might drift. The synergy between dense visual data from cameras and precise geometric data from LiDAR allows for the creation of maps that are both visually photorealistic and geometrically accurate, a requirement for autonomous navigation and inspection tasks.\n\nThe mathematical formulation of 3DGS allows for the seamless integration of multi-modal data. The covariance matrix of a Gaussian, derived from scaling and rotation, can be initialized or regularized based on the uncertainty characteristics of the specialized sensor. For example, the high precision of LiDAR can be used to constrain the scale of Gaussians in the direction of the ray, preventing the blurring artifacts that occur when optimizing solely on photometric loss. This approach aligns with the broader trend in robotics of utilizing sensor fusion to overcome the inherent ambiguities of single-modality perception.\n\n### Alternative Geometric Primitives: Beyond Ellipsoids\n\nThe standard 3DGS representation utilizes anisotropic 3D ellipsoids (Gaussians) to model the scene. While highly flexible, this representation can be inefficient for modeling structured environments dominated by planar surfaces, such as indoor rooms or urban facades. To address this, researchers have explored alternative geometric primitives that impose stronger structural priors.\n\n**Planes and Quadrics:** Incorporating planar primitives is a natural extension for structured environments. Instead of covering a flat wall with hundreds of small Gaussians, a single planar primitive (or a small set) can represent the surface more compactly and accurately. This reduces the storage footprint and computational cost during rendering, which is vital for resource-constrained robotic platforms. Quadrics (e.g., ellipsoids, hyperboloids) offer a middle ground between the flexibility of Gaussians and the rigidity of planes. They can represent curved surfaces like pipes or vehicle bodies more efficiently than a collection of small Gaussians. By parameterizing the scene as a mix of Gaussians and quadrics, the mapping system can achieve higher fidelity with fewer primitives. This hybrid representation is particularly relevant for \"structured environments\" mentioned in the description, where the geometry is regular and predictable.\n\n**Optimized Splats:** Even within the paradigm of point-based splatting, there is a move toward optimizing the primitive shape itself. Recent work suggests that the standard 3D Gaussian might not always be the optimal primitive for all surfaces. For instance, \"2D Gaussian Splatting\" (2DGS) flattens the primitives into surfels (surface elements), which significantly improves geometric alignment and surface reconstruction. This is crucial for robotics applications that require accurate mesh extraction for collision avoidance or physics simulation. By aligning the primitive's orientation with the surface normal, 2DGS reduces the \"thick shell\" artifact inherent in 3D ellipsoids, leading to sharper boundaries and cleaner geometry. This alternative representation directly addresses the challenge of recovering high-fidelity geometry from discrete point clouds, a common bottleneck in 3DGS-based SLAM.\n\n### Application in Off-Road and Unstructured Domains\n\nThe combination of specialized sensors and alternative primitives is particularly transformative for off-road navigation. Off-road environments are characterized by unstructured terrain, vegetation, and varying lighting conditions that challenge standard visual SLAM. Here, the robustness of LiDAR or ToF sensors is indispensable. However, the visual appearance (texture, color) of the terrain is still vital for classifying drivable areas. 3DGS excels at capturing this appearance. By using LiDAR to guide the geometric placement of Gaussians (or alternative primitives like surfels for vegetation), the system can generate a map that is both geometrically sound for path planning and visually detailed for semantic understanding.\n\nFurthermore, in these domains, efficiency is paramount. The computational cost of optimizing thousands of Gaussians can be prohibitive for a robot operating in real-time. Alternative primitives like planes and quadrics offer a significant reduction in complexity. For example, a robot traversing a dirt road might represent the ground plane using a single large primitive, reserving detailed Gaussians only for obstacles and landmarks. This selective application of primitives based on the environment's structure leads to highly efficient maps.\n\n### Efficiency and Robustness in Structured Environments\n\nIn structured environments like warehouses or urban streets, the demand for precision is high. Specialized sensors like high-precision ToF cameras can capture fine details on industrial machinery or building facades. However, the raw data from these sensors often contains noise. The differentiable rendering pipeline of 3DGS provides a mechanism to filter this noise. By optimizing the Gaussian parameters to match the sensor data while enforcing multi-view consistency, the system effectively denoises the input.\n\nMoreover, the explicit nature of 3DGS allows for dynamic updates, which is essential for SLAM in dynamic environments. As the robot moves, new Gaussians can be added, and redundant ones removed. When coupled with specialized sensors that provide reliable depth, the densification strategy becomes more principled. Instead of splitting Gaussians based solely on gradient magnitude (as in the original ADC strategy), the system can use depth uncertainty from the sensor to decide where to densify. This leads to a more controlled and stable mapping process, preventing the explosion of Gaussian counts that plagues many naive 3DGS implementations.\n\n### Conclusion\n\nThe evolution of 3D Gaussian Splatting from a pure vision technique to a multi-modal mapping tool is driven by the integration of specialized sensors and alternative geometric primitives. By fusing the rich appearance data of cameras with the precise geometry of LiDAR and ToF, we can build maps that are robust to the challenges of real-world environments. Simultaneously, moving beyond the standard 3D ellipsoid to planes, quadrics, and surfels allows for more efficient and structurally consistent representations. These advancements are critical for deploying 3DGS in downstream robotics and SLAM applications, where efficiency, robustness, and geometric accuracy are non-negotiable requirements. The future of 3D mapping lies in this hybrid approach, leveraging the strengths of diverse sensors and representations to model the world with unprecedented fidelity and efficiency.\n\n### 8.5 Localization, Relocalization, and Map Matching\n\nLocalization, relocalization, and map matching are fundamental capabilities for autonomous systems, enabling them to determine their position within a pre-built map, recover from tracking loss, and align different map representations. The explicit, high-fidelity, and real-time rendering nature of 3D Gaussian Splatting (3DGS) makes it an exceptionally promising backbone for these tasks, moving beyond traditional dense mapping and feature-based methods. This subsection explores how 3DGS is being leveraged to create more accurate, efficient, and semantically rich systems for precise localization and map alignment, particularly in challenging GNSS-denied environments.\n\n### 8.5.1 Visual Relocalization with 3D Gaussian Splatting\n\nVisual relocalization aims to estimate the 6-DoF pose of a camera given only a single image and a pre-existing map. Traditional approaches often rely on sparse feature matching (e.g., SIFT, ORB) against a point cloud or database of keyframes, which can struggle in texture-poor or visually ambiguous environments. Dense methods, while more robust, are computationally expensive. 3DGS offers a compelling alternative by providing a dense, continuous, and photorealistic model of the scene that can be queried for rendering.\n\nThe core idea behind 3DGS-based relocalization is to treat pose estimation as a differentiable rendering problem. Given a query image, the system can optimize the camera pose by minimizing the photometric difference between the query image and the novel view rendered from the 3DGS map at the estimated pose. This approach, which we can refer to as 3DGS-ReLoc, leverages the entire scene's appearance information rather than just a sparse set of features, making it more robust to partial occlusions and lighting changes. The differentiable rasterization pipeline of 3DGS is key here, as it allows gradients to flow from the rendered pixel colors back to the camera pose parameters, enabling efficient gradient-based optimization. This method can achieve high-accuracy pose estimation, especially when initialized with a rough prior, by iteratively refining the pose to best align the rendered model with the observed reality. The explicit nature of Gaussians also allows for efficient culling and sorting, ensuring that the relocalization process remains fast even in large-scale scenes.\n\n### 8.5.2 Segment-Based Mapping and Semantic Understanding (SegMap)\n\nWhile photorealistic rendering is crucial, semantic understanding is often paramount for robotic navigation and interaction. The concept of \"SegMap\" from traditional robotics highlights the power of segmenting the map into meaningful objects or parts for robust localization and loop closure detection. 3DGS can be naturally integrated with this paradigm. Each 3D Gaussian can be associated with a semantic label, either through pre-trained 2D segmentation models (like SAM) or through joint optimization.\n\nThis creates a semantically-aware 3D map where localization is not just about matching geometry or appearance, but about matching object-level structures. For instance, a robot could localize itself by observing a specific arrangement of \"chair,\" \"table,\" and \"window\" Gaussians, which is far more robust than matching individual points that might be repetitive. This semantic grouping also drastically improves loop closure detection. Instead of comparing raw visual features, the system can compare the bag-of-words representation of the semantically segmented scene, filtering out dynamic objects and focusing on stable, structural elements. The explicit nature of 3DGS makes it straightforward to query which Gaussians are in the current field of view and what their semantic labels are, enabling real-time semantic localization and scene understanding. This is a significant step beyond purely geometric maps, providing a richer context for autonomous decision-making.\n\n### 8.5.3 Multi-Modal Fusion for Robust Localization in GNSS-Denied Environments\n\nIn many real-world applications, especially in indoor settings, urban canyons, or under dense foliage, GNSS signals are unavailable or unreliable. In these scenarios, systems must rely on Simultaneous Localization and Mapping (SLAM) using onboard sensors like cameras and LiDAR. 3DGS is emerging as a superior map representation for multi-modal fusion in these challenging environments.\n\nFirst, 3DGS can be fused with LiDAR data to create photorealistic and geometrically accurate maps. While LiDAR provides precise depth information, 3DGS excels at interpolating between sparse LiDAR points and modeling photometric properties. Methods like [28] demonstrate how to effectively combine these modalities. The LiDAR points can be used to initialize and constrain the positions of the 3D Gaussians, providing a strong geometric prior that accelerates convergence and improves the fidelity of the reconstructed surface. Conversely, the photometric supervision from camera images allows the 3DGS model to fill in details in between the LiDAR scans and model view-dependent effects, which LiDAR cannot capture. During localization, the system can use LiDAR scan matching for coarse pose estimation and then refine it using the dense visual information from the 3DGS model, creating a robust and redundant system.\n\nSecond, 3DGS-based SLAM systems, such as [28], are being developed to build the 3DGS map on-the-fly while simultaneously tracking the camera's pose. These systems typically use a traditional feature-based method for initial tracking and then use the dense photometric loss from 3DGS rendering to refine the pose and update the map. The explicit nature of the map allows for efficient updates; only the Gaussians visible in the current frame need to be optimized. This is a major advantage over implicit methods like NeRF-based SLAM, which often require slow volumetric rendering for each tracking step. By providing a real-time, dense, and high-quality map, 3DGS-based SLAM enables precise localization in complex, dynamic, and feature-poor environments where GNSS is not an option.\n\n### 8.5.4 Map Matching and Conflation\n\nBeyond single-robot localization, 3DGS also facilitates map matching and conflation\u2014the process of aligning and merging different map representations. This is critical for collaborative SLAM, where multiple robots build partial maps that must be fused, or for updating existing maps with new data. The explicit and discrete nature of 3D Gaussians makes them well-suited for this task. Matching can be performed by finding correspondences between Gaussian primitives from different maps based on their position, covariance (scale/rotation), and color. This is often more efficient and robust than matching continuous implicit fields. For example, a system could identify overlapping regions by matching clusters of Gaussians with similar geometric and appearance properties, then compute a rigid transformation to align them. This capability is essential for building large-scale, consistent maps over time and across multiple agents, forming the backbone of scalable and collaborative robotic systems.\n\nIn summary, the integration of 3D Gaussian Splatting into localization and mapping pipelines represents a significant advancement. By providing a map representation that is simultaneously explicit, high-fidelity, and efficient to render and update, 3DGS enables more robust visual relocalization, facilitates semantic understanding through segment-based mapping, and powers tightly-coupled multi-modal SLAM systems that can operate reliably in complex, GNSS-denied environments.\n\n## 9 Challenges, Limitations, and Future Outlook\n\n### 9.1 Fundamental Algorithmic and Representation Limitations\n\nThe rapid ascent of 3D Gaussian Splatting (3DGS) has established it as a formidable explicit alternative to implicit neural representations like Neural Radiance Fields (NeRF), primarily celebrated for its ability to render high-fidelity novel views in real-time. However, despite these advancements, the fundamental algorithmic structure of 3DGS imposes specific constraints that limit its versatility and robustness compared to continuous volumetric fields. This subsection analyzes the core structural limitations of the 3DGS paradigm, focusing on the lack of explicit topology handling, the inherent artifacts associated with discrete point-based representations, and the absence of intrinsic physical properties suitable for simulation.\n\n**Lack of Explicit Topology Handling and Morphological Changes**\n\nOne of the most significant limitations of 3DGS is its inability to naturally handle topological transformations. Unlike implicit representations, which model scenes as continuous volumetric functions, 3DGS relies on a collection of discrete anisotropic Gaussian primitives. While this allows for efficient rendering, it fundamentally restricts the representation's ability to model morphological changes, such as the fusion or separation of objects. In implicit fields, topological changes can emerge naturally as the underlying density function evolves; for example, a single connected surface can split into two distinct surfaces simply by adjusting the continuous field values. In contrast, 3DGS primitives are static entities that move, scale, and rotate, but they do not inherently merge or split to represent new topological structures.\n\nThis limitation is particularly evident in dynamic scene modeling. While dynamic 3DGS methods attempt to model motion through deformation fields or canonical spaces, they often struggle with scenarios where the topology of the scene changes over time. For instance, simulating a liquid pouring and merging with a pool or a solid object melting requires the representation to adapt its underlying structure. 3DGS, being a point-based representation, lacks the connectivity information found in meshes or the continuity of implicit fields. Consequently, attempting to model such events often results in disconnected points or \"ghost\" artifacts where the topology has changed but the discrete primitives fail to fill the gaps coherently. This contrasts with the capabilities of implicit neural rendering, which has been shown to handle complex view synthesis and geometry reconstruction without such rigid structural constraints, as highlighted in surveys like [1] and [76]. The discrete nature of 3DGS primitives makes it a \"point cloud\" evolution rather than a true volumetric field, limiting its expressiveness for applications requiring complex morphological evolution.\n\n**Discrete Representation Artifacts: Floating Primitives and Holes**\n\nThe reliance on a discrete set of 3D Gaussians introduces specific artifacts that are less prevalent in continuous implicit representations. Because the scene is approximated by a finite number of primitives, the density of these primitives is crucial. If the density is insufficient, the representation fails to cover the continuous surface, leading to visible holes or \"leaks\" in the rendered geometry. Conversely, if the density is too high or if the optimization process is imperfect, the representation suffers from floating artifacts\u2014clusters of Gaussians that do not correspond to any physical surface but are retained to minimize photometric error.\n\nThese floating artifacts are a direct consequence of the optimization strategy. 3DGS utilizes an adaptive density control mechanism to add or remove Gaussians based on gradient magnitudes. However, this process is heuristic and can be unstable. In regions with low texture or occluded areas, the optimizer may spawn numerous small, semi-transparent Gaussians to approximate the color, resulting in a \"cloud\" of noise rather than a solid surface. This is in stark contrast to implicit methods, which, while prone to \"floaters\" as noted in [92], generally produce smoother continuous fields. The explicit point nature of 3DGS makes these artifacts more geometrically distinct and harder to filter out without destroying valid geometry.\n\nFurthermore, the discrete nature makes it difficult to guarantee a watertight surface. Meshes provide explicit connectivity, and implicit fields (like SDFs) can be queried at any resolution to find the zero-crossing surface. 3DGS, however, represents surfaces as dense clusters of oriented discs. Without post-processing extraction methods like Gaussian Opacity Fields (GOF) or mesh extraction techniques, the raw Gaussian representation is not suitable for tasks requiring precise geometric boundaries, such as physics simulation or CAD modeling. The lack of connectivity means that simple queries like ray-tracing for collision detection are non-trivial compared to mesh-based approaches.\n\n**Absence of Inherent Physical Properties for Simulation**\n\nA critical limitation of 3DGS is its lack of inherent physical properties. The attributes of a 3D Gaussian\u2014position, rotation, scale, opacity, and color\u2014are optimized solely for visual fidelity during rendering. They do not correspond to physical quantities such as mass, density, elasticity, or friction coefficients. Consequently, 3DGS cannot be directly integrated into physics engines or simulation pipelines without significant modification or external mapping.\n\nThis limitation is a major barrier for applications in robotics, virtual reality, and generative dynamics. For a scene to be physically plausible, objects must interact according to the laws of physics. In 3DGS, moving a Gaussian does not automatically trigger collision responses or deformations because the representation lacks a underlying physical model. While recent efforts like [93] attempt to bridge this gap by integrating Material Point Methods (MPM) and assigning physical material properties to Gaussians, these are external additions rather than intrinsic to the 3DGS formulation. The original 3DGS framework is purely a rendering primitive; it is a \"visual\" representation, not a \"physical\" one.\n\nIn contrast, implicit representations have been more successfully coupled with physical properties. For instance, some works have integrated signed distance fields (SDFs) with NeRFs to enable collision detection and fluid simulation. The continuous nature of implicit fields allows for the computation of physical quantities like gradients (normals) and divergence, which are essential for fluid dynamics. 3DGS, being a collection of discrete splats, requires complex interpolation and regularization to estimate continuous fields (as done in [65] and [47]) to even approximate physical properties. This adds an extra layer of complexity and potential error.\n\n**Comparison with Implicit and Mesh-Based Counterparts**\n\nTo contextualize these limitations, it is useful to compare 3DGS with the two other dominant paradigms: implicit neural fields (NeRF) and explicit meshes.\n\n*   **vs. NeRF ():** Ne NeRF represents the scene as a continuous function $f(x, d) \\rightarrow (c, \\sigma)$. This continuity ensures that the representation is resolution-independent and naturally handles smooth transitions. As discussed in [3], the implicit nature allows for infinite scalability in theory (though computationally expensive). The primary advantage of NeRF is its ability to model complex lighting and geometry without discrete artifacts, provided the network capacity is sufficient. However, NeRF suffers from slow rendering speeds due to volumetric ray marching, which 3GS solves. The trade-off is that 3DGS sacrifices the continuous, physically amenable nature of NeRF for speed.\n\n*   **vs. Meshes (Explicit Connectivity):** Meshes are the standard in computer graphics due to their explicit connectivity and efficiency in rendering (via rasterization). They are inherently physical\u2014vertices define a solid volume or surface that can be deformed. However, reconstructing high-quality meshes from images is difficult. 3DGS sits between meshes and NeRFs: it uses rasterization for speed (like meshes) but lacks connectivity (unlike meshes). It offers easier reconstruction than meshes but fails to provide the structural integrity required for many graphics pipelines. Methods like [11] highlight the effort required to bridge this gap, distilling NeRFs into meshes. 3DGS requires similar distillation to become useful for mesh-based workflows.\n\n**Conclusion on Limitations**\n\nIn summary, the fundamental algorithmic and representation limitations of 3DGS stem from its discrete, point-based nature. It lacks the topological flexibility of implicit fields, making it ill-suited for morphological changes. It is prone to specific geometric artifacts like floating primitives and holes that require specialized densification strategies to mitigate. Most importantly, it lacks the physical grounding necessary for simulation, requiring external frameworks to imbue it with physical properties. While 3DGS represents a massive leap in rendering efficiency, these core limitations define the boundaries of its current applicability and highlight the necessity for hybrid approaches or future innovations that can merge the speed of splatting with the continuity and physical robustness of implicit or mesh-based representations.\n\n### 9.2 Scalability, Storage, and Computational Bottlenecks\n\nThe rapid adoption of 3D Gaussian Splatting (3DGS) has revolutionized novel view synthesis by enabling real-time rendering speeds and high-fidelity reconstructions [15]. However, this explicit representation relies on a massive number of discrete primitives\u2014often numbering in the millions\u2014to capture complex scene details. As the scope of reconstruction expands from small objects to large-scale urban environments and high-resolution captures, the scalability of 3DGS becomes a critical bottleneck. The fundamental challenge lies in the linear relationship between scene complexity and the number of Gaussian primitives required. Consequently, the memory footprint for storing attributes (position, rotation, scale, opacity, and color) and the computational cost for rasterizing these primitives grow prohibitively, hindering deployment on resource-constrained devices and limiting the scope of reconstructable scenes.\n\n**Memory Footprint and Storage Costs**\n\nThe most immediate barrier to scalability is the storage requirement. Unlike implicit representations like Neural Radiance Fields (NeRFs), which compress scene information into neural network weights, 3DGS stores explicit parameters for every primitive. This results in file sizes ranging from hundreds of megabytes to several gigabytes for a single scene, which is impractical for streaming, archiving, or mobile applications. The community has responded with various compression strategies, targeting different aspects of the representation.\n\nOne prominent direction is reducing the number of primitives through aggressive pruning and quantization. The work on \"Reducing the Memory Footprint of 3D Gaussian Splatting\" highlights that a significant portion of Gaussians contributes little to the final render, particularly those in empty space or redundant regions. By implementing a resolution-aware primitive pruning approach, the authors successfully halve the primitive count. Furthermore, they introduce adaptive spherical harmonic (SH) coefficient selection and codebook-based quantization, achieving a 27x reduction in disk storage. Similarly, \"Compact 3D Gaussian Representation for Radiance Field\" and \"Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields\" propose learnable masks to sparsify the Gaussian set and replace view-dependent color representations with grid-based neural fields or vector quantization. These methods demonstrate that significant compression is possible without substantial quality loss, though they often require careful tuning to balance compression ratios against visual fidelity.\n\nHowever, simply reducing the number of Gaussians or quantizing attributes can lead to a loss of high-frequency details. To address this, \"F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting\" introduces tensor factorization techniques. Instead of storing attributes for every Gaussian individually, F-3DGS approximates dense clusters of Gaussians using factorized components, drastically reducing storage while preserving the ability to reconstruct fine details. This approach leverages the inherent redundancy in the spatial distribution of Gaussians, encoding large numbers of primitives through a combination of axis-specific information.\n\nBeyond static compression, dynamic scenes pose an even greater storage challenge. \"An Efficient 3D Gaussian Representation for Monocular Multi-view Dynamic Scenes\" observes that storing per-frame Gaussian parameters leads to linear memory growth with sequence length. Their solution models positions and rotations as time-dependent functions while keeping other attributes static, ensuring memory usage remains constant regardless of sequence duration. This is a crucial step towards scalable dynamic reconstruction, preventing the storage overhead from becoming unmanageable for long videos.\n\nDespite these advances, compression often introduces computational overhead during decoding or rendering. \"Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis\" tackles this by using sensitivity-aware vector clustering and quantization-aware training. The resulting compressed representation not only achieves up to 31x compression but also facilitates faster rendering on lightweight GPUs due to reduced memory bandwidth requirements. This highlights a key trade-off: effective compression must be co-designed with rendering efficiency to ensure that the reduced storage does not come at the cost of slower inference.\n\n**Rendering Bottlenecks in Large-Scale Scenes**\n\nWhile storage is a primary concern for distribution, rendering performance is the bottleneck for interactive applications. The standard 3DGS rasterization pipeline projects Gaussians to screen space, sorts them by depth, and performs alpha blending. In large-scale scenes, particularly urban environments, the number of Gaussians within the viewing frustum can be enormous, leading to high latency and inconsistent frame rates.\n\n\"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians\" identifies that the heuristic density control in vanilla 3DGS struggles to capture details at varying scales, causing performance drops in zoom-out views. To mitigate this, Octree-GS introduces a Level-of-Detail (LoD) structure. By organizing Gaussians into a multi-resolution octree hierarchy, the system can dynamically select appropriate LoD based on the camera distance. This ensures that distant regions are rendered with fewer, larger Gaussians, maintaining consistent frame rates without sacrificing visual quality. This approach mirrors traditional computer graphics techniques, adapting them to the unique properties of Gaussian primitives.\n\nFor truly massive scenes, a single hierarchical representation may not suffice. \"A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" proposes a divide-and-conquer strategy. The scene is divided into independent chunks during training, which are then consolidated into a hierarchical structure. This allows for the reconstruction of scenes spanning several kilometers and involving tens of thousands of images. The hierarchy supports efficient LOD selection and smooth transitions, enabling real-time rendering of vast environments that were previously intractable.\n\nHowever, the rendering pipeline itself remains a bottleneck. The original 3DGS relies on a global sort of Gaussians, which is efficient but can introduce popping artifacts and view inconsistencies. \"StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering\" introduces a hierarchical rasterization approach that resorts and culls splats with minimal overhead. By ensuring accurate per-pixel depth computation, it eliminates artifacts and allows for a reduction in Gaussian count by nearly half while maintaining quality. This effectively doubles rendering performance and halves memory requirements, demonstrating that optimizing the rendering pipeline can yield significant gains in scalability.\n\n\"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting\" further accelerates rendering by pre-filtering Gaussians. By clustering Gaussians offline and projecting these clusters at runtime, the method excludes up to 63% of Gaussians before the projection step, reducing rendering computation by 38.3% without quality loss. This is particularly effective for large scenes where many Gaussians are occluded or outside the field of view.\n\n\"AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius\" addresses load imbalance and serial culling overhead in the rendering pipeline. By moving culling to an earlier parallel stage and using pixels adaptive using using radius radius radius radius, achieves achieves achieves achieves achieves achieves achieves achieves....... achieves achieve achieves...... achieves................................................................ to........................ ..... This is a crucial step towards scalable dynamic reconstruction, preventing the storage overhead from becoming unmanageable for long videos.\n\nDespite these advances, compression often introduces computational overhead during decoding or rendering. \"Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis\" tackles this by using sensitivity-aware vector clustering and quantization-aware training. The resulting compressed representation not only achieves up to 31x compression but also facilitates faster rendering on lightweight GPUs due to reduced memory bandwidth requirements. This highlights a key trade-off: effective compression must be co-designed with rendering efficiency to ensure that the reduced storage does not come at the cost of slower inference.\n\n**Rendering Bottlenecks in Large-Scale Scenes**\n\nWhile storage is a primary concern for distribution, rendering performance is the bottleneck for interactive applications. The standard 3DGS rasterization pipeline projects Gaussians to screen space, sorts them by depth, and performs alpha blending. In large-scale scenes, particularly urban environments, the number of Gaussians within the viewing frustum can be enormous, leading to high latency and inconsistent frame rates.\n\n\"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians\" identifies that the heuristic density control in vanilla 3DGS struggles to capture details at varying scales, causing performance drops in zoom-out views. To mitigate this, Octree-GS introduces a Level-of-Detail (LoD) structure. By organizing Gaussians into a multi-resolution octree hierarchy, the system can dynamically select appropriate LoD based on the camera distance. This ensures that distant regions are rendered with fewer, larger Gaussians, maintaining consistent frame rates without sacrificing visual quality. This approach mirrors traditional computer graphics techniques, adapting them to the unique properties of Gaussian primitives.\n\nFor truly massive scenes, a single hierarchical representation may not suffice. \"A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" proposes a divide-and-conquer strategy. The scene is divided into independent chunks during training, which are then consolidated into a hierarchical structure. This allows for the reconstruction of scenes spanning several kilometers and involving tens of thousands of images. The hierarchy supports efficient LOD selection and smooth transitions, enabling real-time rendering of vast environments that were previously intractable.\n\nHowever, the rendering pipeline itself remains a bottleneck. The original 3DGS relies on a global sort of Gaussians, which is efficient but can introduce popping artifacts and view inconsistencies. \"StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering\" introduces a hierarchical rasterization approach that resorts and culls splats with minimal overhead. By ensuring accurate per-pixel depth computation, it eliminates artifacts and allows for a reduction in Gaussian count by nearly half while maintaining quality. This effectively doubles rendering performance and halves memory requirements, demonstrating that optimizing the rendering pipeline can yield significant gains in scalability.\n\n\"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting\" further accelerates rendering by pre-filtering Gaussians. By clustering Gaussians offline and projecting these clusters at runtime, the method excludes up to 63% of Gaussians before the projection step, reducing rendering computation by 38.3% without quality loss. This is particularly effective for large scenes where many Gaussians are occluded or outside the field of view.\n\n\"AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius\" addresses load imbalance and serial culling overhead in the rendering pipeline. By moving culling to an earlier parallel stage and using adaptive radius to narrow the rendering pixel range for each Gaussian, it achieves a 310% speedup. This work highlights that the rasterization pipeline needs to be re-architected to fully exploit modern GPU parallelism, especially as scene complexity increases.\n\n**Distributed Scalability and Federated Learning**\n\nAs scenes grow in size and complexity, single-machine training and rendering become insufficient. Distributed approaches are necessary to scale 3DGS to city-level reconstructions. While the provided papers do not explicitly mention \"Fed3DGS,\" the concept of distributed scalability is addressed through hierarchical and partitioned methods. \"A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" effectively demonstrates a distributed training paradigm by processing chunks independently. This \"divide-and-conquer\" strategy is a precursor to fully federated systems where different nodes can contribute to a global scene representation without sharing raw data.\n\nIn the context of distributed rendering, \"FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering\" emphasizes the need for hardware-aware scalability. By allowing a scene to be rendered at varying levels of detail, FLoD enables deployment across a spectrum of devices, from high-end GPUs to mobile hardware. This flexibility is essential for distributed systems where rendering nodes may have heterogeneous capabilities. The ability to transmit or render only the necessary LoD ensures that bandwidth and computational resources are used efficiently.\n\n**Future Directions**\n\nTo overcome the scalability and storage bottlenecks, future research must focus on several key areas. First, the integration of implicit representations with explicit Gaussians offers a promising path. \"Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane Representation\" combines explicit point clouds with implicit feature embeddings, achieving high-quality rendering with a compact representation. This hybrid approach could be extended to distributed settings, where implicit fields serve as a global prior and Gaussians provide local detail.\n\nSecond, the rendering pipeline needs to move beyond simple rasterization. \"3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes\" explores using GPU ray tracing hardware to render Gaussians. This could unlock advanced effects like shadows and reflections in large scenes and handle highly distorted cameras common in robotics, potentially offering better scalability for complex lighting scenarios.\n\nThird, the optimization process itself must be scaled. Current methods rely on global optimization, which is computationally intensive. Techniques like \"GaussianPro: 3D Gaussian Splatting with Progressive Propagation\" use MVS-inspired progressive propagation to guide densification, potentially reducing optimization time and improving initialization for large, textureless areas. This could be combined with distributed optimization schemes to handle massive datasets.\n\nFinally, addressing the rendering bottleneck for dynamic scenes is critical. \"SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming with Arbitrary Length\" introduces a streaming framework that uses sliding windows and Markov Chain Monte Carlo to adapt to various 3D scenes across frames. This reduces transmission costs by 83.6% and scales to long video sequences, pointing towards a future where dynamic 3DGS can be streamed in real-time over limited bandwidth.\n\nIn conclusion, while 3DGS has achieved remarkable success, its scalability is hindered by massive storage requirements and rendering bottlenecks in large-scale environments. The community has proposed various solutions, including compression, LoD management, hierarchical representations, and rendering pipeline optimizations. However, achieving true scalability\u2014enabling city-scale, real-time, interactive rendering on diverse hardware\u2014will require a holistic approach that combines efficient representations, distributed processing, and hardware-accelerated rendering pipelines. The convergence of these techniques will be essential for unlocking the full potential of 3DGS in applications like autonomous driving, virtual reality, and large-scale digital twins.\n\n### 9.3 Integration with Physical Simulation and Dynamics\n\nThe rapid advancement of 3D Gaussian Splatting (3DGS) has revolutionized dynamic scene modeling, enabling photorealistic reconstruction and rendering of non-rigid deformations from multi-view videos. However, a fundamental limitation persists: current dynamic 3DGS methods primarily rely on data-driven observation fitting, lacking an intrinsic understanding of physical laws. While these methods can accurately track and interpolate visible motions present in the training data, they struggle to generate physically plausible dynamics in unseen scenarios, handle complex material interactions, or respond realistically to external forces. The representation is essentially a \"visual hull\" optimized for pixel consistency rather than a \"physical hull\" governed by mass, inertia, and elasticity. This disconnect between visual fidelity and physical validity restricts the application of 3DGS in domains requiring simulation, such as virtual object insertion, robotics, and generative physics, and stands in contrast to the geometric limitations discussed previously, which focus on static surface accuracy.\n\nTo bridge this gap, the community has begun exploring the integration of 3DGS with physics simulation engines. The core challenge lies in the mismatch between the discrete, particle-like nature of Gaussians and the continuous fields typically used in computational fluid dynamics (CFD) or finite element methods (FEM). Traditional simulators operate on volumetric grids or meshes, whereas 3DGS represents scenes as unstructured point clouds. Consequently, naive coupling results in unstable simulations or \"floaters\" that violate conservation laws. Addressing this requires a bidirectional translation: mapping physical state variables (velocity, stress, density) to Gaussian attributes (position, scale, rotation, opacity) and vice versa, while ensuring the rendering remains differentiable for optimization.\n\nA pioneering step in this direction is the work presented in **[94]**. This method introduces a novel framework that embeds 3D Gaussians into a continuum mechanics framework. Instead of treating Gaussians as static visual sprites, PhysGaussian endows them with physical properties such as density, elasticity, and viscosity. The authors utilize the Material Point Method (MPM) to handle the complex physics of large deformations and collisions. In this hybrid pipeline, Gaussians serve dual roles: they act as \"material points\" carrying physical state variables for the MPM simulator, and simultaneously as rendering primitives for the 3DGS rasterizer. A critical innovation is the \"Gaussian-to-MPM\" transfer scheme, where the attributes of Gaussians (position, velocity, deformation gradient) are mapped to the background Eulerian grid for physics calculation, and the resulting grid updates are mapped back to the Gaussians. This allows for the generation of realistic generative dynamics, such as cloth draping, fluid splashing, or solid fracturing, which are visually consistent with the material properties defined in the simulation. By enforcing physical constraints, PhysGaussian mitigates the common artifacts of dynamic 3DGS, such as unnatural stretching or penetration, ensuring that the rendered motion obeys Newtonian mechanics.\n\nHowever, handling complex interactions, particularly in granular media or fluid-structure interactions, requires more than just point-based MPM. The discrete nature of Gaussians can lead to holes or lack of cohesion in the representation, which is problematic for simulating continuous materials like water or soft gels. To address the representation of complex, non-smooth geometries and interactions, **[95]** proposes a differentiable simulation pipeline that leverages the explicit nature of 3DGS. GASP focuses on the \"analysis-by-synthesis\" loop, where physical parameters are optimized to match observed dynamic sequences. Unlike PhysGaussian which focuses on generative dynamics, GASP emphasizes inverse physics: estimating material properties from video. It demonstrates how the 3DGS representation can be used to simulate and render interactions involving distinct geometric primitives, handling collisions and friction with high fidelity. GASP highlights the utility of the explicit Gaussian representation for rapid prototyping of physical scenarios, as the lack of connectivity in Gaussians allows for easy splitting and merging\u2014operations that are computationally expensive for traditional mesh-based simulators. This flexibility is crucial for modeling brittle fracture or granular flow, where topology changes frequently.\n\nDespite these advances, integrating physical simulation with 3DGS introduces significant algorithmic and computational challenges. One major issue is the conservation of mass and volume. In standard 3DGS, the \"scale\" attribute of a Gaussian is purely a visual parameter optimized to minimize photometric error. It does not correspond to a physical volume. When coupled with a physics engine, this leads to inconsistencies; a Gaussian might shrink visually to fit a shadow, violating the conservation of mass required by physics. Both **[93]** and **[96]** address this by introducing regularization terms that tie the opacity and volume of Gaussians to physical density. However, this remains an approximation. True physical compliance requires a rigorous mapping between the Gaussian ellipsoid and the mass distribution of the object it represents. Future work must establish a rigorous mathematical framework for \"Physical Gaussian Primitives,\" where the covariance matrix is constrained by the moment of inertia tensor of the object.\n\nFurthermore, the computational overhead of running a coupled simulation-rendering loop is substantial. Physics simulations, particularly MPM, are computationally intensive and typically run at discrete time steps (e.g., 30-60 Hz). 3DGS rendering is real-time (>= 144 Hz). Synchronizing these two rates without introducing temporal aliasing or latency is a non-trivial engineering feat. The literature suggests using \"sub-stepping\" for physics while interpolating Gaussian positions for rendering, but this can lead to visual artifacts if the deformation fields are highly non-linear. The explicit nature of Gaussians, while efficient for rendering, poses a bottleneck for physics queries. For example, calculating collision detection between millions of unstructured Gaussians and a rigid body is expensive. Spatial acceleration structures, such as the hierarchical grids used in **[60]**, might be adapted for physics queries, but the dynamic update of these structures during simulation remains an open problem.\n\nAnother critical limitation is the lack of internal structure in current Gaussian representations. In **[93]**, Gaussians are treated as volumeless points carrying mass. However, for realistic soft-body dynamics, internal stress and strain fields are essential. Current methods approximate this by smoothing attributes over neighboring Gaussians, but this is ad-hoc. A promising research avenue is the integration of implicit neural fields (such as Signed Distance Fields or Neural ODEs) with 3DGS to represent the internal material state. For instance, a neural network could map a Gaussian's position to its stress tensor, while 3DGS handles the surface rendering. This hybrid approach could leverage the efficiency of rasterization for visuals and the continuity of implicit fields for physics.\n\nMoreover, the \"generative\" aspect of physical simulation\u2014creating new dynamic content from scratch or text prompts\u2014requires a deep understanding of material properties. While **[93]** demonstrates generative capabilities, they are largely driven by the MPM simulator's input parameters. Bridging this to high-level semantic understanding (e.g., \"a rubber ball bouncing on a concrete floor\") requires semantic segmentation and material classification integrated into the 3DGS representation. Recent works on semantic-aware 3DGS (e.g., **[97]**) provide a foundation, but linking semantic labels to physical material databases (friction coefficients, Young's modulus) is still in its infancy.\n\nFinally, the robustness of these coupled systems in unbounded, real-world scenes is questionable. Most physical simulation methods assume clean, watertight geometries. However, 3DGS reconstructions often contain noise, holes, and unstructured \"floaters.\" Feeding such noisy geometry into a physics engine can cause numerical instability and explosion. Pre-processing steps to clean the Gaussian cloud or robust physics solvers that can handle \"fuzzy\" geometries are necessary. The concept of \"Gaussian Opacity Fields\" (GOF) proposed in **[21]** could be extended to generate a watertight collision mesh from the dynamic Gaussian field, providing a clean interface for the physics engine while preserving the visual fidelity of the raw Gaussians.\n\nIn conclusion, while 3DGS has achieved remarkable success in visual reconstruction, its integration with physical simulation represents the frontier of \"Digital Twins\" and \"Generative Physics.\" Methods like **[93]** and **[96]** have laid the groundwork by demonstrating that Gaussians can be endowed with mass and elasticity, enabling realistic generative dynamics. However, significant hurdles remain in ensuring mass conservation, handling complex topology changes, and managing computational costs. The future of this field lies in developing \"Physics-Informed Gaussian Primitives\" that are not just visually accurate but physically indistinguishable from reality, paving the way for applications in physics-based rendering, virtual reality, and automated content generation.\n\n### 9.4 Geometric Fidelity and Surface Reconstruction\n\nThe remarkable success of 3D Gaussian Splatting (3DGS) in novel view synthesis is largely attributed to its explicit, point-based representation, which facilitates high-fidelity, real-time rendering. However, this very strength exposes a significant weakness when the objective shifts from photometric fidelity to geometric accuracy. Unlike implicit neural fields such as Neural Radiance Fields (NeRFs) or signed distance field (SDF) representations, 3DGS does not inherently model a continuous surface. Instead, it represents the scene as a collection of discrete, anisotropic 3D ellipsoids. This fundamental difference leads to a critical challenge: the difficulty of recovering accurate, continuous geometry, particularly in regions with low texture, high specularity, or transparency. The optimization process, driven primarily by a photometric loss, often prioritizes rendering the correct color at the expense of geometric plausibility, resulting in a point cloud that may be visually convincing but is geometrically noisy, incomplete, or \"fluffy.\" This subsection delves into the core limitations of 3DGS in geometric fidelity, explores recent advancements aimed at mitigating these issues, and outlines future directions for bridging the gap between rendering quality and surface reconstruction accuracy.\n\nThe primary source of geometric deterioration in standard 3DGS lies in its optimization paradigm. The adaptive density control (ADC) mechanism, a cornerstone of the original method, is designed to add or remove Gaussians based on gradients derived from the photometric loss. While effective for capturing visual details, this strategy is agnostic to the underlying surface structure. In non-textured areas (e.g., white walls, smooth plastics), gradients are small, preventing the densification process from adding sufficient Gaussians to form a dense, continuous surface. Conversely, in areas with complex view-dependent effects, the optimizer may spawn numerous small, overlapping Gaussians that create a \"cloud\" of points rather than a well-defined surface, leading to geometric noise. This phenomenon highlights the inherent ambiguity between geometry and appearance, a problem well-known in the NeRF community as the \"shape-radiance ambiguity\" [87]. While NeRFs suffer from this, the explicit nature of 3DGS makes the resulting geometric artifacts more apparent and problematic for downstream tasks like physics simulation or mesh extraction.\n\nTo address these challenges, a new line of research has emerged, focusing on incorporating geometry-aware priors and regularization into the 3DGS framework. A prominent direction involves integrating implicit surface representations, such as Signed Distance Fields (SDFs), to guide the placement and optimization of Gaussians. The core idea is to enforce that the Gaussians lie on or near a continuous surface, thereby reducing floating artifacts and improving geometric completeness. For instance, methods like \"3DGSR\" (3D Gaussian Splatting with Surface Regularization) explicitly introduce an SDF-based regularization term into the loss function. This term penalizes Gaussians that deviate significantly from the zero-level set of the SDF, effectively \"pulling\" them onto the surface. By coupling the explicit Gaussian representation with an implicit surface model, these approaches leverage the best of both worlds: the rendering speed of 3DGS and the geometric regularity of SDFs. This hybrid strategy helps to resolve the ambiguity in textureless regions, as the SDF prior provides a strong geometric signal even when photometric gradients are weak.\n\nAnother critical approach is the development of novel regularization techniques that directly encourage the Gaussians to form a coherent surface. The \"Gaussian Splatting with Localized Points Management\" method is a key proponent of this philosophy. It introduces geometry-aware optimizations that focus on the spatial distribution and orientation of the Gaussians. For example, it may encourage Gaussians to align their principal axes with the local surface normal, effectively modeling them as oriented \"surfels\" (surface elements) rather than generic 3D blobs. This not only improves the visual quality of the reconstructed geometry but also yields a point cloud that is more amenable to surface reconstruction algorithms like Poisson surface reconstruction. Furthermore, \"Gaussian Splatting with Localized Points Management\" likely incorporates penalties for \"fluffiness\"\u2014the tendency of Gaussians to spread out in low-texture areas\u2014by encouraging a more compact distribution of points on the underlying surface. Such regularization is vital for producing watertight meshes, which are a prerequisite for many applications in computer-aided design, virtual reality, and physical simulation.\n\nBeyond regularization, some works propose fundamentally altering the Gaussian primitive itself to be better suited for surface modeling. The introduction of \"2D Gaussian Splatting\" (2DGS) represents a significant leap in this direction. Instead of using volumetric 3D ellipsoids, 2DGS employs flat, disk-like primitives. This change in primitive topology has profound implications for geometric fidelity. A flat disk is a much more natural representation for a surface element than a 3D blob. By constraining the Gaussians to be two-dimensional, the model is inherently biased towards forming a surface-like structure. The optimization process can then more easily determine the orientation and position of these disks to cover the scene's surface, leading to significantly cleaner geometry with fewer floating points and better-defined boundaries. This approach directly tackles the geometric ambiguity by changing the building blocks of the representation to be more geometrically meaningful.\n\nHowever, even with these advancements, significant challenges remain. The reliance on auxiliary implicit models like SDFs introduces additional computational overhead and complexity. The tight coupling between the SDF and the Gaussians can be delicate, and instability in one can negatively impact the other. Moreover, the quality of the reconstructed geometry is still highly dependent on the quality and completeness of the input views. In cases of severe occlusion or sparse viewpoints, the geometric priors may not be sufficient to recover a plausible surface. The discrete nature of the representation also means that recovering a truly continuous, watertight mesh often requires a post-processing step, which may not be perfectly integrated with the Gaussian optimization.\n\nLooking forward, the path to achieving robust geometric fidelity in 3DGS lies in several key areas. First, the development of more tightly integrated hybrid models is crucial. Instead of treating the SDF and Gaussians as separate entities with a loose coupling, future work could explore unified representations where the Gaussian parameters are direct functions of a continuous neural field, ensuring inherent geometric consistency. Second, leveraging stronger geometric priors from foundation models, such as depth estimation or surface normal prediction models, could provide a more reliable signal for guiding the optimization, especially in challenging regions. Third, moving beyond simple regularization towards learning the physical properties of the surfaces (e.g., material, roughness) and using them to inform the geometric representation could lead to more realistic and physically plausible results. The ultimate goal is to create a 3DGS variant that not only excels at photorealistic rendering but also produces geometric reconstructions that are accurate enough to be used directly in demanding downstream applications, closing the gap between the virtual and the physical.\n\n### 9.5 Convergence with Foundation Models and Generative AI\n\nThe rapid evolution of 3D Gaussian Splatting (3DGS) has primarily focused on enhancing rendering fidelity, geometric accuracy, and computational efficiency. However, a pivotal frontier lies in the convergence of 3DGS with large-scale foundation models and generative AI. This synergy promises to transcend the limitations of current optimization-heavy pipelines, enabling semantic understanding, zero-shot generalization, and automated content creation. As 3DGS matures from a reconstruction technique to a fundamental building block for the 3D digital ecosystem, its integration with models like Geoscience Foundation Models (GFMs) and text-to-3D diffusion models becomes imperative.\n\n### The Role of Generative Priors in 3D Synthesis\n\nThe intersection of 3DGS and generative AI is most visible in the domain of text-to-3D and image-to-3D generation. Traditional 3DGS optimization requires multi-view images, which limits its applicability in scenarios where input data is sparse or non-existent. Generative models, particularly diffusion models, offer a powerful prior to bridge this gap. The dominant paradigm involves using Score Distillation Sampling (SDS) to optimize a 3DGS representation using 2D priors from text or single images. Early works adapted techniques from DreamFusion to the Gaussian domain, but they often suffered from the \"Janus problem\" and lack of geometric consistency.\n\nTo address these issues, recent research has focused on refining the distillation process. For instance, the evolution from standard SDS to Variational Score Distillation (VSD) has significantly improved diversity and quality, mitigating the over-saturation and over-smoothing artifacts common in earlier methods. These advancements are crucial for generating high-fidelity 3D assets that can be rendered in real-time using 3DGS. Furthermore, the explicit nature of Gaussians allows for efficient manipulation and composition, making them an ideal intermediate representation for generative pipelines. Unlike implicit fields like NeRF, which require expensive volume rendering for every step of generation, 3DGS enables rapid feedback loops, essential for interactive generative systems.\n\nHowever, a major challenge remains the 3D inconsistency of 2D priors. To overcome this, researchers have introduced multi-view diffusion models that generate consistent geometry across different angles. These models provide a stronger signal for the optimization process, reducing geometric artifacts. Additionally, depth-conditioned guidance has been employed to inject geometric awareness into the generation pipeline. These methods demonstrate that by tightly coupling generative priors with the specific rendering characteristics of 3DGS, we can achieve high-quality, feed-forward or hybrid generation pipelines that balance speed and fidelity.\n\n### Semantic Understanding and Geoscience Foundation Models\n\nBeyond generic object generation, the convergence with foundation models opens avenues for specialized domains, particularly geoscience. The scale and complexity of geospatial data\u2014ranging from urban landscapes to natural terrains\u2014pose significant challenges for traditional 3D reconstruction. Geoscience Foundation Models (GFMs), trained on vast amounts of satellite and aerial imagery, possess a deep understanding of semantic structures and physical properties of the Earth's surface. Integrating these models with 3DGS can automate the creation of semantically rich, large-scale 3D maps.\n\nFor example, GFMs can provide semantic segmentation masks that guide the optimization of 3DGS, ensuring that Gaussians align with specific land cover types (e.g., vegetation, buildings, water bodies). This semantic regularization prevents the generation of floating artifacts and improves the structural integrity of the reconstructed scene. Furthermore, GFMs can offer zero-shot generalization capabilities, allowing 3DGS models to reconstruct scenes in environments they have not explicitly been trained on, by leveraging prior knowledge encoded in the foundation model. This is particularly relevant for large-scale urban mapping and autonomous driving applications, where robustness to varying conditions is paramount.\n\nThe integration of GFMs also facilitates the extraction of meaningful geometric features. Instead of relying solely on photometric consistency, the 3DGS optimization can be regularized by predicted depth maps or surface normals from GFMs. This leads to more accurate surface reconstruction, addressing one of the persistent weaknesses of standard 3DGS. As explored in the context of inverse rendering, accurate geometry is a prerequisite for high-quality relighting and material estimation. Therefore, the semantic and geometric priors from GFMs act as a crucial catalyst for advancing 3DGS beyond pure view synthesis.\n\n### Towards Automated Content Creation and Editing\n\nThe ultimate goal of combining 3DGS with foundation models is to enable fully automated content creation and intuitive editing workflows. The explicit, discrete nature of Gaussians makes them highly amenable to manipulation guided by natural language or high-level commands. For instance, text-guided editing frameworks can leverage large language models (LLMs) to parse user intent and map it to specific operations on the Gaussian set, such as localized color changes, object insertion, or structural modifications.\n\nRecent work in 3D editing has shown that by associating Gaussians with semantic IDs derived from segmentation models, we can perform complex, multi-object scene compositions. When coupled with generative priors, this allows for the inpainting of missing regions or the removal of unwanted objects while maintaining geometric and photometric consistency. The speed of 3DGS rendering is a critical enabler here, providing real-time feedback that makes these interactive editing systems practical.\n\nFurthermore, the convergence with foundation models paves the way for \"inverse graphics\" pipelines, where a scene described in natural language is automatically decomposed into 3DGS primitives with physically based materials and lighting. This requires a deep integration of generative synthesis (creating the geometry and appearance) and inverse rendering (estimating material and lighting properties). As foundation models become more capable of understanding physical laws and material properties, they can guide the 3DGS optimization to produce assets that are not just visually plausible but also physically valid.\n\n### Challenges and Future Research Avenues\n\nDespite the promising outlook, significant challenges remain. First, the computational cost of training and serving large foundation models is substantial. Efficient distillation techniques are needed to transfer the knowledge from these massive models into lightweight 3DGS representations that can be deployed on edge devices. Second, the alignment between the latent spaces of diffusion models and the explicit 3DGS representation is non-trivial. Developing robust loss functions and supervision strategies that bridge this gap is an active area of research.\n\nThird, the issue of scalability becomes even more pronounced when integrating dense semantic information from GFMs. Managing millions of Gaussians, each potentially carrying semantic and material attributes, requires advanced compression and hierarchical representation strategies. The development of \"NeCGS\" style neural compression for 3D geometry sets suggests a path forward, where the explicit Gaussian primitives can be further compressed using learned codecs.\n\nFinally, the evaluation of these integrated systems requires new metrics that go beyond rendering quality. We need to assess semantic consistency, physical plausibility, and the ability to follow complex instructions. As we move towards the \"Metaverse\" and immersive digital environments, the ability to generate and edit 3D content at scale, guided by high-level semantic understanding, will be a defining capability. The convergence of 3D Gaussian Splatting with foundation models and generative AI is not just an incremental improvement; it represents a paradigm shift towards intelligent, automated, and accessible 3D content creation.\n\n### 9.6 Domain-Specific Applications and Future Research Avenues\n\nWhile 3D Gaussian Splatting (3DGS) has predominantly been explored in the context of general novel view synthesis and dynamic scene reconstruction, its explicit, efficient, and editable nature makes it a compelling candidate for a wide array of specialized, domain-specific applications. The ability to render high-fidelity scenes in real-time, coupled with the potential for physical simulation and integration with sensor data, is driving a paradigm shift in fields that demand robust, interpretable, and resource-efficient pipelines. This subsection highlights emerging opportunities in geoscience, medical imaging, and robotics, emphasizing the shift towards mesh-free simulation and the need for specialized adaptations of the 3DGS framework.\n\n### Geoscience and Environmental Modeling\n\nThe application of 3DGS in geoscience represents a frontier for large-scale environmental analysis and virtual exploration. Traditional Geographic Information Systems (GIS) often rely on discrete mesh or voxel representations, which can be computationally expensive to render and interact with at the fidelity required for immersive analysis. The explicit nature of 3DGS offers a pathway to creating highly detailed, photorealistic digital twins of vast terrains, urban environments, and natural landscapes. By leveraging 3DGS, one can reconstruct complex topographies from drone or satellite imagery, enabling real-time visualization and analysis within a virtual reality context. This allows geoscientists to \"walk through\" reconstructed environments, analyze line-of-sight, and study erosion patterns or urban development with unprecedented visual fidelity.\n\nFurthermore, the scalability of 3DGS is a critical factor for geoscience. Large-scale urban reconstruction is directly applicable here. Methods that partition scenes and manage Level-of-Detail (LoD) are essential for handling the immense scale of geographic data. The explicit representation allows for selective updates; for instance, if a section of a forest changes due to seasonal variations or a construction project, only the relevant Gaussians need to be re-optimized, rather than retraining the entire model. This efficiency is crucial for maintaining up-to-date digital maps. The integration of multi-modal data, such as LiDAR and depth priors [51], is also vital for ensuring geometric accuracy in large-scale outdoor scenes, where visual information alone might be insufficient due to repetitive textures or vast untextured areas like bodies of water. The future research avenue here lies in developing 3DGS pipelines that can ingest and fuse diverse geospatial data sources (e.g., multispectral imagery, radar data) to not only reconstruct geometry and appearance but also to infer semantic properties like vegetation health or material composition directly within the Gaussian representation.\n\n### Medical Imaging and Surgical Planning\n\nIn the domain of medical imaging, the demand for high-precision, real-time visualization is paramount for applications ranging from diagnostic imaging to surgical navigation. 3DGS offers a compelling alternative to traditional mesh-based or volume-rendering techniques used in medical VR/AR systems. For instance, reconstructing a patient's organ from a series of CT or MRI scans can be achieved with 3DGS, providing a photorealistic and interactive model that surgeons can use for pre-operative planning. The explicit control over each Gaussian primitive allows for intuitive manipulation, such as isolating specific anatomical structures or simulating the appearance of tissues under different lighting conditions.\n\nThe challenge in this domain lies in achieving sub-millimeter geometric accuracy and handling the complex, often translucent, properties of biological tissues. While standard 3DGS excels at view synthesis, its geometric reconstruction can be noisy. This is where advancements in surface reconstruction become critical. Techniques like \"Gaussian Opacity Fields (GOF)\" [98] for compact surface extraction and \"2D Gaussian Splatting\" [22] for intrinsic surface modeling are highly relevant. By enforcing geometric constraints, as in \"GeoGaussian\" [65], we can ensure that the reconstructed organ surfaces are smooth and accurate, which is non-negotiable for surgical guidance. Furthermore, the ability to perform real-time relighting [84] could be used to simulate the appearance of an organ under different surgical lighting, aiding in the identification of subtle tissue variations. Future research should focus on developing specialized densification strategies that respect anatomical boundaries and on integrating physics-based rendering to simulate light transport through scattering media like skin or fat, a challenge that \"Don't Splat your Gaussians\" begins to address for volumetric media.\n\n### Robotics, Autonomous Navigation, and SLAM\n\nRobotics and autonomous systems are perhaps the most natural fit for the real-time capabilities of 3DGS. Simultaneous Localization and Mapping (SLAM) is a cornerstone of robotics, and the integration of 3DGS has led to a new class of dense SLAM systems. Methods like \"MM3DGS SLAM\" [89] demonstrate that 3D Gaussians can be used not just for post-hoc reconstruction but for online mapping and pose tracking. By leveraging the differentiable rasterization pipeline, these systems can optimize the map and the camera pose simultaneously, using photometric losses. The explicit nature of the map allows for efficient updates and rendering, which is crucial for real-time path planning and obstacle avoidance.\n\nThe application extends beyond indoor SLAM to large-scale autonomous driving. As explored in \"HGS-Mapping\" and \"GaussianBeV\" [45], 3DGS can be used to create high-definition (HD) maps from multi-camera setups on a vehicle. The Bird's-Eye View (BeV) representation, when fused with 3DGS, can provide a rich, semantically aware map of the environment that is both visually detailed and geometrically consistent. This is a significant step up from sparse point cloud maps. For resource-constrained robotics, such as drones or small mobile robots, the computational efficiency is key. The paper \"Isotropic Gaussian Splatting\" [47] proposes using isotropic kernels to drastically reduce computational complexity, making it feasible to run 3DGS on embedded hardware like NVIDIA Jetson boards. This aligns with the need for \"Embedded Systems and Resource-Constrained Robotics\" where memory and power are limited. Future research avenues include developing robust 3DGS representations that are resilient to dynamic elements in the environment (e.g., moving pedestrians, other vehicles) and creating hybrid maps that combine the photorealism of 3DGS with the structural guarantees of traditional geometric maps (e.g., planes, quadrics) for safer navigation.\n\n### The Shift to Mesh-Free Simulation and Physical Properties\n\nA recurring theme across these domains is the need to move beyond static reconstruction towards dynamic, physically plausible simulation. The traditional graphics pipeline relies heavily on meshes for physics, but 3DGS offers a compelling \"mesh-free\" alternative. This is exemplified by the concept of \"Simplicits\" and the work on \"PhysGaussian\" and \"GASP,\" which integrate continuum mechanics and Material Point Methods (MPM) directly with Gaussian primitives. This allows for the simulation of elastic objects, cloth, and other deformable materials without the need for a pre-defined mesh topology. For example, \"Spring-Gaus\" [99] demonstrates how a spring-mass model can be integrated with 3D Gaussians to simulate the dynamics of elastic objects from video observations. This is a game-changer for robotics, as a robot could interact with a soft object whose physical properties (stiffness, damping) have been identified from vision and simulated in real-time.\n\nSimilarly, \"Gaussian-Informed Continuum\" [100] proposes a framework for estimating physical properties through visual observations by leveraging 3D Gaussians to guide a continuum simulation. This opens up possibilities for system identification directly from images, which is crucial for manipulating unknown objects. In the medical domain, this could translate to simulating the deformation of organs during surgery. In geoscience, it could model the flow of granular materials or the deformation of terrain. The future research direction is clear: we need to develop more sophisticated constitutive models that can be inferred and applied to 3D Gaussian representations, enabling them to behave like a wide range of real-world materials. This requires bridging the gap between computer vision, physics simulation, and machine learning, creating a unified framework where a 3D representation is simultaneously a visual, geometric, and physical entity.\n\n### Conclusion on Future Directions\n\nThe expansion of 3D Gaussian Splatting into these specialized domains underscores a fundamental trend: the convergence of reconstruction, rendering, and simulation. The future of 3DGS lies not just in improving rendering quality, but in enriching the representation with the necessary attributes for domain-specific tasks. This includes robust geometric priors for medical and robotic accuracy, efficient data structures for geospatial scale, and integrated physical models for simulation. The challenge is to maintain the core strengths of 3DGS\u2014speed and explicitness\u2014while adding the complexity required for these demanding applications. The research avenues are vast, from developing domain-aware densification and regularization techniques to creating hybrid pipelines that leverage the strengths of both explicit Gaussians and implicit neural fields for a new generation of intelligent, interactive, and physically grounded 3D systems.\n\n\n## References\n\n[1] NeRF  Representing Scenes as Neural Radiance Fields for View Synthesis\n\n[2] Neural Radiance Fields  Past, Present, and Future\n\n[3] NeRF  Neural Radiance Field in 3D Vision, A Comprehensive Review\n\n[4] Recursive-NeRF  An Efficient and Dynamically Growing NeRF\n\n[5] MIMO-NeRF  Fast Neural Rendering with Multi-input Multi-output Neural  Radiance Fields\n\n[6] Efficient View Synthesis with Neural Radiance Distribution Field\n\n[7] Baking Neural Radiance Fields for Real-Time View Synthesis\n\n[8] Efficient Neural Light Fields (ENeLF) for Mobile Devices\n\n[9] Immersive Neural Graphics Primitives\n\n[10] NeRF-Editing  Geometry Editing of Neural Radiance Fields\n\n[11] NeRFMeshing  Distilling Neural Radiance Fields into  Geometrically-Accurate 3D Meshes\n\n[12] Ref-NeRF  Structured View-Dependent Appearance for Neural Radiance  Fields\n\n[13] Reference-guided Controllable Inpainting of Neural Radiance Fields\n\n[14] Seal-3D  Interactive Pixel-Level Editing for Neural Radiance Fields\n\n[15] 3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities\n\n[16] 3D Gaussian as a New Vision Era  A Survey\n\n[17] Spec-Gaussian  Anisotropic View-Dependent Appearance for 3D Gaussian  Splatting\n\n[18] GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled Appearance and Geometry Modeling\n\n[19] Mathematical Supplement for the $\\texttt{gsplat}$ Library\n\n[20] Analytic-Splatting  Anti-Aliased 3D Gaussian Splatting via Analytic  Integration\n\n[21] Gaussian Opacity Fields  Efficient and Compact Surface Reconstruction in  Unbounded Scenes\n\n[22] 2D Gaussian Splatting for Geometrically Accurate Radiance Fields\n\n[23] Revising Densification in Gaussian Splatting\n\n[24] Density-embedding layers  a general framework for adaptive receptive  fields\n\n[25] Density Uncertainty Quantification with NeRF-Ensembles  Impact of Data  and Scene Constraints\n\n[26] Gaussian Splatting with Localized Points Management\n\n[27] NeRFs  The Search for the Best 3D Representation\n\n[28] 3DGSR  Implicit Surface Reconstruction with 3D Gaussian Splatting\n\n[29] GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting\n\n[30] Interactive All-Hex Meshing via Cuboid Decomposition\n\n[31] SAGS: Structure-Aware 3D Gaussian Splatting\n\n[32] SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic Scene Destruction\n\n[33] I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions\n\n[34] Search Combinators\n\n[35] PDQP qpoly = ALL\n\n[36]  {Math, Philosophy, Programming, Writing}  = 1\n\n[37] StopThePop  Sorted Gaussian Splatting for View-Consistent Real-time  Rendering\n\n[38] AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius\n\n[39] Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting\n\n[40] 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes\n\n[41] A Survey on 3D Gaussian Splatting\n\n[42] ABSP System for The Third DIHARD Challenge\n\n[43] Pixel Objectness\n\n[44] Mini-Splatting  Representing Scenes with a Constrained Number of  Gaussians\n\n[45] The Gaussian Transform\n\n[46] Mesh2Tex  Generating Mesh Textures from Image Queries\n\n[47] Gaussian Splatting in Style\n\n[48] MIP =RE\n\n[49] Mip-Splatting  Alias-free 3D Gaussian Splatting\n\n[50] A Flexible Pipeline for the Optimization of CSG Trees\n\n[51] From Chaos to Clarity: 3DGS in the Dark\n\n[52] Compact 3D Gaussian Representation for Radiance Field\n\n[53] LP-3DGS: Learning to Prune 3D Gaussian Splatting\n\n[54] Do-Not-Answer  A Dataset for Evaluating Safeguards in LLMs\n\n[55] HAC  Hash-grid Assisted Context for 3D Gaussian Splatting Compression\n\n[56] Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields\n\n[57] Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity\n\n[58] 3D-HGS: 3D Half-Gaussian Splatting\n\n[59] 2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and Better Geometry Reconstruction\n\n[60] A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\n\n[61] Octree-GS  Towards Consistent Real-time Rendering with LOD-Structured 3D  Gaussians\n\n[62] FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering\n\n[63] Compact 3D Scene Representation via Self-Organizing Gaussian Grids\n\n[64] GauStudio  A Modular Framework for 3D Gaussian Splatting and Beyond\n\n[65] Geotree of Geodetector: An Anatomy of Knowledge Diffusion of a Novel Statistic\n\n[66] Dynamic 2D Gaussians: Geometrically accurate radiance fields for dynamic objects\n\n[67] RaDe-GS: Rasterizing Depth in Gaussian Splatting\n\n[68] VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction\n\n[69] GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting\n\n[70] Uncited papers are not unread\n\n[71] VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors\n\n[72] D-NeRF  Neural Radiance Fields for Dynamic Scenes\n\n[73] An Efficient 3D Gaussian Representation for Monocular Multi-view Dynamic  Scenes\n\n[74] Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering\n\n[75] GaussianDiffusion  3D Gaussian Splatting for Denoising Diffusion  Probabilistic Models with Structured Noise\n\n[76] BeyondPixels  A Comprehensive Review of the Evolution of Neural Radiance  Fields\n\n[77] Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering\n\n[78] Urban Magnetism Through The Lens of Geo-tagged Photography\n\n[79] GeoGaussian  Geometry-aware Gaussian Splatting for Scene Rendering\n\n[80] DISTWAR  Fast Differentiable Rendering on Raster-based Rendering  Pipelines\n\n[81] 3D Gaussian Splatting for Real-Time Radiance Field Rendering\n\n[82] BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian Splatting\n\n[83] RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for Realistic Rendering\n\n[84] A Roadmap for Big Model\n\n[85] From NeRFs to Gaussian Splats, and Back\n\n[86] The Category TOF\n\n[87] Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form  Color Estimation Method\n\n[88] PhySG  Inverse Rendering with Spherical Gaussians for Physics-based  Material Editing and Relighting\n\n[89] MM3DGS SLAM  Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements\n\n[90] Reducing the Memory Footprint of 3D Gaussian Splatting\n\n[91] Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering\n\n[92] Clean-NeRF  Reformulating NeRF to account for View-Dependent  Observations\n\n[93] Physics-AI Symbiosis\n\n[94] DreamGaussian4D  Generative 4D Gaussian Splatting\n\n[95] GASP: Gaussian Splatting for Physic-Based Simulations\n\n[96] A Refinement of Expurgation\n\n[97] Research Re  search & Re-search\n\n[98] Convolutional Neural Opacity Radiance Fields\n\n[99] Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D  Gaussians\n\n[100] Gaussian-Informed Continuum for Physical Property Identification and Simulation\n\n\n",
    "reference": {
        "1": "2003.08934v2",
        "2": "2304.10050v2",
        "3": "2210.00379v5",
        "4": "2105.09103v1",
        "5": "2310.01821v1",
        "6": "2308.11130v1",
        "7": "2103.14645v1",
        "8": "2406.00598v1",
        "9": "2211.13494v1",
        "10": "2205.04978v1",
        "11": "2303.09431v1",
        "12": "2112.03907v1",
        "13": "2304.09677v2",
        "14": "2307.15131v2",
        "15": "2407.17418v1",
        "16": "2402.07181v1",
        "17": "2402.15870v1",
        "18": "2409.12954v1",
        "19": "2312.02121v1",
        "20": "2403.11056v2",
        "21": "2404.10772v1",
        "22": "2403.17888v1",
        "23": "2404.06109v1",
        "24": "2006.12779v2",
        "25": "2312.14664v1",
        "26": "2406.04251v2",
        "27": "2308.02751v2",
        "28": "2404.00409v1",
        "29": "2405.15491v1",
        "30": "2109.06279v1",
        "31": "2404.19149v1",
        "32": "2405.17793v1",
        "33": "2405.06408v1",
        "34": "1203.1095v1",
        "35": "1805.08577v1",
        "36": "1803.05998v4",
        "37": "2402.00525v1",
        "38": "2409.08669v1",
        "39": "2402.13827v1",
        "40": "2407.07090v2",
        "41": "2401.03890v2",
        "42": "2102.09939v1",
        "43": "1701.05349v2",
        "44": "2403.14166v1",
        "45": "2006.11698v1",
        "46": "2304.05868v1",
        "47": "2403.08498v1",
        "48": "2001.04383v3",
        "49": "2311.16493v1",
        "50": "2008.03674v2",
        "51": "2406.08300v1",
        "52": "2311.13681v2",
        "53": "2405.18784v1",
        "54": "2308.13387v2",
        "55": "2403.14530v2",
        "56": "2408.03822v1",
        "57": "2407.09733v2",
        "58": "2406.02720v2",
        "59": "2408.16982v1",
        "60": "2406.12080v1",
        "61": "2403.17898v1",
        "62": "2408.12894v1",
        "63": "2312.13299v1",
        "64": "2403.19632v1",
        "65": "2408.06839v1",
        "66": "2409.14072v1",
        "67": "2406.01467v2",
        "68": "2406.05774v1",
        "69": "2406.18199v1",
        "70": "2002.10160v1",
        "71": "2407.02945v3",
        "72": "2011.13961v1",
        "73": "2311.12897v1",
        "74": "2408.06286v1",
        "75": "2311.11221v1",
        "76": "2306.03000v3",
        "77": "2409.10335v1",
        "78": "1503.05502v3",
        "79": "2403.11324v1",
        "80": "2401.05345v1",
        "81": "2308.04079v1",
        "82": "2408.13370v1",
        "83": "2406.05852v1",
        "84": "2203.14101v4",
        "85": "2405.09717v3",
        "86": "1804.10360v4",
        "87": "2312.12726v1",
        "88": "2104.00674v1",
        "89": "2404.00923v1",
        "90": "2406.17074v1",
        "91": "2403.14244v1",
        "92": "2303.14707v1",
        "93": "2109.05959v1",
        "94": "2312.17142v2",
        "95": "2409.05819v1",
        "96": "2307.12162v3",
        "97": "2403.13705v1",
        "98": "2104.01772v1",
        "99": "2403.09434v2",
        "100": "2406.14927v1"
    }
}