# A Comprehensive Survey on Multimodal Large Language Models: Architectures, Training, Evaluation, and Applications

## 1 Introduction and Background

### 1.1 Definition and Core Functionality of MLLMs

Multimodal Large Language Models (MLLMs) represent a pivotal evolution in artificial intelligence, extending the remarkable reasoning and generation capabilities of Large Language Models (LLMs) to encompass a diverse spectrum of sensory data. At their core, MLLMs are defined as systems capable of processing, interpreting, and reasoning over heterogeneous data types—including text, images, audio, and video—within a unified architectural and cognitive framework [1]. Unlike their unimodal predecessors, which were siloed into distinct domains of natural language processing or computer vision, MLLMs are designed to perceive the world through multiple sensory channels simultaneously, much like humans. This conceptual foundation rests on the idea of a generalist "brain" (the LLM) augmented with sensory "organs" (modality encoders), positioning the LLM as the central cognitive component [2]. The core functionality, therefore, is not merely to "see" or "hear" but to reason about what is seen and heard in the context of language, enabling capabilities like Visual Question Answering (VQA), cross-modal generation, and complex instruction following [1].

The core functionality of an MLLM can be broken down into a sequence of critical operations that bridge sensory data with linguistic reasoning:

1.  **Multimodal Perception and Encoding:** The first step is to perceive the world in its raw, multimodal form. For visual data, this involves using powerful vision encoders like CLIP's image encoder or Vision Transformers (ViT) to convert pixel data into a sequence of feature vectors or "visual tokens" [1]. For audio, encoders like Whisper are used to transform sound waves into textual representations or embeddings. The quality and resolution of this initial encoding are critical, as they determine the richness of information available to the downstream LLM. Recent work has explored scaling image resolution and the number of visual tokens to enhance performance on detail-oriented tasks like document understanding and OCR [3].

2.  **Modality Alignment:** This is the crucial "bridge" step. The representations from different modalities exist in distinct feature spaces and must be mapped to a common space that the LLM can understand. Early paradigms, such as LLaVA, employed a simple linear layer to project visual features into the word embedding space of the LLM, demonstrating that even this straightforward approach could yield impressive results [1]. More advanced architectures, like Flamingo, utilized cross-attention mechanisms, allowing the LLM to dynamically attend to visual features during text generation [4]. Other innovative approaches include using a Q-Former to learn a compressed, query-based representation of the image before feeding it to the LLM, or unified frameworks like OneLLM that use a universal projection module to align eight different modalities with language [5].

3.  **Linguistic Reasoning and Integration:** Once the multimodal information is represented as a sequence of tokens within the LLM's input space, the model's pre-trained reasoning capabilities take over. The self-attention mechanism within the LLM's transformer layers allows it to draw connections between textual instructions and the aligned visual or auditory cues. For example, when asked "What is the breed of the dog in this image?", the model can associate the token for "dog" with the visual features corresponding to a specific breed, and then generate a coherent textual answer. This integration enables emergent capabilities that go beyond simple perception, such as OCR-free mathematical reasoning on charts, writing stories based on images, and understanding complex spatial relationships [1].

4.  **Cross-Modal Generation:** While many MLLMs are initially designed for understanding (input: image, output: text), the paradigm is rapidly expanding to include generation across modalities. This includes text-to-image generation, image editing via natural language instructions, and even video generation. The core functionality here is to use the LLM as a controller or "brain" that guides a generative model (like a diffusion model) based on multimodal inputs and instructions [6]. For instance, a model could take a source image and a text prompt ("change the dog's collar to red") and generate the edited image.

The power of MLLMs lies in their ability to perform these functions in a zero-shot or few-shot manner, generalizing to new tasks and concepts without explicit task-specific training. This is a direct inheritance from the in-context learning abilities of LLMs. The core functionality is not just about recognizing objects or transcribing speech; it is about building a holistic, grounded understanding of a scenario described through multiple modalities and reasoning about it in an open-ended, conversational way. For example, an MLLM can look at a picture of a messy kitchen and not only identify the objects but also infer the sequence of events that led to the mess, answer questions about it, and even suggest a cleaning plan, all by leveraging its integrated world knowledge [1].

This definition and functional description highlight the fundamental shift MLLMs represent. They move AI from a collection of specialized tools to a single, versatile agent capable of perceiving and interacting with the world in a more holistic and human-like manner. The core function of bridging sensory inputs with linguistic reasoning is the key that unlocks a vast array of complex, real-world applications, from autonomous systems and healthcare diagnostics to creative content generation and advanced educational tools. The ongoing research in this field is focused on refining this bridge—making it more efficient, more accurate, and capable of handling an ever-expanding universe of modalities and tasks.

### 1.2 Evolutionary Trajectory: From Unimodal to Multimodal

The journey towards Multimodal Large Language Models (MLLMs) is not a sudden leap but a gradual, evolutionary process rooted in the distinct advancements of unimodal artificial intelligence. To fully appreciate the architecture and capabilities of modern MLLMs, one must trace the lineage from the dominance of specialized Large Language Models (LLMs) and Computer Vision (CV) models to the pivotal era of Vision-Language (VL) pre-training. This trajectory reflects a broader paradigm shift in AI research: moving from siloed, task-specific systems toward unified, generalist foundations capable of perceiving and reasoning across diverse data modalities.

The foundation of the current AI revolution rests upon the unprecedented success of Large Language Models. For years, natural language processing (NLP) was dominated by recurrent and convolutional architectures, but the introduction of the Transformer architecture fundamentally altered the landscape. Models like GPT-3 demonstrated that scaling up model parameters and training data could yield "emergent" capabilities, such as few-shot learning and complex reasoning, which were absent in smaller models [1]. These LLMs became the "brain" of the system, possessing a robust world model and the ability to generate coherent, contextually relevant text. However, despite their linguistic prowess, they remained fundamentally unimodal—blind to the visual world. They could describe a cat based on text descriptions but could not perceive a cat in an image.

Simultaneously, the field of Computer Vision was undergoing its own revolution. The era of hand-crafted features gave way to deep learning, starting with architectures like AlexNet and evolving into highly efficient models such as ResNet [7]. The introduction of the Vision Transformer (ViT) marked a critical milestone, proving that the Transformer architecture, which excelled in NLP, could also be adapted for visual processing by treating images as sequences of patches [8]. These vision encoders became the "eyes" of AI, capable of extracting rich, hierarchical features from pixel data. However, these models were largely discriminative; they were trained to classify objects or detect boundaries but lacked the generative and reasoning capabilities of LLMs. They could identify an object but could not explain the context of a scene or answer complex questions about it.

The convergence of these two powerful unimodal streams led to the first phase of multimodal research: specialized Vision-Language (VL) models. Before the advent of MLLMs, researchers focused on bridging the gap between visual and textual modalities through pre-training objectives designed to align these distinct feature spaces. This era was characterized by models that learned from massive-scale image-text pairs found on the internet, such as the LAION and CC12M datasets [1]. The goal was to learn a shared representation where an image embedding and its corresponding text embedding would be close in a latent space.

Early VL models utilized various architectures to facilitate this alignment. Some relied on dual-encoder structures, while others employed fusion-based approaches that combined visual and textual features within a shared network. The concept of "pre-training" became paramount. By pre-training on web-scale data, these models learned general visual concepts and linguistic associations, which could then be fine-tuned for specific downstream tasks like Visual Question Answering (VQA) or image captioning [9]. This pre-train-and-fine-tune paradigm was the standard for years, establishing the critical "modality alignment" that serves as the bedrock for today's MLLMs.

However, these specialized VL models had significant limitations. They were often rigid in their input-output formats, typically requiring task-specific heads for each new application. They lacked theLMs. The transition to the modern MLLM paradigm was catalyzed by the realization that the powerful reasoning capabilities of LLMs could be leveraged directly for multimodal tasks. Instead of training a new model from scratch to fuse vision and language, researchers began to ask: can we simply "teach" a pre-trained LLM to understand images? This question led to the foundational architectural paradigms that define the field today, setting the stage for the explosive growth of MLLMs like GPT-4V and LLaVA.

The evolutionary path from unimodal dominance to the current multimodal era can be categorized into three distinct phases:

1.  **The Era of Unimodal Specialists:** This initial phase was defined by the independent maturation of NLP and CV. LLMs like GPT-3 [1] and BERT revolutionized text processing, while CV models like ResNet and ViT [8] advanced visual perception. These models were powerful but isolated. An LLM could write an essay but could not read a diagram; a CV model could classify a tumor in an MRI scan but could not generate a radiology report. This specialization created a dichotomy in AI development, where progress in one domain did not inherently benefit the other. The lack of a unified model meant that building applications that required both vision and language required complex, brittle pipelines of separate models.

2.  **The Bridge Era: Vision-Language Pre-training:** The realization that vision and language could be jointly learned led to the development of specialized VL models. This era focused on learning joint embeddings from massive image-text datasets. The core innovation was the pre-training objective, such as masked language modeling applied to image-text pairs or contrastive learning. Models like CLIP [7] demonstrated that by training on 400 million image-text pairs from the internet, a model could learn robust visual concepts that aligned with natural language descriptions. This allowed for powerful zero-shot classification capabilities, where a model could recognize an object it had never explicitly been trained to identify by matching it with a text prompt. However, these models were primarily encoders; they were excellent at understanding and matching but not at generation or complex reasoning. They laid the groundwork for modality alignment but lacked the generative "brain" of an LLM.

3.  **The Emergence of MLLMs:** The current era represents a paradigm shift where the LLM is placed at the center of the system. Instead of training a new fusion model, researchers developed methods to connect a frozen vision encoder and a frozen LLM with a lightweight projection module. This approach, popularized by models like LLaVA [1], showed that a simple linear projection of visual features into the LLM's text embedding space could unlock impressive multimodal capabilities. This "LLM-as-a-brain" architecture leverages the pre-existing world knowledge and reasoning capabilities of the LLM, requiring only a small amount of instruction tuning to adapt it to visual inputs. This shift dramatically reduced the computational cost of developing MLLMs and allowed the field to ride the scaling laws of LLMs. The evolutionary trajectory thus culminates in a model that is not just a fusion of vision and language, but a unified system where the language model reasons about visual information as if it were a foreign language it has just learned to read.

The transition from unimodal to multimodal was not merely an architectural change but also a shift in data and training philosophy. Early VL models relied on simple image-text pairs from the web. Modern MLLMs, however, require high-quality instruction-tuning data to learn how to follow complex multimodal instructions [10]. This data often involves synthetic generation using powerful models like GPT-4V to create detailed conversations about images, a process that bridges the gap between passive perception and active reasoning.

Furthermore, the evolution has been driven by the need for generalization. Specialized VL models were often good at one or two tasks, but MLLMs aim to be generalists. They can describe a scene, answer a question, read text in an image (OCR), reason about spatial relationships, and even write code based on a diagram, all within a single model [11]. This generalist capability is the hallmark of the current paradigm and distinguishes MLLMs from their predecessors.

In conclusion, the evolutionary trajectory from unimodal models to MLLMs is a story of convergence. It began with the parallel development of powerful "eyes" (CV models) and "brains" (LLMs). It progressed through a phase of "bridging" where these senses were loosely aligned. It has now arrived at a phase of "integration," where the eyes feed information directly to the brain in a language-like format, enabling the emergence of true multimodal intelligence. This journey highlights that the path to AGI may not require building a monolithic new architecture from scratch, but rather intelligently connecting the powerful, specialized components we have already built.

### 1.3 Foundational Architectures and Paradigm Shifts

The evolution of Multimodal Large Language Models (MLLMs) from their unimodal predecessors represents a pivotal moment in artificial intelligence, marked by the convergence of powerful visual perception capabilities with the linguistic reasoning prowess of Large Language Models (LLMs). This convergence was not instantaneous but was catalyzed by a few seminal architectures that established the foundational paradigms for modern MLLMs. Among these, two distinct architectural lineages stand out: the cross-attention-based approach popularized by DeepMind's Flamingo and the simple linear projection paradigm introduced by Microsoft's LLaVA. These two approaches differ fundamentally in how they bridge the modality gap, how they are trained, and the resulting capabilities they unlock, effectively setting the stage for the rapid innovation observed in the field today.

The Flamingo architecture, introduced in the paper [12], represents a landmark achievement in bridging frozen vision encoders with pre-trained LLMs. Prior to Flamingo, integrating vision and language typically required either training a model from scratch or heavily fine-tuning both components. Flamingo's key innovation was its ability to inject visual information into a frozen, pre-trained LLM without disrupting its exceptional language capabilities. It achieves this by inserting new cross-attention layers between the existing layers of the frozen LLM. In these layers, the LLM's internal representations attend to visual features extracted by a Vision Encoder (typically a Perceiver Resampler). This design allows the LLM to "see" images and videos by conditioning its text generation on visual context. Crucially, Flamingo was trained on massive, interleaved datasets of images and text (e.g., web pages containing both), which endowed it with powerful few-shot learning capabilities. By providing a few image-text examples in the prompt, Flamingo could adapt to new tasks, such as visual question answering or captioning, without any task-specific fine-tuning. This cross-attention mechanism, while computationally intensive, ensures that the visual information is deeply integrated into the LLM's reasoning process at every layer, making it a potent but architecturally complex solution.

In stark contrast, the LLaVA paradigm, proposed in the paper [1], introduced a remarkably simple yet effective alternative. LLaVA (Large Language and Vision Assistant) also operates on a frozen vision encoder (CLIP) and a frozen LLM. However, instead of inserting complex cross-attention layers, LLaVA employs a simple linear projection layer (or a lightweight MLP) to map the visual features from the vision encoder into the semantic space of the LLM. This projection transforms the visual features into a sequence of "visual tokens" that are treated just like text tokens by the LLM. The entire sequence is then prepended to the text prompt. This architectural simplicity is coupled with a straightforward training strategy called "visual instruction tuning." LLaVA is trained on a high-quality, synthetic dataset of image-instruction-response pairs, generated using a powerful LLM (like GPT-4) to describe images. By training only the lightweight projection layer, LLaVA effectively teaches the LLM to understand the visual tokens. This approach demonstrated that a frozen LLM could be adapted to understand and reason about visual content with minimal architectural changes and computational cost, democratizing the development of MLLMs.

The fundamental difference between these two paradigms lies in the depth of fusion and the training philosophy. Flamingo's cross-attention mechanism allows for a deep, dynamic interaction between visual and textual modalities throughout the LLM's forward pass. The LLM can query visual features at different levels of abstraction as it generates each token. This is powerful for complex reasoning but comes with a higher computational footprint and architectural complexity. LLaVA, on the other hand, performs a static fusion: all visual information is encoded into a fixed set of tokens at the beginning. The LLM then processes this "visual prompt" autoregressively. While this might seem less sophisticated, LLaVA's success proved that the powerful in-context learning and reasoning abilities of modern LLMs are highly transferable to the visual domain, even with a simple alignment mechanism. This shifted the research focus from complex architectural engineering to data curation and instruction tuning strategies.

The impact of these two foundational paradigms has been immense, spawning a vast ecosystem of research. The Flamingo architecture inspired a line of work focused on improving the efficiency and performance of cross-attention-based models, such as [13], which provided an open-source replication effort. Meanwhile, the LLaVA paradigm's simplicity led to an explosion of follow-up work. Researchers quickly realized that while the linear projection was simple, its effectiveness was highly dependent on the quality and diversity of the instruction-tuning data. This led to the creation of larger and more diverse instruction datasets, such as [14], and the exploration of more efficient projectors, like the Q-Former used in some early Flamingo-inspired models and later adapted in others, or the novel projectors in [15] and [16]. The core insight from LLaVA—that a frozen LLM can learn to "see" through a simple alignment layer—became the dominant open-source paradigm due to its ease of implementation and training efficiency.

However, the field did not remain static. The limitations of both paradigms spurred further innovation. The static nature of LLaVA's visual tokenization can struggle with high-resolution images, as the number of tokens either explodes or detail is lost. This led to research on dynamic high-resolution strategies, such as the tile-based approach in [17] and the dual-perspective cropping in [18]. These methods aim to provide the model with both a global context and fine-grained local details, addressing a key weakness of the original LLaVA design. Furthermore, the computational cost of processing many visual tokens, even in the LLaVA paradigm, became a bottleneck. This motivated research into token reduction techniques like [16] and the removal of redundant tokens during inference as seen in [19].

The evolution also involved a re-evaluation of the "frozen LLM" assumption. While both Flamingo and LLaVA initially froze the LLM, later work explored the benefits of partial or full fine-tuning. Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA became standard practice. The paper [20] highlights how adapting the LLM with specialized experts can mitigate performance degradation when training on diverse datasets. This represents a paradigm shift from purely aligning modalities to actively adapting the LLM's reasoning capabilities for multimodal tasks.

Furthermore, the architectural landscape has expanded beyond the two initial paradigms. The rise of State Space Models (SSMs) like Mamba offers a new path for efficiency. Models like [21] and [22] replace the Transformer backbone of the LLM with a Mamba architecture, promising linear scaling with sequence length and faster inference, which is particularly beneficial for long contexts like videos or high-resolution images. This represents a fundamental architectural shift, moving away from the attention-based core that defined both Flamingo and LLaVA.

In conclusion, the foundational architectures of Flamingo and LLaVA established the two primary paths for MLLM development: deep, cross-attention-based fusion for powerful few-shot reasoning, and simple, alignment-based fusion for efficient and scalable instruction following. Their contrasting designs and training strategies defined the initial research landscape. The subsequent explosion of innovation has been largely driven by addressing the limitations and building upon the strengths of these two paradigms. This includes improving visual perception through high-resolution processing and mixture of encoders [23], enhancing efficiency through token reduction and novel architectures [24], and deepening reasoning capabilities through advanced training strategies and data curation [17]. The journey from Flamingo and LLaVA to today's state-of-the-art models illustrates a dynamic interplay between architectural design, data strategy, and the relentless pursuit of more capable, efficient, and generalist multimodal intelligence.

The landscape of Multimodal Large Language Models (MLLMs) has undergone a paradigm shift, moving away from rigid, narrow systems designed for specific tasks toward flexible, generalist models capable of handling a vast array of multimodal challenges. This transition marks a pivotal moment in the pursuit of Artificial General Intelligence (AGI), where models are no longer confined to a single domain but exhibit a broad, adaptable understanding of the world. The release of proprietary models like GPT-4V and Gemini has been instrumental in this shift, demonstrating that scaling both model size and data diversity can lead to the emergence of capabilities that were previously unattainable by their specialized predecessors [25; 26].

Historically, multimodal systems were constructed as pipelines of distinct components: a vision model for feature extraction, a language model for processing, and a fusion mechanism to bridge the two. These systems, while effective on curated benchmarks, often failed when faced with open-ended, real-world scenarios requiring fluid reasoning across modalities. The advent of generalist MLLMs has disrupted this approach by integrating sensory perception and linguistic reasoning into a single, cohesive architecture. These models are trained on massive, heterogeneous datasets, enabling them to generalize across tasks they have never explicitly encountered. This evolution is not merely an incremental improvement but a qualitative leap, as evidenced by the "sparks" of AGI observed in early GPT-4 evaluations, where the model demonstrated human-level performance across diverse domains including vision, law, and psychology [27].

One of the most defining characteristics of these emerging generalist models is their ability to perform OCR-free reasoning. Traditional document understanding systems relied heavily on separate Optical Character Recognition (OCR) engines to extract text before any reasoning could occur. This pipeline approach was brittle and prone to error. In contrast, models like GPT-4V and the open-source InternVL 1.5 can directly interpret visual representations of text within images, performing complex reasoning tasks without an explicit text extraction step. This capability allows them to analyze charts, diagrams, and scanned documents with a level of understanding that rivals human experts. For instance, InternVL 1. a a " " " " ",, " " " " models " " in the, with,, and, the,,,, models,,,, the, the, The  models, the, the. models models, and parameters,, different different,, different,, with,,,, models with parts parts, with have have with parts with with, have with with parts with with have have parts,, have have with with,,, have have have with, have have have have have have have have have have, have with have have have have have have have have are with with with with with have. in covering covering The The current of L vision L L M L The for for the, they like GPT-- and and, LLaVA, However, the rise of generalist MLLMs also brings new challenges. As these models become more capable, evaluating their performance becomes increasingly difficult. Traditional benchmarks often fall short in assessing the nuanced, open-ended reasoning that generalist models exhibit. This has led to the development of new evaluation frameworks that focus on holistic assessment, including trustworthiness, causal reasoning, and robustness [28; 29]. Moreover, the very nature of "emergent" capabilities implies that we may not fully understand the limits or failure modes of these systems, making safety and alignment critical areas of ongoing research.

In conclusion, the emergence of generalist capabilities in MLLMs represents a fundamental shift in artificial intelligence. By moving beyond narrow task specialization, models like GPT-4V and Gemini are demonstrating a form of intelligence that is more flexible, creative, and aligned with human cognition. This evolution is driven by architectural innovations, high-quality instruction tuning, and the scaling of both data and model size. As we continue to explore the frontiers of what these models can do, from complex visual reasoning to agentic tool use, it is clear that we are witnessing the dawn of a new era in AI—one where machines can perceive, reason, and communicate across modalities in ways that were once the sole domain of human intelligence.

### 1.4 The Emergence of Generalist Capabilities

The landscape of Multimodal Large Language Models (MLLMs) has undergone a paradigm shift, moving away from rigid, narrow systems designed for specific tasks toward flexible, generalist models capable of handling a vast array of multimodal challenges. This transition marks a pivotal moment in the pursuit of Artificial General Intelligence (AGI), where models are no longer confined to a single domain but exhibit a broad, adaptable understanding of the world. The release of proprietary models like GPT-4V and Gemini has been instrumental in this shift, demonstrating that scaling both model size and data diversity can lead to the emergence of capabilities that were previously unattainable by their specialized predecessors [25; 26].

Historically, multimodal systems were constructed as pipelines of distinct components: a vision model for feature extraction, a language model for processing, and a fusion mechanism to bridge the two. These systems, while effective on curated benchmarks, often failed when faced with open-ended, real-world scenarios requiring fluid reasoning across modalities. The advent of generalist MLLMs has disrupted this approach by integrating sensory perception and linguistic reasoning into a single, cohesive architecture. These models are trained on massive, heterogeneous datasets, enabling them to generalize across tasks they have never explicitly encountered. This evolution is not merely an incremental improvement but a qualitative leap, as evidenced by the "sparks" of AGI observed in early GPT-4 evaluations, where the model demonstrated human-level performance across diverse domains including vision, law, and psychology [27].

One of the most defining characteristics of these emerging generalist models is their ability to perform OCR-free reasoning. Traditional document understanding systems relied heavily on separate Optical Character Recognition (OCR) engines to extract text before any reasoning could occur. This pipeline approach was brittle and prone to error. In contrast, models like GPT-4V and the open-source InternVL 1.5 can directly interpret visual representations of text within images, performing complex reasoning tasks without an explicit text extraction step. This capability allows them to analyze charts, diagrams, and scanned documents with a level of understanding that rivals human experts. For instance, InternVL 1.5 introduces a "Dynamic High-Resolution" mechanism that allows it to process images at up to 4K resolution, significantly boosting its performance on OCR-related tasks and demonstrating the power of high-fidelity visual perception in generalist reasoning [30].

Furthermore, generalist MLLMs have shown remarkable proficiency in complex instruction following, a capability that distinguishes them from earlier models that were often limited to simple Q&A formats. These models can parse nuanced, multi-part instructions that require interleaved understanding of visual and textual cues. For example, they can follow instructions like "Identify the object in the image that is out of place and explain why," which requires not only object recognition but also contextual reasoning and natural language explanation. This ability to handle open-ended, user-tailored tasks is a hallmark of generalist intelligence, moving beyond the static, predefined task formats of the past [11]. The shift towards instruction tuning has been crucial here, as it aligns the model's behavior with human intent, enabling it to function as a versatile assistant rather than a mere pattern recognizer.

The emergence of these capabilities is also reshaping the ecosystem of AI development. We are witnessing a convergence where the gap between open-source and proprietary models is narrowing. Models like InternVL 1.5 and MiniCPM-V are rapidly closing the performance gap with GPT-4V, achieving state-of-the-art results on various benchmarks while being deployable on end-side devices [30; 31]. This democratization of generalist capabilities suggests that the future of AI lies not in a monolithic, centralized model but in a diverse ecosystem of models tailored to different needs—from cloud-based behemoths to efficient, on-device assistants.

However, the rise of generalist MLLMs also brings new challenges. As these models become more capable, evaluating their performance becomes increasingly difficult. Traditional benchmarks often fall short in assessing the nuanced, open-ended reasoning that generalist models exhibit. This has led to the development of new evaluation frameworks that focus on holistic assessment, including trustworthiness, causal reasoning, and robustness [28; 29]. Moreover, the very nature of "emergent" capabilities implies that we may not fully understand the limits or failure modes of these systems, making safety and alignment critical areas of ongoing research.

In conclusion, the emergence of generalist capabilities in MLLMs represents a fundamental shift in artificial intelligence. By moving beyond narrow task specialization, models like GPT-4V and Gemini are demonstrating a form of intelligence that is more flexible, creative, and aligned with human cognition. This evolution is driven by architectural innovations, high-quality instruction tuning, and the scaling of both data and model size. As we continue to explore the frontiers of what these models can do, from complex visual reasoning to agentic tool use, it is clear that we are witnessing the dawn of a new era in AI—one where machines can perceive, reason, and communicate across modalities in ways that were once the sole domain of human intelligence.

### 1.5 Scope and Taxonomy of the Survey

This section provides a comprehensive roadmap for our exploration of Multimodal Large Language Models (MLLMs), delineating scope and presenting a structured taxonomy to organize this rapidly evolving landscape. The primary objective of this survey is to synthesize foundational principles, cutting-edge advancements, and persistent challenges that define the current state of MLLM research. To achieve this, we move beyond a superficial overview, delving into the intricate interplay between architectural design, data curation, training paradigms, evaluation methodologies, and the critical imperative of building robust and safe systems. Our scope is intentionally broad, aiming to capture the holistic ecosystem required to develop, assess, and deploy capable and trustworthy MLLMs.

The survey is structured into eight core sections, each addressing a critical pillar of the MLLM domain. Following this introductory section, we begin with a deep dive into **Core Architectures and Multimodal Fusion (Section 2)**. This section forms the bedrock of our analysis, exploring the fundamental building blocks of MLLMs. We will dissect the various strategies for aligning heterogeneous data streams, from the simple yet effective linear projection methods pioneered by models like LLaVA to more sophisticated, learnable alignment modules such as Q-Former. Furthermore, we will examine the evolution of vision encoders, the impact of resolution scaling, and the emergence of unified and hybrid architectures that challenge the conventional encoder-projector-LLM paradigm. The discussion will also encompass parameter-efficient fusion techniques and critical optimizations for deployment, acknowledging the practical constraints that shape architectural choices.

Recognizing that model performance is inextricably linked to the quality and diversity of training data, we then turn to the **Data Ecosystem and Curation Strategies (Section 3)**. This section underscores the pivotal role of data in MLLM development, surveying the vast pre-training corpora like LAION and the meticulously crafted instruction-tuning datasets such as ShareGPT4V. We will explore the nuanced techniques for data curation, quality refinement, and the increasingly important role of synthetic data generation in bridging the modality gap. The section also addresses the critical need for data efficiency, covering methods for intelligent data selection and the unique challenges associated with curating data for specialized domains like 3D point clouds and video.

Building on the foundations of architecture and data, **Section 4: Training Paradigms and Optimization** details the methodologies that breathe life into these models. We will trace the typical MLLM training pipeline, from the initial pre-training phase for modality alignment to the crucial visual instruction tuning that elicits complex reasoning and conversational abilities. This section will also provide a thorough analysis of parameter-efficient fine-tuning (PEFT) techniques like LoRA, which have become indispensable for adapting large models to specific tasks without prohibitive computational costs. Additionally, we will cover optimization strategies such as quantization and pruning, which are essential for making these resource-intensive models accessible for real-world deployment.

A model is only as good as its evaluation, a principle that motivates our fifth section, **Evaluation Benchmarks and Metrics (Section 5)**. This part of the survey provides a panoramic view of the benchmarks used to measure MLLM capabilities, moving from foundational tasks like Visual Question Answering (VQA) to complex assessments of logical reasoning, spatial understanding, and temporal dynamics. We will critically examine benchmarks designed to probe for specific weaknesses, such as hallucination (e.g., AMBER) and vulnerability to adversarial attacks (e.g., MM-SafetyBench). The discussion will also extend to the methodologies of evaluation itself, contrasting traditional automatic metrics with the rise of LLM-as-a-Judge paradigms and the importance of domain-specific evaluations in fields like medicine.

With a firm grasp on how MLLMs are built and evaluated, we proceed to explore their **Advanced Capabilities and Reasoning (Section 6)**. This section highlights the "emergent" abilities that distinguish modern MLLMs from their predecessors. We will focus on complex reasoning tasks, including multimodal chain-of-thought (M-CoT) prompting, which enables models to generate step-by-step visual and textual rationales. The section also covers advanced perception, such as interpreting visual prompts like bounding boxes, and the burgeoning field of agentic reasoning, where MLLMs are used to plan and utilize external tools to solve complex, multi-step problems.

The practical impact of these capabilities is the focus of **Section 7: Applications and Real-World Deployments**. Here, we survey the diverse and transformative applications of MLLMs across various sectors. We will discuss their integration into multimodal retrieval-augmented generation (RAG) systems to combat hallucinations, their role as the perceptual and cognitive core for autonomous systems and robotics, and their deployment in specialized domains like healthcare for diagnostic assistance and finance for market analysis. This section also highlights their use in creative content generation, mixed reality, and even novel applications in wireless network management, illustrating their versatility as a general-purpose intelligence layer.

Finally, we confront the significant limitations and risks in **Section 8: Challenges: Robustness, Safety, and Hallucination**. This section provides a critical assessment of the vulnerabilities inherent in MLLMs. We will dissect their susceptibility to adversarial attacks, the mechanisms of safety alignment failures and jailbreaking, and the pervasive issue of object and factual hallucination. This critical analysis is informed by a body of research that questions the straightforward correlation between standard testing metrics and true model robustness, as highlighted in studies that find limited correlation between coverage criteria and robustness [32]. We will survey the benchmarks designed to quantify these risks and the mitigation strategies being developed, from training-based alignment to inference-time guardrails, emphasizing the paramount importance of building trustworthy systems.

In essence, this survey adopts a multi-faceted taxonomy that categorizes research along the entire MLLM lifecycle: **Architecture** (the structural design), **Data** (the fuel for learning), **Training** (the process of learning), **Evaluation** (the measurement of success), **Capabilities** (the emergent functionalities), **Applications** (the real-world impact), and **Challenges** (the critical hurdles of robustness and safety). By structuring our review this way, we aim to provide a coherent and comprehensive resource that not only captures the current state of the art but also illuminates the path toward more capable, efficient, and aligned multimodal artificial intelligence.

## 2 Core Architectures and Multimodal Fusion

### 2.1 Vision Encoders and Resolution Scaling

### 2.2 Modality Alignment Modules and Projectors

### 2.3 Unified and Hybrid Architectures

### 2.4 Parameter-Efficient Fusion and Model Merging

### 2.5 Efficiency Optimizations for Deployment

## 3 Data Ecosystem and Curation Strategies

### 3.1 The Data Landscape: Pre-training and Instruction-Tuning Corpora

### 3.2 Data Curation and Quality Refinement

### 3.3 Synthetic Data Generation and Augmentation

### 3.4 Data Selection and Efficiency Strategies

### 3.5 Domain-Specific and Multimodal Data Engineering

## 4 Training Paradigms and Optimization

### 4.1 Pre-training Strategies and Alignment

The initial pre-training phase constitutes a foundational pillar in the development of Multimodal Large Language Models (MLLMs), serving as the critical stage where the model learns to associate visual signals with linguistic concepts. This phase is primarily characterized by training on massive-scale image-text pairs, which are typically harvested from the web. Datasets such as LAION and CC12M provide the sheer volume of data necessary to train the extensive parameter spaces of modern MLLMs. The fundamental objective of this stage is modality alignment: bridging the semantic gap between the continuous, pixel-based representation of images and the discrete, token-based representation of text. Unlike the subsequent instruction tuning phase, which focuses on complex reasoning and instruction following, pre-training is largely concerned with establishing a basic, shared latent space where visual and textual information can be understood in relation to one another [1].

The architecture employed during this phase typically involves a frozen vision encoder, such as a Vision Transformer (ViT) or a CLIP-based encoder, which processes raw image pixels into high-dimensional feature vectors. These visual features are then mapped into the embedding space of a frozen Large Language Model (LLM) via a lightweight projection module, often a simple linear layer or a more complex Q-Former. The training objective is usually a reconstruction or contrastive loss, where the model is tasked with predicting the text corresponding to an image or aligning the representations of matched image-text pairs. This process effectively "teaches" the LLM to "see" by translating visual patterns into a format the language model can process. As highlighted in the survey [1], this approach leverages the powerful reasoning capabilities of pre-trained LLMs, using them as a "brain" to perform multimodal tasks. The pre-training data, being noisy and vast, allows the model to learn broad visual concepts and basic associations, forming the bedrock of its multimodal capabilities.

However, relying solely on web-crawled, noisy image-text pairs presents significant challenges. The alignment in such datasets is often loose; the text may describe only a subset of the image content, be overly generic, or contain irrelevant information. This creates a "modality gap" where the visual and textual representations remain somewhat disjointed, hindering the model's ability to perform fine-grained understanding and complex reasoning. To address this, recent research has emphasized the role of synthetic data generation to augment or refine the pre-training corpus. A prominent example is the use of advanced proprietary models like GPT-4V to generate high-quality, detailed descriptions for images, a method explored in works like Genixer. By generating synthetic data, researchers can create image-text pairs that are more descriptive, accurate, and structured than their web-crawled counterparts. This synthetic data acts as a bridge, providing a cleaner signal for modality alignment and helping to close the semantic gap before the model moves on to the more data-intensive instruction tuning phase.

Furthermore, the composition and strategy of pre-training data have been shown to be crucial. The survey [3] provides a comprehensive analysis, demonstrating that a careful mix of different data types is essential for achieving state-of-the-art performance. Specifically, they found that combining image-caption pairs (for basic alignment), interleaved image-text data (for multi-image reasoning and context), and text-only data (to preserve the LLM's original capabilities) is a winning recipe. This finding suggests that pre-training is not merely about teaching the model to associate a single image with a single sentence, but about enabling it to understand sequences of multimodal information. The insights from [33] underscore that the design of the pre-training data mixture is as important as the model architecture itself. This approach helps the model develop emergent properties such as in-context learning and few-shot reasoning capabilities, which are hallmarks of powerful MLLMs.

The pre-training stage also involves critical decisions regarding the vision encoder and the resolution of input images. Higher resolutions allow the model to capture finer details, which is essential for tasks like Optical Character Recognition (OCR) or identifying small objects. However, increasing resolution also increases the number of visual tokens, which can strain computational resources and complicate the alignment process. The work on [3] also investigated the impact of image resolution and the number of visual tokens, finding that these factors have a substantial impact on performance, while the specific design of the vision-language connector is comparatively less critical. This highlights a key trade-off in pre-training: balancing the richness of visual information with the computational efficiency required to process it. The choice of vision encoder, such as the widely used CLIP-ViT, is also paramount as it determines the quality and nature of the visual features that are fed into the projection module and, ultimately, the LLM.

In conclusion, the pre-training and alignment phase is a sophisticated process of distilling vast, noisy visual-world knowledge into a coherent, shared representation space with language. It moves beyond simple association to establish a robust foundation for complex multimodal reasoning. The strategic use of large-scale datasets like LAION, the careful curation of data mixtures as advocated in [3], and the innovative application of synthetic data generation techniques are all integral to this success. By effectively aligning modalities at this early stage, MLLMs are equipped with the fundamental perceptual capabilities necessary to tackle the diverse and challenging tasks presented in the subsequent instruction tuning and evaluation phases. This foundational work is indispensable for transforming a standard LLM into a truly multimodal generalist.

### 4.2 Visual Instruction Tuning and Dataset Construction

### 4.2 Visual Instruction Tuning and Dataset Construction

Visual Instruction Tuning (VIT) represents a pivotal phase in the development of Multimodal Large Language Models (MLLMs), marking the transition from the foundational modality alignment achieved during pre-training to sophisticated, general-purpose conversational abilities. While the pre-training stage, as discussed in the previous section, equips MLLMs with the ability to associate visual signals with linguistic concepts using large-scale, noisy image-text pairs, VIT serves as the crucial fine-tuning step that refines this foundation. This process transforms the model into a capable assistant that can follow complex human instructions, engage in multi-turn dialogues, and perform a wide array of reasoning tasks. The efficacy of VIT is fundamentally dependent on the quality, diversity, and complexity of the instruction-tuning datasets. This subsection details the methodologies for constructing these high-quality datasets, the prominent datasets that serve as benchmarks, and the critical importance of instruction diversity and complexity.

The paradigm of visual instruction tuning was popularized by models like LLaVA [1], which demonstrated that connecting a frozen vision encoder and a frozen Large Language Model (LLM) with a simple linear projection, followed by instruction tuning on a curated dataset, could yield impressive conversational capabilities. This approach highlighted that the LLM's reasoning and generation capabilities could be effectively transferred to the multimodal domain without extensive architectural modifications or retraining of the core language model. The process typically involves generating a high-quality dataset of image-instruction-response tuples, which is then used to train the projection layer and, in some cases, the LLM itself.

A foundational approach to dataset construction, exemplified by the creation of the LLaVA-Instruct dataset, involves a two-stage process leveraging the powerful reasoning capabilities of advanced LLMs (like GPT-4). First, the process begins with a large collection of images and associated captions. To bridge the gap between static images and dynamic conversation, synthetic conversations are generated. This is often done by prompting a text-only LLM to role-play as an AI assistant and generate diverse question-answer pairs based on the provided image caption and a set of predefined scenarios. For instance, the LLaVA team used GPT-4 to synthesize conversations by feeding it image captions and asking it to generate dialogues covering detailed descriptions, complex reasoning, and even humorous or creative interactions. This "LLM rewriting" strategy is highly effective because it distills the sophisticated reasoning and linguistic style of a powerful LLM into a format that a smaller, target MLLM can learn from. The generated instruction-response pairs are then paired with the original images to form the final training set. This method moves beyond simple captioning to create rich, multi-turn dialogues that teach the model not just what is in the image, but how to reason about it and discuss it naturally.

However, the quality of the initial seed data and the generation process are paramount. To address this and scale up data production, subsequent research has explored more automated and refined pipelines. For example, the MMEvol framework [34] proposes a sophisticated data evolution framework to systematically improve instruction quality. MMEvol iteratively refines an initial set of instructions by applying three core evolution strategies: fine-grained perception, cognitive reasoning, and interaction evolution. This process expands the diversity of instruction types, extends visual reasoning steps to enhance cognitive abilities, and thoroughly explores fine-grained information within images. By evolving a seed dataset like SEED-163K, MMEvol generates a more complex and diverse instruction dataset, leading to significant performance improvements in the resulting MLLM. This demonstrates a key trend: moving from static, human-curated datasets to dynamic, model-generated, and iteratively refined datasets is crucial for pushing the boundaries of MLLM capabilities.

The resulting high-quality datasets from these construction methods have become the cornerstones of modern MLLM training. A prominent example is the LLaVA-mix dataset, a composite of several high-quality instruction-tuning datasets, including LLaVA-Instruct, which is renowned for its conversational richness. Another significant contribution is the M^3IT (Multi-modal Instruction-instruction Tuning) dataset [1], which is a large-scale, multi-modal instruction dataset designed to cover a vast range of tasks. M^3IT is constructed by harmonizing over 30 existing datasets, transforming them into a unified instruction-response format. This curation process ensures broad coverage of capabilities, from basic perception to complex reasoning. Similarly, Vision-Flan [1] is a comprehensive vision-language instruction dataset that covers 1,600+ tasks with 400+ distinct instruction types, emphasizing task diversity to foster generalization. The construction of these datasets often involves rule-based transformations to convert task-specific annotations (e.g., bounding boxes for detection, masks for segmentation) into a natural language instruction-following format. For example, a detection dataset can be transformed into instructions like "Locate all the cats in the image," with the model learning to output bounding box coordinates in a text format.

Beyond sheer scale, the *diversity* and *complexity* of instructions are critical factors that determine the final performance and robustness of an MLLM. A model trained only on simple descriptive questions will struggle with abstract reasoning or creative tasks. This insight has led to the development of datasets and methodologies that explicitly target these aspects. The ComVint benchmark [1] was designed to evaluate the complexity and diversity of instruction tuning datasets, highlighting that many existing datasets lack sufficient variety. To address this, researchers have curated datasets that mix different types of instructions, including:

1.  **Descriptive Tasks:** Simple image captioning and object identification.
2.  **Reasoning Tasks:** Questions that require logical deduction, inference, or mathematical calculation based on visual information.
3.  **Creative Tasks:** Generating stories, poems, or code based on an image.
4.  **Instruction Following:** Complex, multi-step commands that require the model to manipulate or focus on specific parts of the image.

The importance of this diversity cannot be overstated. A model exposed to a wide spectrum of tasks during tuning develops a more generalized understanding of the relationship between vision and language, making it more adaptable to unseen scenarios. For instance, training on both visual question answering (VQA) and visual grounding tasks helps the model learn to associate linguistic concepts with precise spatial regions, improving its ability to answer questions about object locations and relationships.

Furthermore, the construction of instruction datasets is increasingly becoming a data-centric engineering challenge. The "Synergy between Data and Multi-Modal Large Language Models" [10] emphasizes the co-evolution of models and data. In this paradigm, powerful MLLMs are used to generate, filter, and refine training data, which in turn is used to train even better MLLMs. This virtuous cycle allows for the creation of datasets that are not only large but also highly tailored to the model's weaknesses. For example, if an MLLM exhibits a tendency for hallucination, a dataset can be specifically constructed to include challenging examples that penalize such behavior. The use of MLLMs for data curation also extends to filtering out low-quality or noisy samples, a critical step given that the initial pre-training data is often scraped from the web and contains significant noise.

In conclusion, visual instruction tuning is the defining step that transforms a model capable of simple association into a powerful, general-purpose multimodal assistant. The success of this process hinges entirely on the quality of the instruction datasets. The field has evolved from manually curated, small-scale datasets to massive, automatically generated, and systematically refined datasets like LLaVA-mix, M^3IT, and those produced by frameworks like MMEvol. The focus has shifted from merely scaling up data volume to strategically enhancing instruction diversity and complexity, as championed by research like ComVint. This data-centric approach, often involving a synergistic loop where MLLMs help create better training data for themselves, is the engine driving the rapid advancement of MLLM capabilities, pushing them closer to true, generalist intelligence. This refined capability is a prerequisite for the subsequent application of Parameter-Efficient Fine-Tuning (PEFT) techniques, which allow for the efficient adaptation of these powerful models to specialized downstream tasks.

### 4.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques

### 4.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques

The rapid evolution of Multimodal Large Language Models (MLLMs) has brought forth a significant challenge: the immense computational cost associated with full fine-tuning. As MLLMs grow in size and complexity, adapting them to specific downstream tasks or domains by updating all parameters becomes prohibitively expensive in terms of memory, storage, and training time. This has catalyzed the rise of Parameter-Efficient Fine-Tuning (PEFT) techniques, which aim to adapt large pre-trained models by modifying only a small fraction of their parameters. In the context of MLLMs, PEFT methods are not merely a convenience but a necessity for democratizing research and enabling practical deployment. This subsection analyzes prominent PEFT strategies, including Low-Rank Adaptation (LoRA), its quantized variant QLoRA, and advanced Mixture of LoRA Experts (MoLE/MoCLE) frameworks, highlighting their roles in mitigating data conflicts, reducing computational overhead, and facilitating efficient domain adaptation.

Low-Rank Adaptation (LoRA) has emerged as a cornerstone of the PEFT landscape. The core principle of LoRA is to freeze the pre-trained model weights and inject trainable low-rank matrices into the transformer layers, specifically within the attention mechanisms. Instead of updating the full weight matrix \( W \), LoRA represents the update \( \Delta W \) as a product of two low-rank matrices, \( \Delta W = BA \), where \( B \) and \( A \) have significantly smaller dimensions. During training, only \( B \) and \( A \) are updated, while the original weights remain frozen. This approach drastically reduces the number of trainable parameters. For instance, in a typical MLLM like LLaVA, which consists of a vision encoder, a projection layer, and a large language model backbone, applying LoRA to the LLM component allows for task-specific adaptation without altering the foundational knowledge encoded in the billions of parameters of the base model. The primary advantage of LoRA is its ability to reduce GPU memory consumption and accelerate training, making it feasible to fine-tune large models on consumer-grade hardware. Furthermore, since the base model is untouched, LoRA avoids catastrophic forgetting, ensuring that the model retains its general-purpose capabilities learned during pre-training. This is particularly crucial for MLLMs, which are expected to perform a wide array of tasks. The modularity of LoRA also allows for easy switching between different adaptations by simply loading different low-rank weight sets, which is highly efficient for serving multiple specialized models from a single base checkpoint.

Building upon the foundation of LoRA, QLoRA (Quantized LoRA) pushes the boundaries of parameter efficiency even further by incorporating quantization. QLoRA introduces a novel approach where the base model weights are quantized to a lower precision (e.g., 4-bit NormalFloat) and then frozen, while LoRA adapters are trained in half-precision (FP16 or BF16). This technique, as explored in the broader LLM literature, enables fine-tuning of massive models with significantly lower memory requirements. For MLLMs, QLoRA is a game-changer. It allows researchers to fine-tune models with tens or hundreds of billions of parameters on a single GPU, a task that was previously unimaginable. This democratizes access to state-of-the-art MLLM fine-tuning, enabling smaller labs and individual researchers to experiment with and adapt these powerful models. The reduction in memory footprint also translates to lower costs and energy consumption, making the development and deployment of specialized MLLMs more sustainable. While QLoRA's primary contribution is in reducing the memory footprint of the base model, it works in synergy with LoRA's parameter-efficient update mechanism to create a highly efficient fine-tuning pipeline.

However, a significant challenge arises when fine-tuning MLLMs on datasets comprising mixed domains or tasks. This is known as the "data conflict" or "negative transfer" problem. When a model is trained on a diverse mixture of instruction data (e.g., medical imaging, artistic captioning, and scientific diagram interpretation), the gradients from different tasks can interfere with each other, leading to a performance drop on specific domains compared to training on a specialized dataset. To address this, researchers have developed Mixture of LoRA Experts (MoLE/MoCLE) frameworks. These methods draw inspiration from the Mixture of Experts (MoE) architecture. Instead of a single, monolithic LoRA adapter, MoLE employs a set of LoRA experts. Each expert is a distinct LoRA module potentially specialized for a different domain or task. A routing mechanism, often a lightweight network, determines which expert (or combination of experts) should be activated for a given input. For a specific data sample, only the top-k most relevant experts are updated and their outputs are combined. This sparse activation allows the model to learn diverse skills without interference. For example, in [20], the authors demonstrate that by using a sparse mixture of LoRA experts for the MLP layers of the LLM, the model can effectively mitigate data conflicts when training on multiple distinct instruction datasets. The router learns to direct tokens from different domains to their corresponding experts, allowing for adaptive fine-tuning. This approach not only improves performance on mixed datasets but can even surpass a plain-LoRA baseline trained on twice the amount of data, showcasing its superior data efficiency.

The application of MoLE in MLLMs is particularly insightful because different modalities and tasks often require distinct reasoning capabilities. For instance, understanding fine-grained spatial relationships in a medical scan is a different skill from generating a creative story based on a landscape image. A single LoRA adapter might struggle to master both. By employing a mixture of experts, the model can dedicate specific parameter subsets to each skill set. The router's decision is typically based on the content of the input, allowing the model to dynamically switch its "mode of thinking." This is a step towards more dynamic and context-aware MLLMs. The computational cost of MoLE remains relatively low during inference, as only a subset of experts is activated for any given query, making it a scalable solution for multi-domain adaptation.

Beyond LoRA and its variants, other PEFT techniques have been explored for MLLMs. For example, some works investigate tuning only specific components like the projection layer that bridges the vision and language modalities. The paper [35] provides a fascinating perspective on this. Their findings suggest that even when only the projection layer is fine-tuned, the MLLM can acquire domain-specific visual capabilities, but these capabilities are modeled within the LLM itself, not by the projection layer extracting more relevant visual attributes. This implies that the projection layer's role is more about aligning the feature spaces, while the LLM's powerful reasoning engine is what truly adapts to new domains. This insight is crucial for PEFT, as it suggests that targeted tuning of even smaller components can yield significant performance gains, further reducing the number of parameters that need to be updated.

Another dimension of PEFT for MLLMs involves architectural adaptations. For instance, [36] proposes dynamic tuning of the projector and LLM parameters using HyperNetworks to generate adaptive parameter shifts. This allows the model to dynamically adjust its behavior based on the visual and language context, moving beyond static parameter sets. Similarly, frameworks like [37] introduce modality-specific adapter experts and attention gates to better decouple and handle mixed modalities. These methods represent a more sophisticated form of PEFT that goes beyond simply adding low-rank matrices, instead focusing on creating modular and adaptable architectures.

The benefits of PEFT for MLLMs are multifaceted. Firstly, it drastically reduces computational costs. Training a 7B parameter MLLM with full fine-tuning can require multiple high-end GPUs, whereas LoRA or QLoRA can often be performed on a single GPU. This makes experimentation and iteration much faster and cheaper. Secondly, it addresses the storage problem. Instead of saving a full copy of the model for each task (which could be 10s of GBs), one only needs to save the small adapter weights (often just tens of MBs). This is essential for deploying numerous specialized MLLM applications. Thirdly, as demonstrated by MoLE, PEFT techniques offer a principled way to handle multi-task and multi-domain learning, mitigating the negative interference that plagues naive fine-tuning approaches.

In conclusion, Parameter-Efficient Fine-Tuning techniques are indispensable for the practical advancement and application of Multimodal Large Language Models. Methods like LoRA and QLoRA have democratized access to fine-tuning by drastically reducing memory and computational requirements. More advanced frameworks like MoLE tackle the complex problem of data conflicts in multi-domain settings by employing sparse, specialized experts. As the field moves towards more capable and generalist MLLMs, PEFT will continue to be a critical area of research, enabling efficient adaptation, personalization, and deployment of these powerful models across a vast spectrum of real-world applications. The ongoing exploration of novel PEFT strategies, including hybrid approaches that combine different techniques, promises to further unlock the potential of MLLMs while keeping their operational costs in check.

---
### 4.4 Optimizing Training and Deployment

Optimizing the training and deployment of Multimodal Large Language Models (MLLMs) is a critical area of research, driven by the immense computational costs and memory footprints associated with these large-scale models. As MLLMs grow in size and complexity, making them more efficient for both training and inference becomes paramount for their practical application. This subsection addresses key optimization strategies that improve training efficiency and enable deployment on resource-constrained hardware, focusing on techniques such as quantization, pruning, pipeline optimization, and the use of specialized frameworks.

Quantization is one of the most prominent techniques for reducing the memory and computational requirements of MLLMs. It involves converting the model's weights and activations from high-precision formats (like 32-bit floating-point, FP32) to lower-precision formats (such as 8-bit or 4-bit integers, INT8/INT4). This reduction in precision significantly decreases the memory bandwidth needed to load weights and the computational complexity of arithmetic operations, leading to faster inference and lower energy consumption. For instance, post-training quantization (PTQ) can be applied to a pre-trained model with minimal performance degradation, while quantization-aware training (QAT) integrates the quantization process into the training loop to better preserve accuracy. The development of efficient quantization methods is crucial for deploying MLLMs on edge devices like smartphones, where memory and power are limited. The push towards making powerful models like GPT-4V-level MLLMs deployable on end-side devices is a significant trend, as highlighted by models like [31], which emphasizes efficient deployment on mobile phones. This work demonstrates that with careful architectural design and optimization, high-performance MLLMs can be made practical for offline and privacy-preserving scenarios. Similarly, the [17] project, while focusing on frontier-class performance, also acknowledges the importance of production-grade multimodality, which inherently involves considerations for efficient inference. The use of INT4 quantization, for example, can reduce model size by up to 75% compared to FP16, making it feasible to run these models on hardware that would otherwise be unable to accommodate them.

Another vital optimization strategy is pruning, which aims to reduce the model size by removing redundant or less important parameters. Pruning can be structured, where entire neurons or attention heads are removed, or unstructured, where individual weights are set to zero. Unstructured pruning often requires specialized hardware or libraries to realize its full speedup potential, while structured pruning is more hardware-friendly. The goal is to create a sparser, more compact model that retains most of the original model's performance. This is particularly relevant for MLLMs, where the vision encoder and the language model backbone can both be pruned to reduce the overall parameter count. For example, techniques that prune the vision encoder can reduce the computational burden of processing high-resolution images, a key challenge in MLLMs. While the provided papers do not explicitly detail a specific pruning method for MLLMs, the general principle of reducing model complexity for efficiency is a cornerstone of model deployment. The broader trend of making models smaller and more efficient, as discussed in [38], underscores the importance of such techniques. The survey points out that while large models offer superior capabilities, there is a growing need for smaller, more efficient models that can be deployed in practical settings. Pruning is a key technique in this "small model" paradigm, allowing for the creation of compact yet capable models.

Beyond individual model optimization, pipeline and system-level optimizations are essential for scaling MLLM training and inference. Frameworks like [39] (Scalable lightWeight Infrastructure for Fine-Tuning) are designed to streamline the fine-tuning infrastructure for large models. Such frameworks abstract away the complexities of distributed training, mixed-precision training, and memory management, allowing researchers to focus on model development. They provide optimized kernels and communication strategies that can significantly speed up training. For instance, SWIFT likely incorporates techniques like gradient checkpointing to reduce memory usage at the cost of a slight increase in computation, and efficient data loaders to prevent I/O bottlenecks. The importance of such infrastructure is highlighted in [40], which discusses the entire ecosystem of deploying and serving large models. This survey emphasizes that system-level enhancements are crucial for improving performance and efficiency without altering the core model architecture. It covers innovations in batching, caching, and hardware-software co-design that are directly applicable to MLLMs. For MLLMs, which often involve processing multiple modalities, efficient data pipelines that can handle image, text, and potentially audio or video data in a synchronized manner are critical.

A specific example of pipeline optimization is the work on [41], which focuses on optimizing the training pipeline for large-scale models. While the paper is not provided in the source list, the concept of pipeline optimization is a well-established field. In the context of MLLMs, this could involve optimizing the flow of data from the vision encoder to the language model, minimizing latency in the projection layer, and efficiently managing the attention mechanism across modalities. For example, some MLLMs generate a large number of visual tokens, which can become a bottleneck for the subsequent LLM layers. Optimizations like token pruning or compression, as seen in some advanced architectures, can be viewed as a form of pipeline optimization. The [17] paper introduces a 1-D tile-tagging design for dynamic high-resolution images, which is a form of data preprocessing and pipeline optimization that significantly boosts performance on OCR-related tasks by making the model more efficient at handling high-resolution inputs. This demonstrates that how data is prepared and fed into the model is as important as the model architecture itself for achieving efficiency.

Furthermore, the choice of training strategy itself is an optimization for efficiency. The common three-stage training pipeline (pre-training, alignment, and instruction tuning) is designed to be more efficient than training a model from scratch on instruction data. Pre-training on large-scale, noisy image-text pairs like LAION and CC12M allows the model to learn fundamental visual-linguistic alignments in a data-efficient manner. This is followed by a more computationally intensive but shorter instruction tuning phase on high-quality data. This staged approach is a form of meta-optimization for the training process. The [42] survey provides a comprehensive overview of this training pipeline, highlighting how it augments existing LLMs with multimodal capabilities in a cost-effective manner. By leveraging pre-trained unimodal models, researchers can avoid the prohibitive cost of training a full MLLM from scratch.

Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA (Low-Rank Adaptation) and its variants, are also central to optimizing training efficiency. Instead of fine-tuning all parameters of a massive MLLM, PEFT methods freeze the pre-trained weights and inject small, trainable adapters. This drastically reduces the number of trainable parameters, leading to faster training, lower memory consumption, and the ability to train multiple tasks without catastrophic forgetting. The [43] paper is a prime example of this. It demonstrates how to efficiently transform an LLM into a visual instruction follower by adding only a small number of learnable parameters (14M in their case) on top of a frozen LLM. This approach not only makes training faster but also allows for the creation of specialized MLLMs for different tasks by simply swapping or combining adapters. The joint training paradigm they propose, which optimizes disjoint groups of learnable parameters for image-text alignment and instruction following, is a sophisticated optimization strategy that prevents interference between tasks and achieves strong performance with limited data.

In conclusion, optimization and training efficiency for MLLMs is a multi-faceted challenge that spans from low-level numerical representation (quantization) and model structure (pruning) to high-level training paradigms (staged training, PEFT) and system-level infrastructure (frameworks like SWIFT). These techniques are often combined to achieve the best results. For instance, one might use a pre-trained, quantized MLLM backbone and then apply LoRA for efficient fine-tuning on a specific task, all within an optimized framework like SWIFT. The ongoing research in this area is crucial for democratizing access to powerful MLLMs, enabling their deployment in a wide range of applications from cloud-based services to on-device assistants, and ultimately pushing the field closer to the goal of efficient and scalable artificial general intelligence.

### 4.4 Optimization and Training Efficiency

### 4.4 Optimizing Training and Deployment

Optimizing the training and deployment of Multimodal Large Language Models (MLLMs) is a critical area of research, driven by the immense computational costs and memory footprints associated with these large-scale models. As MLLMs grow in size and complexity, making them more efficient for both training and inference becomes paramount for their practical application. This subsection addresses key optimization strategies that improve training efficiency and enable deployment on resource-constrained hardware, focusing on techniques such as quantization, pruning, pipeline optimization, and the use of specialized frameworks.

Quantization is one of the most prominent techniques for reducing the memory and computational requirements of MLLMs. It involves converting the model's weights and activations from high-precision formats (like 32-bit floating-point, FP32) to lower-precision formats (such as 8-bit or 4-bit integers, INT8/INT4). This reduction in precision significantly decreases the memory bandwidth needed to load weights and the computational complexity of arithmetic operations, leading to faster inference and lower energy consumption. For instance, post-training quantization (PTQ) can be applied to a pre-trained model with minimal performance degradation, while quantization-aware training (QAT) integrates the quantization process into the training loop to better preserve accuracy. The development of efficient quantization methods is crucial for deploying MLLMs on edge devices like smartphones, where memory and power are limited. The push towards making powerful models like GPT-4V-level MLLMs deployable on end-side devices is a significant trend, as highlighted by models like [31], which emphasizes efficient deployment on mobile phones. This work demonstrates that with careful architectural design and optimization, high-performance MLLMs can be made practical for offline and privacy-preserving scenarios. Similarly, the [17] project, while focusing on frontier-class performance, also acknowledges the importance of production-grade multimodality, which inherently involves considerations for efficient inference. The use of INT4 quantization, for example, can reduce model size by up to 75% compared to FP16, making it feasible to run these models on hardware that would otherwise be unable to accommodate them.

Another vital optimization strategy is pruning, which aims to reduce the model size by removing redundant or less important parameters. Pruning can be structured, where entire neurons or attention heads are removed, or unstructured, where individual weights are set to zero. Unstructured pruning often requires specialized hardware or libraries to realize its full speedup potential, while structured pruning is more hardware-friendly. The goal is to create a sparser, more compact model that retains most of the original model's performance. This is particularly relevant for MLLMs, where the vision encoder and the language model backbone can both be pruned to reduce the overall parameter count. For example, techniques that prune the vision encoder can reduce the computational burden of processing high-resolution images, a key challenge in MLLMs. While the provided papers do not explicitly detail a specific pruning method for MLLMs, the general principle of reducing model complexity for efficiency is a cornerstone of model deployment. The broader trend of making models smaller and more efficient, as discussed in [38], underscores the importance of such techniques. The survey points out that while large models offer superior capabilities, there is a growing need for smaller, more efficient models that can be deployed in practical settings. Pruning is a key technique in this "small model" paradigm, allowing for the creation of compact yet capable models.

Beyond individual model optimization, pipeline and system-level optimizations are essential for scaling MLLM training and inference. Frameworks like [39] (Scalable lightWeight Infrastructure for Fine-Tuning) are designed to streamline the fine-tuning infrastructure for large models. Such frameworks abstract away the complexities of distributed training, mixed-precision training, and memory management, allowing researchers to focus on model development. They provide optimized kernels and communication strategies that can significantly speed up training. For instance, SWIFT likely incorporates techniques like gradient checkpointing to reduce memory usage at the cost of a slight increase in computation, and efficient data loaders to prevent I/O bottlenecks. The importance of such infrastructure is highlighted in [40], which discusses the entire ecosystem of deploying and serving large models. This survey emphasizes that system-level enhancements are crucial for improving performance and efficiency without altering the core model architecture. It covers innovations in batching, caching, and hardware-software co-design that are directly applicable to MLLMs. For MLLMs, which often involve processing multiple modalities, efficient data pipelines that can handle image, text, and potentially audio or video data in a synchronized manner are critical. A specific example of pipeline optimization is the work on [41], which focuses on optimizing the training pipeline for large-scale models. In the context of MLLMs, this could involve optimizing the flow of data from the vision encoder to the language model, minimizing latency in the projection layer, and efficiently managing the attention mechanism across modalities. For example, some MLLMs generate a large number of visual tokens, which can become a bottleneck for the subsequent LLM layers. Optimizations like token pruning or compression, as seen in some advanced architectures, can be viewed as a form of pipeline optimization. The [17] paper introduces a 1-D tile-tagging design for dynamic high-resolution images, which is a form of data preprocessing and pipeline optimization that significantly boosts performance on OCR-related tasks by making the model more efficient at handling high-resolution inputs. This demonstrates that how data is prepared and fed into the model is as important as the model architecture itself for achieving efficiency.

Furthermore, the choice of training strategy itself is an optimization for efficiency. The common three-stage training pipeline (pre-training, alignment, and instruction tuning) is designed to be more efficient than training a model from scratch on instruction data. Pre-training on large-scale, noisy image-text pairs like LAION and CC12M allows the model to learn fundamental visual-linguistic alignments in a data-efficient manner. This is followed by a more computationally intensive but shorter instruction tuning phase on high-quality data. This staged approach is a form of meta-optimization for the training process. The [42] survey provides a comprehensive overview of this training pipeline, highlighting how it augments existing LLMs with multimodal capabilities in a cost-effective manner. By leveraging pre-trained unimodal models, researchers can avoid the prohibitive cost of training a full MLLM from scratch.

Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA (Low-Rank Adaptation) and its variants, are also central to optimizing training efficiency. Instead of fine-tuning all parameters of a massive MLLM, PEFT methods freeze the pre-trained weights and inject small, trainable adapters. This drastically reduces the number of trainable parameters, leading to faster training, lower memory consumption, and the ability to train multiple tasks without catastrophic forgetting. The [43] paper is a prime example of this. It demonstrates how to efficiently transform an LLM into a visual instruction follower by adding only a small number of learnable parameters (14M in their case) on top of a frozen LLM. This approach not only makes training faster but also allows for the creation of specialized MLLMs for different tasks by simply swapping or combining adapters. The joint training paradigm they propose, which optimizes disjoint groups of learnable parameters for image-text alignment and instruction following, is a sophisticated optimization strategy that prevents interference between tasks and achieves strong performance with limited data.

In conclusion, optimization and training efficiency for MLLMs is a multi-faceted challenge that spans from low-level numerical representation (quantization) and model structure (pruning) to high-level training paradigms (staged training, PEFT) and system-level infrastructure (frameworks like SWIFT). These techniques are not mutually exclusive; they are often combined to achieve the best results. For instance, one might use a pre-trained, quantized MLLM backbone and then apply LoRA for efficient fine-tuning on a specific task, all within an optimized framework like SWIFT. The ongoing research in this area is crucial for democratizing access to powerful MLLMs, enabling their deployment in a wide range of applications from cloud-based services to on-device assistants, and ultimately pushing the field closer to the goal of efficient and scalable artificial general intelligence.

### 4.5 Data Selection and Quality Refinement in Training

### 4.5 Data Selection and Quality Refinement in Training

The performance of Multimodal Large Language Models (MLLMs) is fundamentally constrained by the quality and composition of the training data. While the preceding pre-training phase relies on massive, noisy web-scale corpora to learn basic modality alignment, the subsequent instruction-tuning phase is critical for eliciting complex reasoning, conversational abilities, and adherence to user intent. However, constructing high-quality instruction datasets is a significant bottleneck, often requiring expensive human annotation or generation via proprietary models. Consequently, a major focus of recent research has been on developing automated strategies to select the most impactful data subsets and refine the quality of existing data, thereby maximizing training efficacy and computational efficiency. This data-centric approach complements the model-centric optimization techniques discussed in the previous section, such as quantization and PEFT, by ensuring that the training signal itself is as potent as possible.

#### Strategies for High-Quality Data Selection

The core principle behind data selection is that not all data points contribute equally to model performance. Training on a curated, high-value subset often yields better results than training on a larger, uncurated dataset. This has led to the development of various metrics and algorithms to score and filter data.

One of the simplest yet surprisingly effective heuristics is **selecting the longest responses**. The intuition is that longer, more detailed answers often contain richer information, more comprehensive reasoning steps, and better formatting, which can serve as a strong signal for the model to learn from. While this approach does not explicitly measure the correctness or relevance of the response, it acts as a proxy for complexity and detail, filtering out low-effort or trivial examples that might not contribute significantly to the model's capabilities.

More sophisticated approaches leverage the model's own training dynamics to estimate the value of each data point. **Gradient-based value estimation** has emerged as a powerful technique. A prominent example is **TIVE (Gradient-based Value Estimation for Data Selection)**, which estimates the importance of a training sample by analyzing the gradients it produces. The core idea is that data points that induce larger gradients are those that the model has not yet learned well, and thus provide more "learning signal." By prioritizing these high-loss or high-gradient samples, training can be made more efficient and effective. This method moves beyond simple heuristics to a more principled, model-centric view of data value. It addresses the problem of data redundancy and ensures that the model's capacity is focused on the most challenging and informative examples.

Another line of research focuses on **self-filtering** mechanisms, where the model itself helps to identify high-quality data. This can involve training a smaller "critic" model to score data quality or using the target model to filter out examples that lead to high perplexity or inconsistent gradients. These methods aim to create a virtuous cycle where the model's improving understanding of the data helps it to select better data for further improvement.

#### Refining Instruction Quality through Automated Revision

Beyond selecting existing data, a parallel research direction focuses on actively improving the quality of instruction-response pairs. Raw data, whether human-generated or model-generated, can often be ambiguous, poorly formatted, or lack the necessary detail for effective fine-tuning.

**CoachLM** represents a significant step forward in this area. It is a system designed for the **automated revision of instruction data**. Instead of simply filtering data, CoachLM actively rewrites responses to make them more helpful, accurate, and aligned with desired conversational styles. It can correct factual errors, improve clarity, expand on concise answers, and ensure the response directly addresses the user's instruction. By training on high-quality human revisions, CoachLM learns to perform these tasks automatically at scale. This approach is crucial for bridging the "quality gap" between raw, noisy data and the polished, high-fidelity examples required for training state-of-the-art MLLMs.

Similarly, **LIFT (Learning from Instruction Fine-tuning Data)** explores methods for refining the instruction-following capabilities of models. While CoachLM focuses on response revision, LIFT-like approaches often focus on the instruction itself, ensuring diversity, complexity, and clarity. The goal is to construct a dataset that covers a wide range of skills and instruction types, preventing the model from overfitting to a narrow distribution of prompts. For instance, the dataset might include questions requiring multi-step reasoning, creative generation, detailed descriptions, and complex comparisons. By refining the instructions, the model is forced to develop a more robust and generalizable understanding of user intent.

#### The Synergy of Selection and Refinement

These strategies are not mutually exclusive; in fact, they are often used in combination. A typical pipeline might involve generating a large set of instruction data using a powerful teacher model (like GPT-4V), applying a selection strategy like TIVE to identify the most promising examples, and then using a revision model like CoachLM to polish the selected data before it is used for fine-tuning.

This holistic approach to data curation is essential for addressing the unique challenges of multimodal training. The "modality gap" between visual and textual representations means that the model requires exceptionally clear and well-aligned data to learn effective cross-modal reasoning. By carefully selecting and refining data, researchers can create smaller, higher-quality datasets that punch far above their weight, enabling the training of capable MLLMs without the prohibitive cost of processing terabytes of raw data. This focus on data-centric AI is a key driver of progress, complementing advancements in model architecture and training algorithms.

## 5 Evaluation Benchmarks and Metrics

### 5.1 Overview of Evaluation Landscape

The rapid proliferation and remarkable capabilities of Multimodal Large Language Models (MLLMs) have fundamentally shifted the landscape of artificial intelligence, moving us closer to systems that can perceive, reason, and communicate across diverse sensory inputs. As models like GPT-4V, Gemini, and various open-source alternatives demonstrate emergent abilities in visual question answering, complex reasoning, and even creative generation, the imperative for rigorous, standardized, and comprehensive evaluation has become paramount. Unlike traditional unimodal models, where evaluation might focus on a single metric like accuracy or F1 score, MLLMs operate in a high-dimensional space of interconnected modalities, necessitating a multifaceted evaluation approach that captures not just perceptual acuity but also cognitive depth, safety, and trustworthiness. The current evaluation landscape is a sprawling ecosystem of benchmarks, metrics, and methodologies, each designed to probe specific facets of model performance. This subsection provides a high-level taxonomy of this landscape, distinguishing between the broad capabilities assessed by general-purpose suites and the deep, domain-specific challenges addressed by specialized benchmarks.

The necessity for a robust evaluation framework stems from the unique architecture and training paradigms of MLLMs. These models typically consist of a frozen vision encoder (e.g., CLIP or ViT), a modality alignment mechanism (e.g., a linear projector or Q-Former), and a powerful Large Language Model (LLM) backbone. The evaluation must therefore assess the efficacy of each component and their synergistic integration. A simple benchmark might test whether the model can correctly identify an object in an image, but a more sophisticated evaluation will determine if the model can reason about the object's relationship to its environment, predict its future state, or generate a coherent narrative based on its appearance and context. The evolution of evaluation has mirrored the evolution of the models themselves. Early evaluation focused heavily on foundational perception and understanding tasks, but as models have grown more capable, the benchmarks have shifted to assess higher-order cognitive functions, robustness against adversarial attacks, and the mitigation of hallucinations.

To navigate this complex terrain, we can broadly categorize existing evaluation efforts into two primary types: general-purpose evaluation suites and specialized evaluation benchmarks. This distinction is crucial as it reflects the dual goals of the field: to build versatile, generalist models capable of handling a wide array of tasks, and to push the boundaries of performance in specific, high-impact domains.

**General-Purpose Evaluation Suites**

General-purpose benchmarks are designed to provide a holistic, broad-strokes assessment of an MLLM's capabilities. They typically aggregate a diverse set of tasks spanning multiple domains and difficulty levels, aiming to measure the model's overall competence and generalization abilities. These suites are often the first point of reference when comparing new models against the state-of-the-art. For instance, benchmarks like MME [44] and SEED-Bench [29] are widely adopted for their comprehensive coverage. MME, for example, measures both perception and cognition abilities across 14 distinct subtasks, including object recognition, scene understanding, and text reading. Its design, which utilizes manually crafted instruction-answer pairs to avoid data leakage, provides a fair and standardized platform for comparing a large number of models. Similarly, SEED-Bench offers a large-scale evaluation across 19 tasks, covering both vision and language understanding, providing a granular view of model performance.

More recent efforts have pushed the boundaries of general-purpose evaluation by focusing on the integration of multiple capabilities. The ChEF framework [45] introduces a modular approach, structuring evaluation into four components: Scenario, Instruction, Inferencer, and Metric. This allows for flexible and standardized construction of evaluation "recipes" that can assess a wide range of desiderata, including calibration, in-context learning, instruction following, and robustness. By providing a unified framework, ChEF facilitates fair comparisons and enables the community to build upon a common foundation. Another notable development is the MM-Vet benchmark [29], which is designed to evaluate the integration of different capabilities, such as recognition, reasoning, and generation. It uses an LLM-as-a-Judge approach for scoring open-ended responses, a paradigm that is gaining traction for its ability to evaluate complex, multi-faceted answers that are difficult to score with traditional metrics. These general-purpose suites are essential for tracking the overall progress of the field and identifying areas where models are still lacking.

**Specialized Evaluation Benchmarks**

While general-purpose suites provide a valuable overview, specialized benchmarks are indispensable for deep-diving into specific capabilities and uncovering subtle model weaknesses. These benchmarks are often more challenging and are designed to test the limits of current models in areas that are critical for real-world applications.

*   **Complex Reasoning and Cognitive Abilities:** As models demonstrate basic perception, the frontier of evaluation has shifted to complex reasoning. Benchmarks like LogicVista [46] are specifically designed to assess integrated logical reasoning in visual contexts, covering tasks like deductive and abductive reasoning. Similarly, NPHardEval4V [47] provides a dynamic benchmark for pure reasoning abilities, converting textual NP-hard problems into image representations to disentangle reasoning from perception. The study on nonverbal abstract reasoning using Raven's Progressive Matrices [48] further highlights the significant performance gap, especially between open-source and proprietary models, in solving abstract, non-linguistic puzzles.

*   **Hallucination and Robustness:** A critical failure mode of MLLMs is hallucination, where models generate content inconsistent with the visual input. Benchmarks like AMBER and CorrelationQA are designed to quantify these issues. The pervasive nature of this problem is a key focus of current research [49]. Beyond hallucination, robustness against adversarial attacks is a major concern. Models can be easily misled by subtle perturbations to images or text, which can bypass safety alignments [50]. Benchmarks like MM-SafetyBench [2] and JailBreakV-28K are designed to test these vulnerabilities, probing the model's susceptibility to jailbreaking and the generation of harmful content.

*   **Safety and Trustworthiness:** As MLLMs are deployed in real-world applications, ensuring their safety and trustworthiness is non-negotiable. This involves evaluating their resistance to generating toxic, biased, or private information. MM-SafetyBench [2] and MLLMGuard are examples of benchmarks that focus on these safety dimensions. They assess a model's ability to handle malicious prompts, protect user privacy, and adhere to ethical guidelines, which is crucial for building public trust and ensuring responsible AI development.

*   **Multi-image and Temporal Understanding:** Moving beyond single-image inputs, a new class of benchmarks evaluates the ability to understand relationships across multiple images or over time. MIBench and MuirBench [29] focus on multi-image understanding, testing capabilities like image comparison and reasoning over image sequences. For video, benchmarks are emerging that assess temporal reasoning and event prediction [51], which is a step towards understanding dynamic, real-world environments.

*   **Domain-Specific Evaluation:** The application of MLLMs in specialized fields like medicine, finance, and robotics necessitates domain-specific benchmarks. For instance, the potential of MLLMs in bioimage analysis [52] requires benchmarks that test their ability to interpret complex biological structures and phenomena. Similarly, 3D point cloud benchmarks like 3DBench [29] and low-level vision benchmarks like Q-Bench [29] assess capabilities in non-standard modalities, pushing the model's understanding beyond the typical 2D image-text paradigm.

In conclusion, the evaluation landscape for MLLMs is a dynamic and multi-layered ecosystem. It has evolved from simple perception tasks to a sophisticated array of benchmarks that probe the cognitive, safety, and robustness dimensions of these powerful models. The distinction between general-purpose and specialized suites is critical: the former provides a necessary overview of progress, while the latter provides the granular insights needed to drive targeted improvements. As the field continues to advance towards more capable and generalist models, the development of even more challenging, dynamic, and human-aligned evaluation frameworks will be the key to guiding research and ensuring the responsible deployment of Multimodal Large Language Models.

### 5.2 Foundational Perception and Understanding Benchmarks

Foundational perception and understanding benchmarks serve as the bedrock for evaluating Multimodal Large Language Models (MLLMs), providing a standardized measure of a model's ability to process and interpret the visual world. These benchmarks are designed to test core capabilities that are prerequisites for more complex reasoning tasks. They typically assess fundamental skills such as object recognition, attribute identification, spatial localization, and the generation of descriptive text from visual inputs. The evolution of these benchmarks mirrors the progression of the models themselves, moving from simple classification tasks to more nuanced and open-ended evaluations that test the limits of machine perception. A comprehensive survey on the evaluation of MLLMs [2] highlights the critical need for these foundational benchmarks, categorizing them as essential for assessing general multimodal recognition and perception before moving on to higher-order cognitive functions.

The most prominent category within foundational benchmarks is Visual Question Answering (VQA), which requires a model to answer natural language questions about an image. This task inherently combines visual perception with language understanding. The VQAv2 dataset is a cornerstone in this domain. It was specifically designed to mitigate the language bias prevalent in earlier datasets, where models could often answer correctly by relying on question priors without truly understanding the image. VQAv2 features a vast collection of images from the COCO dataset, with each image paired with multiple questions and corresponding answers, encouraging the model to ground its responses in the visual content. The success of models on VQAv2 is often seen as a primary indicator of their basic visual reasoning prowess. The dataset's scale and diversity have made it a de facto standard for pre-training and fine-tuning evaluation. Similarly, the GQA dataset [53] extends the VQA paradigm by focusing on compositional reasoning and structured question answering. GQA questions are designed to require multi-step reasoning, object identification, and attribute and relation understanding, providing a more rigorous test of a model's ability to parse complex queries and synthesize information from different parts of an image.

Another fundamental benchmark is image captioning, which evaluates a model's ability to generate coherent and descriptive text for a given image. The COCO dataset [54] is the most influential benchmark for this task. Beyond its use in object detection and segmentation, COCO's extensive image captioning split has been instrumental in driving research in generative vision-language models. Models are evaluated on metrics such as BLEU, METEOR, CIDEr, and SPICE, which compare the machine-generated captions to human-authored references. However, as noted in research surrounding MLLM evaluation [55], these automatic metrics often fail to capture the nuances of human language, such as fluency, relevance, and creativity. This has led to a growing reliance on LLM-as-a-Judge paradigms and human evaluation for more accurate assessment, especially for the open-ended, free-form text generation capabilities of modern MLLMs [56].

Beyond VQA and captioning, foundational benchmarks also encompass tasks that test more specific perceptual skills. For instance, the Flickr30K dataset is widely used for sentence retrieval and image retrieval tasks, assessing the model's ability to align visual and textual representations in a shared embedding space. The ImageNet dataset, a classic in computer vision, is often used in a zero-shot or few-shot setting to evaluate the generalization capabilities of vision-language models [7]. Models like CLIP have demonstrated remarkable zero-shot performance on ImageNet by learning a robust alignment between images and text descriptions, a capability that forms the backbone of many modern MLLMs. The evaluation on such datasets provides a clear metric for the quality of the vision encoder and the modality alignment module before the LLM's reasoning capabilities are even tested.

The evaluation of foundational perception and understanding is not without its challenges. A significant concern is the risk of benchmark contamination, where models may have been inadvertently trained on the test data, leading to inflated performance scores. This issue is particularly acute for large-scale models trained on web-crawled data. Furthermore, the static nature of these benchmarks can lead to overfitting, where models excel at specific dataset patterns but fail to generalize to real-world scenarios. The limitations of traditional metrics for generative tasks have also been widely discussed, pushing the field towards more robust, model-based evaluation methods [2]. The shift towards using powerful LLMs like GPT-4 to act as judges for open-ended responses [55] represents a significant methodological evolution, aiming to align automated evaluation more closely with human judgment.

In summary, foundational perception and understanding benchmarks like VQAv2, COCO, and GQA are indispensable tools for the MLLM community. They provide a necessary, albeit not sufficient, measure of a model's core competencies in seeing and describing the world. While they have been crucial for driving progress, their limitations highlight the need for a more diverse and dynamic evaluation ecosystem, one that includes complex reasoning, robustness, and safety assessments, as will be discussed in subsequent sections of this survey. The continued evolution of these benchmarks, alongside the models they are designed to test, will remain central to the pursuit of more capable and reliable multimodal AI systems.

### 5.3 Complex Reasoning and Cognitive Capabilities

### 5.3 Complex Reasoning and Cognitive Capabilities

As Multimodal Large Language Models (MLLMs) progress beyond foundational perception tasks, their evaluation increasingly focuses on complex reasoning and higher-order cognitive functions. These capabilities are essential for Artificial General Intelligence (AGI) and distinguish mere pattern recognizers from systems capable of understanding, logic, and inference. This subsection surveys the specialized benchmarks designed to probe these advanced abilities, which move beyond simple Visual Question Answering (VQA) to test logical deduction, mathematical problem-solving, and spatial understanding.

#### Logical and Abstract Reasoning

A primary indicator of cognitive capability is the ability to perform logical reasoning, particularly with abstract patterns and non-verbal cues. Benchmarks in this domain often draw from psychological assessments of human intelligence, such as Raven's Progressive Matrices (RPM), which require identifying underlying rules in visual patterns. Modern benchmarks often integrate these with linguistic instructions, testing the model's ability to perceive visual patterns accurately and apply abstract logical rules to predict outcomes. This challenges the model to generalize beyond its training data and handle novel logical structures.

To provide a comprehensive assessment, benchmarks like LogicVista evaluate logical reasoning within a multimodal context, encompassing tasks that require integrating visual information with logical deduction, such as syllogisms involving visual premises [29]. Similarly, InfiMM-Eval assesses a range of cognitive abilities, including deductive and abductive reasoning [57]. Results from these benchmarks often reveal a significant performance gap between open-source models and proprietary frontier models like GPT-4V, suggesting that while perception has improved, the ability to construct and follow complex logical chains involving visual evidence is still nascent. A core challenge is the model's ability to maintain a coherent "mental model" of the situation and manipulate it according to logical rules. Furthermore, the abstract nature of these tasks exposes the limitations of current architectures, which may excel at memorizing patterns but struggle with genuine reasoning. This is compounded by the risk of data contamination, necessitating benchmarks with novel, unseen problem formats to ensure a true measure of cognitive ability.

#### Mathematical and Quantitative Reasoning

Mathematical problem-solving is another critical domain for evaluating the cognitive prowess of MLLMs, extending beyond simple arithmetic to include multi-step reasoning and the interpretation of visual data like graphs, charts, and geometric figures. The ability to "see" a mathematical problem and solve it is a hallmark of human intelligence and a crucial step towards building AI assistants for scientific and technical fields.

MathVista stands out as a pioneering benchmark, aggregating tasks that require mathematical reasoning within visual contexts, such as geometric reasoning and statistical data analysis from charts [29]. A typical MathVista problem might ask a model to calculate the rate of change from a graph or find the area of a region in a geometric figure. Solving these problems demands a tight integration of visual perception, language understanding, and symbolic reasoning. Performance on MathVista highlights both progress and persistent challenges. While models show an emerging ability to handle simple mathematical queries, their performance degrades significantly as reasoning complexity increases. Common failures include misinterpreting visual elements, formulating incorrect mathematical expressions, or making arithmetic errors, indicating that MLLMs lack a robust, grounded understanding of mathematical concepts. Research into incorporating dedicated reasoning modules or leveraging external tools like code interpreters is an active area aimed at overcoming these limitations.

#### Spatial and Relational Understanding

A fundamental cognitive capability is understanding the spatial world, which for MLLMs involves perceiving 3D structure, understanding object relationships (e.g., "to the left of," "behind"), and reasoning about affordances and physical plausibility. This is crucial for applications in robotics and autonomous navigation. Benchmarks for spatial understanding often involve tasks that require inferring properties not explicitly stated but implied by the visual arrangement, such as determining if an object is supported by another. More advanced tasks involve spatial imagination, like predicting a scene from a different viewpoint.

Recent research has also focused on 3D spatial reasoning with benchmarks like M3DBench, which evaluates an MLLM's ability to understand and reason about 3D point clouds or multi-view images [58]. Questions range from simple part identification to complex relational queries like "Is the red block stable on top of the blue block?", requiring the model to build an internal 3D representation from 2D inputs. Performance on these 3D tasks is generally lower than on 2D tasks, indicating a wider modality gap for geometric data. The evaluation of relational understanding also extends to video, where temporal and spatial reasoning across frames is critical, representing a frontier for MLLM capabilities.

#### Integrated Cognitive Benchmarks

Recognizing that real-world intelligence requires the seamless integration of perception, reasoning, and knowledge, several holistic benchmarks have been designed to evaluate these capabilities in a combined manner. These benchmarks present problems that require the synthesis of multiple cognitive skills, providing a more realistic assessment of an MLLM's overall intelligence.

MME (Massive Multimodal Evaluation) is one such comprehensive benchmark, covering a wide range of perception and cognition tasks. Its cognitive subset presents problems that require going beyond simple recognition, such as interpreting a flowchart-like image to determine the next step in a process, which demands both visual parsing and logical deduction. Similarly, SEED-Bench evaluates models across 12 tasks with a strong emphasis on cognitive reasoning, including chart reasoning and video understanding, which inherently requires integrating information across time and space. MM-Vet specifically targets integrated capabilities by defining core abilities like recognition, knowledge, language, and reasoning, and constructing tasks that require their combination. For example, a question might require recognizing a landmark, recalling knowledge about it, and reasoning about its location relative to another object. MM-Vet often employs an LLM-as-a-Judge for evaluation, which is well-suited for assessing the nuanced reasoning processes behind open-ended responses.

The development of these integrated benchmarks reflects a growing consensus that true progress in MLLMs will be measured not by performance on isolated tasks, but by the ability to synthesize information and solve complex, multi-faceted problems. As MLLMs become more capable, these benchmarks will need to evolve to focus on areas where human-like reasoning, such as common sense and causal inference, is paramount. This evolution is critical for guiding the development of more robust and cognitively capable MLLMs, a necessary step before their reliability and trustworthiness can be assured in high-stakes applications, as explored in the subsequent discussion on hallucination and robustness.

### 5.4 Hallucination and Robustness Evaluation

The rapid advancement of Multimodal Large Language Models (MLLMs) has brought forth significant challenges regarding their reliability and trustworthiness. As these models are increasingly deployed in real-world applications, the need to rigorously evaluate their propensity for hallucination and their robustness against adversarial manipulations has become paramount. This subsection delves into the specific benchmarks and evaluation methodologies designed to probe these critical vulnerabilities, focusing on the assessment of object and factual hallucinations as well as the models' resilience to adversarial attacks and leading questions.

Hallucination in MLLMs refers to the phenomenon where a model generates outputs that are not grounded in the provided multimodal inputs, fabricating objects, attributes, or relationships that do not exist in the visual data. This issue is particularly pernicious in MLLMs due to the inherent "modality gap" between visual and linguistic representations, often leading the model to rely more heavily on its linguistic priors than on visual evidence. To systematically measure this, several benchmarks have been developed. One prominent example is AMBER, a benchmark specifically designed to quantify hallucinations across different dimensions. AMBER evaluates models on their ability to accurately identify objects, attributes, and relationships present in an image, providing a fine-grained analysis of where and why hallucinations occur. By creating controlled scenarios, it helps disentangle the model's visual understanding capabilities from its tendency to generate plausible-sounding but factually incorrect information. Similarly, CorrelationQA is designed to test for factual hallucinations by assessing whether a model can correctly identify causal or correlational relationships between events or objects in a visual scene. These benchmarks are crucial for moving beyond anecdotal evidence of hallucination and towards a more quantitative understanding of the problem. The pervasive nature of this issue is highlighted in the survey [1], which identifies hallucination as a key area of concern that distinguishes current MLLMs from truly reliable systems. Furthermore, empirical studies evaluating the capabilities of MLLMs often reveal that even state-of-the-art models struggle with maintaining factual consistency, a limitation that is a key focus of the analysis in [28].

Beyond internal inconsistencies, the reliability of MLLMs is also tested by their robustness against external manipulation. Adversarial robustness is a critical area of evaluation, examining how models perform when their inputs are subtly perturbed to induce erroneous behavior. The continuous nature of visual data presents a unique attack surface compared to discrete text tokens. Adversarial attacks on MLLMs can be multimodal, involving carefully crafted visual perturbations (e.g., adversarial patches or pixel-level noise) that are imperceptible to humans but can completely mislead a model's perception and reasoning. For instance, an adversarial sticker placed on a stop sign could cause an MLLM to misclassify it, or a subtly modified image could trigger a model to bypass its safety alignment and generate harmful content. These vulnerabilities are not limited to visual inputs; they can also be multimodal, where a benign-looking image paired with a carefully constructed text prompt can conspire to "jailbreak" the model's safety filters. The susceptibility of MLLMs to such attacks is a significant concern, as it demonstrates that their understanding of the world can be brittle and easily subverted. The analysis in [25] and similar comparative studies often touch upon the models' performance on standard benchmarks, but the true test of their robustness lies in their ability to withstand these targeted adversarial efforts.

In addition to adversarial attacks, a more subtle form of unreliability arises from leading questions and prompt sensitivity. MLLMs can be highly susceptible to the phrasing of a query, where a slight change in wording can lead to a drastically different, and often incorrect, response. This indicates a lack of deep, stable understanding and an over-reliance on statistical patterns in the training data. Benchmarks designed to test robustness often include sets of questions with varying phrasings, including leading questions that suggest a particular (often incorrect) answer. A robust model should be able to resist such suggestions and provide an answer based on its own visual analysis. The evaluation of these properties is crucial for understanding the "trustworthiness" of MLLMs, a key theme explored in [28]. This work emphasizes that trustworthiness is a multi-faceted property, encompassing not only factual accuracy but also consistency and resistance to manipulation.

To address these challenges, the research community has proposed a range of mitigation strategies and defense mechanisms. These can be broadly categorized into training-based and training-free approaches. Training-based methods involve fine-tuning models on datasets specifically designed to reduce hallucinations or improve adversarial robustness. This includes curating high-quality instruction-tuning data that emphasizes visual grounding and penalizes fabricated content. For example, data curation strategies that focus on complex, multi-step reasoning tasks can force the model to pay closer attention to visual details, thereby reducing its tendency to hallucinate. Training-free methods, on the other hand, aim to improve model behavior at inference time without modifying the model's weights. These can include techniques like adaptive shield prompting, where the system automatically adds instructions to the user's prompt to guide the model towards more grounded and cautious responses. Another approach involves using external tools or models, such as a separate vision model to verify the presence of objects mentioned by the MLLM, creating a form of multi-agent debate or verification system to detect and correct hallucinations before the final output is generated. The development of these defenses is an ongoing arms race against increasingly sophisticated failure modes, and their effectiveness is measured using the very benchmarks designed to expose these vulnerabilities.

In conclusion, the evaluation of hallucination and robustness is a cornerstone of assessing the true capabilities and limitations of MLLMs. Benchmarks like AMBER and CorrelationQA provide the necessary tools to quantify the prevalence of factual inconsistencies, while a broader set of evaluation protocols probes the models' resilience against adversarial attacks and leading questions. The insights gained from these evaluations are vital for guiding the development of more reliable, safe, and trustworthy MLLMs. As the field progresses towards more generalist and agentic systems, ensuring that these models can be trusted to provide accurate and consistent information, even in the face of challenging or malicious inputs, will be a critical determinant of their successful and responsible integration into society. The ongoing research into both understanding the root causes of these failures and developing effective mitigation strategies is therefore one of the most important frontiers in MLLM development.

### 5.5 Safety and Trustworthiness Assessment

### 5.5 Safety and Trustworthiness Assessment

The rapid proliferation of Multimodal Large Language Models (MLLMs) into high-stakes domains necessitates rigorous evaluation of their safety and trustworthiness. Unlike unimodal systems, MLLMs introduce a complex, expanded attack surface by integrating visual, auditory, and textual inputs, creating unique vulnerabilities that challenge traditional security paradigms. Assessing these models requires moving beyond standard performance metrics to probe for susceptibility to malicious manipulation, privacy violations, and the generation of harmful content. The evaluation landscape for MLLM safety is characterized by the development of specialized benchmarks designed to stress-test models against adversarial attacks, jailbreaking attempts, and inherent biases. This subsection details the critical benchmarks and methodologies used to evaluate these safety dimensions, highlighting the ongoing "arms race" between model developers and adversarial attackers.

A primary concern in MLLM safety is **vulnerability to adversarial attacks**, where imperceptible perturbations to input data can cause catastrophic failures. While adversarial examples have been studied extensively in traditional computer vision, their impact on MLLMs is more profound due to the semantic reasoning capabilities of the underlying LLM. Adversarial attacks can be crafted to bypass safety alignments, causing the model to generate harmful or prohibited content. The continuous nature of image data, as opposed to discrete text tokens, provides a vast and often unexplored search space for these attacks. Research has shown that adversarial examples can be transferred across models, making them a potent threat. Furthermore, the evaluation of robustness against such attacks is complicated by the fact that standard testing methodologies often fail to capture the model's behavior under duress. For instance, studies on coverage metrics for deep neural networks have revealed a limited correlation between structural coverage (e.g., neuron coverage) and robustness, suggesting that simply increasing internal neuron activation diversity does not guarantee a more robust model [32; 59]. This disconnect highlights the need for specialized robustness benchmarks that evaluate MLLMs against a diverse set of adversarial perturbations tailored to the multimodal context.

**Jailbreaking and safety alignment failures** represent another critical axis of evaluation. Jailbreaking involves crafting prompts that circumvent the safety guardrails fine-tuned into the model to prevent the generation of toxic, unethical, or dangerous content. In the multimodal context, attackers can leverage both text and images to create "competing objectives" or exploit "mismatched generalization," where the model's safety training on text does not generalize to visual inputs. For example, an image containing a seemingly innocuous visual riddle might, when combined with a specific text prompt, trick the model into generating harmful instructions. To systematically probe these vulnerabilities, benchmarks like **MM-SafetyBench** have been proposed. These benchmarks typically consist of a large collection of adversarial prompts and images designed to test specific safety failure modes. They evaluate a model's ability to refuse harmful requests that are disguised through multimodal obfuscation. The evaluation process often involves automated judging, where another LLM or a set of rules determines if the MLLM's response is compliant or harmful. The development of such benchmarks is crucial for quantifying the effectiveness of safety alignment techniques and for identifying the specific failure modes of different models. The challenge is compounded by the fact that safety alignment is often a moving target, with new jailbreaking techniques emerging rapidly, necessitating dynamic and continuously updated evaluation suites.

The pervasive issue of **hallucination** also falls under the umbrella of trustworthiness assessment. While not always a direct safety issue, hallucinations erode user trust and can lead to dangerous misinformation. MLLMs are prone to generating objects, attributes, or relationships that are not present in the visual input, a phenomenon often categorized into object, attribute, and relation hallucinations. Benchmarks designed to evaluate hallucination, such as **AMBER**, provide a quantitative measure of a model's tendency to fabricate information. These benchmarks typically involve tasks where the model must describe an image or answer a question about it, and the generated text is automatically compared against the ground truth visual content to identify inconsistencies. A trustworthy MLLM should demonstrate a strong grounding in the provided visual evidence and refrain from making up details. Evaluating hallucination is intrinsically linked to evaluating robustness, as models that are easily misled by adversarial inputs are also more likely to hallucinate. The underlying causes of hallucination, such as the modality gap between vision and language representations, are a key area of research, and evaluation benchmarks serve as the primary tool for measuring progress in mitigating this issue.

**Privacy leakage** is a more subtle but equally important safety dimension. MLLMs trained on vast, often scraped, internet-scale datasets may inadvertently memorize and regurgitate sensitive or personally identifiable information (PII) present in their training data. When prompted with specific visual or textual cues, a model might leak private information, posing a significant risk to privacy. Evaluating this risk is challenging, as it requires auditing the model's training data and developing test cases that trigger memorization. While dedicated benchmarks for privacy leakage in MLLMs are still an emerging area, the principles from unimodal LLM privacy research are being adapted. The evaluation often involves "canary" experiments, where specific, unique data points are inserted into the training set to test if the model can be prompted to reproduce them. The integration of visual inputs adds another layer of complexity, as a visual cue might be sufficient to trigger the recall of associated private text.

To address these multifaceted safety challenges, a holistic evaluation framework is required. **MLLMGuard** is an example of a comprehensive benchmark that aims to assess multiple safety dimensions, including toxicity, jailbreak resistance, and privacy, in a unified manner. Such frameworks are essential for providing a standardized way to compare the safety profiles of different MLLMs. They often employ a combination of automated metrics and human-in-the-loop evaluation to ensure a nuanced assessment. For instance, while automated judges can efficiently screen for obvious safety violations, human evaluators are often needed to assess the subtlety of jailbreak attempts or the potential for generating subtly biased or misleading content. The development of these comprehensive benchmarks reflects a growing recognition that safety is not a monolithic property but a collection of distinct capabilities that must be individually tested and verified.

In conclusion, the assessment of safety and trustworthiness in MLLMs is a dynamic and critical field. It requires the development of sophisticated benchmarks that can probe the unique vulnerabilities arising from multimodal integration. From adversarial robustness and jailbreaking to hallucination and privacy, each dimension presents distinct evaluation challenges. The works referenced in this section, such as MM-SafetyBench and the insights from robustness studies [60], underscore the complexity of the problem. As MLLMs become more capable and widely deployed, the continuous refinement of these evaluation methodologies will be paramount to ensuring they are developed and deployed responsibly. The ultimate goal is to move from a reactive posture—scrambling to patch vulnerabilities after they are discovered—to a proactive one, where safety and trustworthiness are built into the model architecture and training process from the outset, guided by rigorous and comprehensive evaluation.

### 5.6 Multi-image and Temporal Understanding

The evaluation of Multimodal Large Language Models (MLLMs) has traditionally focused on single-image understanding tasks, such as Visual Question Answering (VQA) and image captioning. However, to achieve Artificial General Intelligence (AGI), models must possess the capability to reason over sequences of images and videos, understanding temporal dynamics, causal relationships, and narrative progression. This subsection examines benchmarks designed to evaluate these advanced capabilities, moving beyond static scenes to assess multi-image reasoning and temporal understanding.

### 5.6.1 The Need for Multi-Image and Temporal Evaluation

Single-image benchmarks often saturate quickly, failing to distinguish between models that merely memorize visual patterns and those that genuinely comprehend complex scenarios. Real-world interactions frequently involve dynamic environments where the order of events matters, or static collections where context is derived from multiple viewpoints or related images. Consequently, new benchmarks have emerged to test the "state-tracking" and "narrative understanding" abilities of MLLMs. These benchmarks require models to perform tasks like identifying differences between images, summarizing video content, or answering questions that require integrating information across a temporal sequence.

The shift towards multi-image evaluation is also driven by the architectural evolution of MLLMs. Early models like [61] primarily handled single images, but recent advancements in handling long contexts allow models to process multiple frames or images simultaneously. However, simply increasing the context window is insufficient; the model must effectively attend to relevant information across this extended sequence. This necessitates benchmarks that specifically target cross-image reasoning, such as identifying the sequence of events in a comic strip or tracking object states in a video.

### 5.6.2 Benchmarks for Multi-Image Reasoning

Several benchmarks have been proposed to evaluate the ability of MLLMs to understand relationships across distinct but related images.

**MIBench and MuirBench:** Two prominent benchmarks in this domain are [62] and [63]. [62] is designed to evaluate multi-image understanding through a diverse set of tasks, including visual correspondence, difference captioning, and reasoning over multi-view images. It provides a comprehensive suite of challenges that test whether a model can identify subtle changes or synthesize information from different visual perspectives. For instance, a model might be presented with a "spot the difference" task or asked to describe the relationship between a diagram and a photograph.

Similarly, [63] focuses on multi-image understanding and reasoning. It comprises a collection of tasks that require models to reason over multiple images to answer complex questions. This benchmark is particularly challenging because it often requires implicit knowledge and logical deduction. For example, a question might involve comparing the weather in two different photos taken at different times of the day, requiring the model to infer the time of day based on lighting and shadows. The introduction of these benchmarks has revealed that while MLLMs perform well on single-image tasks, their performance often degrades significantly when required to integrate information across multiple images, highlighting a gap in current capabilities.

**Video-MME:** While strictly a video benchmark, [64] serves as a rigorous test for multi-image understanding due to its evaluation of long-form video content. It assesses the model's ability to retain and utilize information over hundreds of frames. The benchmark covers various domains and includes both short and long videos, testing the model's temporal perception. The results from [64] indicate that maintaining temporal consistency is a major hurdle for current MLLMs, as they tend to "forget" information from earlier frames or fail to track object identities over time.

### 5.6.3 Video Understanding Benchmarks

Video understanding is a natural extension of multi-image reasoning, introducing the dimension of time. Benchmarks in this area evaluate whether MLLMs can perceive motion, causality, and temporal order.

**ActivityNet-QA and MSVD-QA:** Traditional video QA benchmarks like [65] and [66] have been used to evaluate the temporal understanding of video LLMs. These datasets contain questions about specific video clips, requiring answers that depend on the temporal evolution of the scene. For example, a question might ask, "What does the person do after opening the door?" which requires the model to identify the sequence of actions. However, these older benchmarks are often limited in complexity and scale.

**Video-LLaVA and the Role of Resolution:** Recent works like [67] have unified video and image understanding within a single framework, but their evaluation relies heavily on benchmarks that test temporal perception. The challenge in video understanding is not just processing the data but doing so efficiently. High-resolution video processing is computationally expensive, leading to trade-offs between frame rate (temporal granularity) and spatial resolution. Benchmarks like [64] explicitly test these trade-offs by including videos of varying lengths and complexities.

**Temporal Perception in High-Resolution Models:** The connection between spatial resolution and temporal understanding is becoming increasingly apparent. Models designed for high-resolution images, such as [68] and [69], often employ strategies that could be adapted for video. For instance, [69] introduces a Naive Dynamic Resolution mechanism that processes images of varying resolutions into different numbers of tokens. This concept is crucial for video, where the amount of information per frame varies. A benchmark that evaluates "temporal resolution" would assess how well a model can perceive rapid changes versus slow evolution, a capability that [69]'s dynamic tokenization might support.

### 5.6.4 Challenges in Multi-Image and Temporal Evaluation

Evaluating multi-image and temporal capabilities presents unique challenges that differ from single-image assessment.

**Hallucination and Consistency:** One major issue is temporal hallucination, where models generate plausible but factually incorrect descriptions of events that never occurred in the sequence. For example, in a video of a person walking from left to right, a model might hallucinate that the person turned around. Benchmarks like [33] and [70] have focused on object hallucination in single images, but extending these to video is non-trivial. The evaluation must ensure that the model not only identifies objects correctly but also maintains their identity and state across frames.

**Computational Constraints:** As noted in [71], processing high-resolution images requires significant computational resources. This problem is exacerbated in video, where the number of frames multiplies the input size. Benchmarks must account for efficiency. A model that achieves high accuracy on [64] but requires 100x the inference time of a slightly less accurate model may not be practical. Therefore, evaluation metrics often include a trade-off between accuracy and efficiency (e.g., frames per second processed).

**Instruction Following in Dynamic Contexts:** [68] highlights the importance of handling arbitrary aspect ratios and resolutions. In a multi-image context, the inputs might not be uniform; a user might provide a collage of images with different sizes. Benchmarks like [62] likely include such variations to test the robustness of the model's visual encoder and projector. The ability of [72] to compress visual tokens while retaining details is relevant here, as reducing the token count per frame allows for processing more frames within the LLM's context limit, potentially improving temporal understanding.

### 5.6.5 Architectural Adaptations for Temporal Tasks

The design of MLLMs is evolving to better handle multi-image and video inputs, and these architectural changes are often evaluated using the benchmarks discussed.

**Mixture of Encoders and Resolution Scaling:** [73] explores the design space for MLLMs with a mixture of encoders, suggesting that combining different vision encoders (e.g., one for global context, one for detail) improves performance. For video, this could mean using one encoder for spatial details and another for temporal features. Similarly, [74] proposes Mixture-of-Resolution Adaptation (MRA), which could be adapted to process different frames at different resolutions—high resolution for key frames and low resolution for others.

**Unified Architectures:** [75] proposes a unified architecture for spatial-temporal understanding, capable of handling images, videos, and 3D scenes at arbitrary resolutions. This work directly addresses the need for models that can seamlessly switch between single-image and multi-image/video modes. Benchmarks evaluating [75] would need to test this flexibility, perhaps by interleaving image and video queries.

**Efficiency Optimizations:** [76] and [77] propose methods to reduce redundancy in visual tokens. In video, consecutive frames are highly redundant. Benchmarks that evaluate "long-context" video understanding implicitly test the effectiveness of these redundancy reduction techniques. If a model can answer a question about an event that happened 5 minutes ago in a video, it suggests that the model has effectively managed its context window and retained relevant tokens while discarding redundant ones.

### 5.6.6 Future Directions in Evaluation

As MLLMs become more capable, benchmarks for multi-image and temporal understanding must also evolve.

**Dynamic and Interactive Benchmarks:** Current benchmarks are largely static. Future benchmarks might involve interactive environments where the model can query specific frames or time ranges, similar to how humans might scrub through a video timeline. This would test the model's ability to localize information temporally.

**Causal Reasoning:** Beyond simple sequence understanding, future benchmarks should test causal reasoning. For example, "If the person had not stopped at the red light, what would have happened?" This requires a counterfactual understanding of the video content, which is a step closer to human-level reasoning.

**Integration with World Models:** As discussed in [78], future MLLMs may integrate world models to predict future states. Benchmarks for temporal understanding could evolve to test predictive capabilities, asking models to forecast the next frame or the likely outcome of a sequence of events.

In conclusion, the evaluation of multi-image and temporal understanding is a rapidly advancing field. Benchmarks like [62], [63], and [64] provide essential tools for assessing the limitations of current MLLMs. They reveal that while models are becoming proficient at single-image perception, true temporal and cross-image reasoning remains a significant challenge. The development of efficient architectures like [75] and [73], combined with rigorous evaluation, will be crucial in bridging this gap and achieving models that can truly understand the dynamic world.

### 5.7 Domain-Specific and Expert-Level Evaluation

While general-purpose benchmarks provide a broad assessment of Multimodal Large Language Model (MLLM) capabilities, they often fail to probe the depth of knowledge required for professional applications. As MLLMs are increasingly deployed in high-stakes environments, the need for rigorous, domain-specific evaluation has become paramount. These specialized benchmarks are designed to assess whether models can move beyond casual visual understanding to perform tasks that require expert-level reasoning, precise interpretation of domain-specific data, and adherence to professional standards. This subsection reviews benchmarks tailored for specialized domains such as medicine, 3D point clouds, and low-level vision, highlighting the unique challenges and evaluation paradigms they introduce.

**Medical and Scientific Domains**
The application of MLLMs in healthcare holds immense promise, from assisting in diagnostics to automating the analysis of medical reports. However, the high-stakes nature of medicine demands exceptional accuracy and reliability. General-purpose benchmarks are insufficient for evaluating these models, as they do not capture the nuances of medical imaging or the specific reasoning required for clinical decision-making. Consequently, a new class of benchmarks has emerged to rigorously assess MLLMs in the medical domain.

These benchmarks typically involve tasks such as visual question answering (VQA) on medical images (e.g., X-rays, CT scans, dermatology photos), interpretation of clinical reports, and multi-modal reasoning that combines patient data with visual evidence. For instance, a model might be presented with a chest X-ray and a patient's symptoms and asked to generate a differential diagnosis or identify specific pathologies. The evaluation metrics in these contexts go beyond simple accuracy, often incorporating measures of clinical relevance, factual correctness against established medical knowledge, and the absence of hallucinations that could lead to harmful outcomes. The development of such benchmarks is critical for building trust in AI systems for healthcare, as they provide a standardized way to measure progress and identify failure modes specific to the medical context. The need for such specialized evaluation is implicitly supported by the broader trend of applying MLLMs to diverse domains, as highlighted in surveys like [79], which discusses the expansion of LLMs into various fields, necessitating tailored evaluation strategies.

**3D Spatial and Geometric Reasoning**
A significant limitation of many early MLLMs is their confinement to 2D image understanding. To interact with the physical world, models must possess a grasp of 3D geometry and spatial relationships. This has spurred the creation of benchmarks that evaluate MLLMs on 3D point cloud data. A prominent example is **3DBench**, which assesses a model's ability to reason about 3D scenes. Tasks within such benchmarks include object localization in 3D space, spatial relationship reasoning (e.g., "What is to the left of the red chair?"), and counting objects in complex 3D environments. These tasks are fundamentally different from 2D VQA, as they require the model to infer a three-dimensional structure from point cloud data and reason about positions and orientations in a volumetric space. The evaluation of 3D capabilities is a crucial step towards developing MLLMs for applications in robotics, autonomous navigation, and augmented reality, where a deep understanding of the physical world is essential. The importance of this area is underscored by research into unified visual representations, such as [80], which aims to create generic modeling frameworks that can handle diverse vision tasks, potentially including 3D perception in the future.

**Low-Level Vision and Perceptual Fidelity**
Beyond high-level semantic understanding, a different class of vision tasks requires a focus on low-level attributes such as image quality, distortion, and aesthetic quality. These tasks are critical in fields like photography, image processing, and video compression. Standard MLLM benchmarks, which primarily test semantic content, are ill-suited for this purpose. In response, benchmarks like **Q-Bench** have been developed to evaluate MLLMs on low-level vision capabilities. Q-Bench includes tasks such as assessing image quality (e.g., predicting a mean opinion score), identifying specific distortions (e.g., blur, noise, compression artifacts), and evaluating aesthetic quality. These tasks demand a level of perceptual sensitivity that is distinct from the object recognition or scene understanding tested in other benchmarks. For example, while a standard MLLM can identify that an image contains a "cat," a low-level vision benchmark would assess whether the image is blurry, overexposed, or aesthetically pleasing. This line of research is vital for creating MLLMs that can serve as expert assistants in creative and technical fields. The challenge of equipping MLLMs with these capabilities is highlighted in works like [81], which explicitly notes that most MLLMs are "blind to low-level features" and proposes specialized frameworks to address this gap.

**Domain-Specific Data Engineering and Evaluation**
The creation of these expert-level benchmarks is a non-trivial task that often involves significant data engineering efforts. For specialized domains, high-quality, annotated data is scarce and expensive to produce. This has led to the development of novel data curation and generation strategies. For instance, in the 3D domain, the creation of 3DBench likely involved complex pipelines for data acquisition and annotation. Similarly, for low-level vision, datasets must be carefully constructed with human-annotated quality scores. The survey [82] implicitly covers these challenges, discussing the importance of high-quality, domain-specific data for effective model training and evaluation. The principles of data curation, such as ensuring diversity, quality, and relevance, are just as critical for building evaluation benchmarks as they are for training datasets. Furthermore, the evaluation methodologies themselves must be adapted. While standard metrics like accuracy or CIDEr might be used for some tasks, others may require specialized metrics or even human-in-the-loop evaluation to assess qualitative aspects like clinical safety or aesthetic appeal.

**Challenges and Future Directions**
The development of domain-specific and expert-level benchmarks presents several ongoing challenges. First, there is the risk of **data contamination**, where evaluation data inadvertently leaks into the training set, leading to inflated performance scores. As models are trained on ever-larger, often uncurated web-scale datasets, ensuring the purity of benchmark data becomes increasingly difficult. Second, these benchmarks are often static, which can lead to overfitting as models are specifically optimized for them. Future benchmarks may need to be more dynamic or adversarial in nature to provide a more robust and lasting measure of true expertise. Third, the evaluation of "expertise" itself is complex. It requires not just factual accuracy but also reasoning, justification, and an understanding of domain-specific constraints and ethics. The development of benchmarks that can capture these multifaceted aspects of expert performance is a key direction for future research. As MLLMs continue to evolve, becoming more capable and integrated into professional workflows, the sophistication of our evaluation methods must keep pace. The ultimate goal is to create benchmarks that can reliably answer the question: "Is this model truly an expert in this domain, and can it be trusted to perform critical tasks safely and effectively?"

### 5.8 Evaluation Methodologies and Metrics

The evaluation of Multimodal Large Language Models (MLLMs) is a multifaceted challenge that extends far beyond the scope of traditional unimodal benchmarks. As these models evolve to handle complex reasoning, instruction following, and safety-critical tasks, the methodologies and metrics used to assess them must also advance. This subsection analyzes the diverse landscape of evaluation methodologies, contrasting traditional automatic metrics with the emerging paradigms of LLM-as-a-Judge and human-in-the-loop evaluation, while also considering the nuances of task-specific scoring.

### Traditional Automatic Metrics: Foundations and Limitations

Historically, the evaluation of multimodal systems has relied on a suite of deterministic, automated metrics derived from computer vision and natural language processing. For tasks like image captioning, metrics such as CIDEr, BLEU, ROUGE, and SPICE have been the standard. These metrics function by measuring the n-gram overlap or semantic similarity between a model's generated caption and a set of human-written reference captions. Similarly, in tasks involving visual localization or object detection, metrics like Intersection over Union (IoU) and Average Precision (AP) provide a quantitative measure of spatial accuracy. These metrics are computationally inexpensive, reproducible, and offer a clear, objective signal for model comparison, making them indispensable for rapid iteration during model development.

However, these traditional metrics suffer from significant limitations when applied to the open-ended, generative nature of modern MLLMs. They often fail to capture the semantic richness and factual correctness of a response. For instance, a generated caption might be semantically perfect but use different wording than the reference, leading to a low CIDEr score. Conversely, a model might generate a factually incorrect statement that happens to share n-grams with the references, resulting in an artificially high score. This disconnect is particularly problematic for complex reasoning tasks, where the "correct" answer is not a fixed set of words but a logical deduction. The rise of MLLMs capable of generating long, multi-sentence explanations and engaging in dialogue renders these rigid metrics increasingly inadequate for a holistic assessment of model capabilities [1].

### The Rise of LLM-as-a-Judge: Scalable and Nuanced Evaluation

To address the shortcomings of traditional metrics, the community has increasingly turned to a paradigm known as "LLM-as-a-Judge." In this approach, a powerful, proprietary (e.g., GPT-4) or open-source LLM is used as an automated evaluator to score the outputs of an MLLM. The judge LLM is typically provided with the model's response, the input (image and/or text), and a set of evaluation criteria or a rubric. It then generates a score or a qualitative critique. This methodology offers several key advantages.

First, it enables the evaluation of complex, open-ended tasks that are difficult to quantify with n-gram overlap. The judge LLM can assess qualities like helpfulness, truthfulness, instruction following, and even the logical coherence of a multi-step reasoning process. For example, benchmarks like MM-Vet and MMT-Bench leverage this approach to evaluate the integration of different skills, moving beyond simple multiple-choice formats. Second, LLM-as-a-Judge can provide more granular and interpretable feedback. Instead of just a number, it can explain *why* a response was deemed poor, citing specific failures in reasoning or factual inaccuracies. This is invaluable for diagnosing model weaknesses and guiding future improvements. Third, it scales efficiently, allowing for the evaluation of thousands of responses without the need for expensive and time-consuming human annotation. This scalability is crucial as the number of models, tasks, and datasets continues to grow exponentially.

Despite its promise, the LLM-as-a-Judge paradigm is not without its own set of challenges. There are concerns about the potential for bias in the judge model, which may favor responses that mimic its own style or those generated by models from the same family. The cost of using high-quality judge models can also be substantial. Furthermore, the reliability of the judge itself must be validated, often against human preferences, creating a recursive evaluation problem. Nonetheless, this paradigm represents a significant step forward, enabling a more nuanced and scalable evaluation that is better aligned with the sophisticated capabilities of modern MLLMs.

### Human-in-the-Loop Evaluation: The Gold Standard for Alignment and Safety

While automated metrics and LLM judges provide efficiency, human evaluation remains the gold standard for assessing qualities that are deeply tied to human values, preferences, and subjective experiences. Human-in-the-loop evaluation involves human annotators or raters who directly assess model outputs based on a predefined set of criteria. This is particularly critical for evaluating safety, alignment, and the nuanced nature of conversational AI.

Human raters are uniquely qualified to identify subtle forms of bias, toxicity, or harmful content that automated systems might miss. They can also judge the "vibe" of a conversation—whether a model's response is empathetic, engaging, or appropriately cautious. For tasks like visual question answering, humans can verify factual claims made by the model against the image content with a level of common-sense reasoning that is currently beyond the capabilities of automated systems. Benchmarks focused on safety, such as MM-SafetyBench, often rely on human evaluation to confirm whether a model has been successfully jailbroken or has produced a genuinely harmful output.

However, human evaluation is notoriously expensive, slow, and can suffer from inter-annotator variability. To mitigate these issues, researchers often use human evaluation in a targeted manner, focusing on high-stakes scenarios or using it as a final validation step for promising models. Techniques like collecting Likert scale ratings or performing pairwise comparisons (e.g., "Model A's response is better than Model B's") help to structure the feedback and make it more quantifiable. Ultimately, human evaluation serves as the ultimate arbiter of model quality, especially for alignment and safety, and is often used to calibrate the performance of automated evaluation methods like LLM-as-a-Judge.

### Hybrid and Task-Specific Evaluation Paradigms

In practice, a comprehensive evaluation strategy for MLLMs employs a hybrid approach, leveraging the strengths of each methodology. For foundational perception tasks like object recognition or optical character recognition (OCR), traditional metrics like IoU and accuracy remain highly relevant and efficient. For complex reasoning and generation tasks, LLM-as-a-Judge provides a scalable way to assess performance across a broad range of capabilities. For safety-critical applications and for understanding user preference alignment, targeted human evaluation is indispensable.

Furthermore, the choice of evaluation methodology is often dictated by the specific task and benchmark. For instance, the "Levels of AGI" framework suggests different evaluation criteria for different levels of capability, moving from performance-based metrics for narrow AI to more behavioral and capability-based assessments for generalist models. The development of specialized benchmarks also drives innovation in evaluation. For example, benchmarks for multi-image understanding, such as MIBench, require evaluation methods that can assess a model's ability to reason over sequences or sets of images, a task for which simple per-image metrics are wholly insufficient. Similarly, benchmarks for hallucination, like AMBER, require fine-grained annotations to distinguish between different types of hallucinations (object, attribute, relation), pushing evaluation beyond simple accuracy scores.

In conclusion, the methodologies for evaluating MLLMs are rapidly evolving from a reliance on simple, automatic metrics to a more sophisticated, multi-layered ecosystem. This ecosystem integrates the efficiency of traditional metrics, the nuance and scalability of LLM-as-a-Judge, and the ultimate authority of human judgment. The future of MLLM evaluation lies in developing robust, reliable, and cost-effective frameworks that can holistically assess the full spectrum of model capabilities—from perceptual accuracy and reasoning prowess to safety, alignment, and helpfulness—thereby ensuring that the development of these powerful models remains on a path toward beneficial and trustworthy artificial intelligence.

### 5.9 Challenges and Future Directions in Evaluation

The evaluation of Multimodal Large Language Models (MLLMs) is undergoing a rapid evolution, yet it faces profound structural and methodological challenges that limit our ability to accurately gauge the true capabilities and risks of these systems. As MLLMs transition from narrow task-specific solvers to generalist agents, the benchmarks designed to assess them struggle to keep pace. Current evaluation landscapes are plagued by issues ranging from data contamination to a fundamental misalignment between automated metrics and human-centric reasoning. This subsection critically examines these limitations and outlines the future trajectory for creating more dynamic, holistic, and safety-aware evaluation frameworks.

### The Crisis of Contamination and Static Benchmarks

One of the most pressing challenges in MLLM evaluation is the pervasive issue of data contamination. As the pre-training corpora for state-of-the-art models grow to encompass nearly the entire public internet, the distinct line between training data and evaluation benchmarks blurs. Many popular benchmarks, such as those for Visual Question Answering (VQA) or image captioning, have been publicly available for years. Consequently, models may have "memorized" the correct answers or patterns associated with specific benchmark images and questions during pre-training. This leads to inflated performance scores that do not reflect genuine reasoning or generalization capabilities. The community lacks robust, contamination-resistant evaluation protocols that can verify if a model is truly understanding a novel visual scene or simply retrieving a memorized association. This problem is exacerbated by the trend of scaling laws, where larger models trained on more data inevitably encounter benchmark data, making it difficult to disentangle memorization from true learning.

Furthermore, static benchmarks suffer from the "saturation" effect. As models improve, they achieve near-perfect scores on existing datasets (e.g., VQAv2, COCO), rendering them ineffective for distinguishing between incremental improvements or identifying specific failure modes. To address this, the community has proposed dynamic and adversarial benchmarks. These frameworks, such as those probing robustness against adversarial attacks or jailbreaking attempts [83], generate new test cases on the fly or use LLMs to generate challenging variations of existing data. However, generating high-quality, diverse, and biased adversarial examples for multimodal inputs is significantly harder than for text alone, as it requires manipulating both pixel space and semantic space simultaneously.

### The Lack of Evaluation for Emergent Capabilities

A significant gap in current evaluation is the inability to effectively measure "emergent" capabilities—complex behaviors that arise only in sufficiently large models. Standard benchmarks focus on narrow tasks like object recognition or basic arithmetic, which fail to capture the holistic reasoning, planning, and tool-use abilities observed in frontier models like GPT-4V or Gemini. For instance, while a model might score highly on [33] or [84], it may still fail at complex multi-step visual reasoning or exhibit severe hallucinations when asked to describe intricate relationships in a scene.

This gap necessitates a shift from atomistic tasks to integrated, holistic evaluations. Benchmarks like [85] and [86] attempt to assess the intersection of perception, knowledge, and reasoning. However, they are still limited in scope. Future evaluation frameworks must move beyond static multiple-choice questions to assess open-ended generation, agentic behavior, and the ability to follow complex, multi-modal instructions. The evaluation of "System 2" reasoning—the capacity for slow, deliberate thought—is particularly underdeveloped. We need benchmarks that require models to generate visual chain-of-thought (CoT) rationales [87], verify their own outputs against visual evidence, and interact with external tools or environments.

### The Subjectivity of Evaluation and the "LLM-as-a-Judge" Paradigm

As MLLMs generate increasingly open-ended and creative content, traditional automatic metrics like CIDEr or BLEU become insufficient. These metrics rely on n-gram overlap with reference captions, penalizing valid semantic variations and failing to capture the nuance of visual understanding. This has led to the rise of the "LLM-as-a-Judge" paradigm, where a powerful text-only LLM (like GPT-4) is used to evaluate the quality of multimodal model outputs. While this approach offers scalability and flexibility, it introduces new challenges. LLM judges may possess biases, such as preferring longer responses or favoring the stylistic conventions of the model they are derived from. Furthermore, text-only judges struggle to verify visual facts, potentially rewarding hallucinations if they are phrased confidently.

To mitigate these issues, future directions involve developing specialized multimodal judges or hybrid evaluation systems that combine automated metrics with human-in-the-loop verification. Human evaluation remains the gold standard for assessing safety, bias, and subjective quality, but it is expensive and difficult to scale. Research into "weak-to-strong" supervision, where a weaker model helps supervise a stronger one, might extend to evaluation, using human preferences to train robust reward models that can judge complex multimodal outputs.

### Safety, Robustness, and the Modality Gap

Perhaps the most critical future direction is the rigorous evaluation of safety, robustness, and trustworthiness. MLLMs introduce unique attack surfaces compared to text-only models. The continuous nature of image data allows for subtle adversarial perturbations that are imperceptible to humans but can completely jailbreak a model's safety alignment [83]. Current safety benchmarks often focus on text-based jailbreaks or simple visual harmful content detection. They fail to assess complex scenarios where visual and textual inputs conspire to elicit harmful behavior (e.g., "competing objectives" or "mismatched generalization").

The "modality gap"—the discrepancy in representation spaces between images and text—also poses evaluation challenges. It is difficult to measure how well a model has aligned these modalities, leading to failures in fine-grained understanding and increased hallucination. Future evaluation must quantify this gap and assess robustness against modality drop-out or corruption. Benchmarks like [33] for hallucination and [88] for safety are steps in the right direction, but they must evolve to cover a wider spectrum of risks, including privacy leakage, copyright infringement, and social bias across diverse cultural contexts.

### Towards Dynamic and Agentic Evaluation Frameworks

Looking ahead, the future of MLLM evaluation lies in dynamic, interactive, and agentic frameworks. Instead of static datasets, we need evaluation environments where models can act and be judged on their cumulative performance. This includes:

1.  **Interactive Simulation Environments:** Evaluating MLLMs as agents in simulated worlds (e.g., embodied AI tasks) where they must perceive, reason, and act to achieve goals. This tests planning and grounding capabilities far more effectively than passive Q&A.
2.  **Continual Learning Evaluation:** Assessing how models adapt to new tasks without forgetting old ones, particularly relevant for parameter-efficient fine-tuning (PEFT) methods [89].
3.  **Multimodal RAG (Retrieval-Augmented Generation) Evaluation:** Moving beyond closed-book evaluation to assess how well models integrate external multimodal knowledge, crucial for reducing hallucinations in real-world applications.
4.  **Cognitive Science-Inspired Tests:** Designing benchmarks based on human cognitive development to test abstract reasoning, theory of mind, and causal inference in visual contexts.

In conclusion, while current benchmarks have been instrumental in driving progress, they are reaching their limits. The field must urgently address data contamination, develop metrics for emergent capabilities, and rigorously evaluate safety and robustness. The transition from static, narrow evaluation to dynamic, holistic, and human-aligned assessment frameworks is not merely a technical necessity but a prerequisite for deploying trustworthy and capable multimodal AI systems. As models become increasingly integrated into our daily lives, our methods for evaluating them must become equally sophisticated, ensuring that we measure not just what they can memorize, but how well they can understand, reason, and act safely in a complex multimodal world.

## 6 Advanced Capabilities and Reasoning

### 6.1 Multimodal Chain-of-Thought (M-CoT) and Visual Rationale Generation

The extension of Chain-of-Thought (CoT) reasoning to multimodal contexts represents a pivotal advancement in the capabilities of Multimodal Large Language Models (MLLMs). While standard CoT encourages Large Language Models (LLMs) to articulate intermediate reasoning steps in text, Multimodal Chain-of-Thought (M-CoT) requires models to integrate visual information into this reasoning process. This capability is essential for solving complex, compositional problems where visual cues are as critical as textual logic. M-CoT aims to bridge the gap between visual perception and cognitive reasoning, moving beyond simple visual captioning to a form of "visual reasoning" where the model explicitly articulates how it uses visual evidence to arrive at a conclusion. This aligns with the broader goal of advancing from perceptual tasks to complex logical and abstract reasoning.

The fundamental challenge in achieving effective M-CoT is the "modality gap"—the semantic disconnect between continuous visual features and discrete linguistic tokens. Early attempts to address this often relied on simply feeding image captions into a text-based CoT pipeline, but this discarded crucial spatial and relational information contained in the image. To address this, researchers have proposed architectures that explicitly integrate visual processing into the reasoning loop. A notable technique in this domain is the "Image-of-Thought" approach, which conceptualizes visual reasoning not just as input processing but as an iterative refinement of visual attention guided by the LLM’s internal monologue. By allowing the model to "look" at specific regions of an image corresponding to each step of its textual reasoning, the model can ground its logic in visual evidence. This is distinct from earlier methods that might process the image once and then rely solely on the resulting embedding for subsequent reasoning.

One of the seminal frameworks that formalized this integration is the "Cantor" architecture. Cantor distinguishes itself by treating visual reasoning as a process of iterative attention refinement. Instead of generating a static representation of the image, Cantor allows the LLM to query the visual encoder multiple times during the generation of a single response. As the model generates a chain of textual reasoning, it simultaneously generates "visual prompts" or attention maps that focus on relevant parts of the image for the next reasoning step. For example, if the model is asked to solve a visual puzzle, it might first attend to the overall layout, then focus on specific shapes, and finally zoom in on color details, all while generating the corresponding textual explanation. This dynamic interplay between visual attention and textual generation mimics the human cognitive process of scanning and analyzing visual scenes, significantly enhancing the model's ability to handle complex visual reasoning tasks [90].

However, training MLLMs to perform such sophisticated M-CoT reasoning requires high-quality datasets that provide annotations for intermediate reasoning steps, not just final answers. Standard vision-language datasets typically consist of image-text pairs where the text is a direct description or answer, lacking the "rationale" or "thought process" required for CoT training. To address this, researchers have curated specialized datasets like "Visual CoT." These datasets are constructed by annotating images with multi-step reasoning chains that explicitly link visual evidence to intermediate conclusions. For instance, in a Visual CoT dataset, a question about an image might be accompanied by a rationale such as: "First, I identify the red object in the center. Next, I observe that it is circular. Therefore, it is likely a ball." This provides the necessary supervision signal for the model to learn not just *what* to say, but *how* to reason visually.

The impact of M-CoT and visual rationale generation extends beyond simple question answering. It is foundational for tasks requiring compositional reasoning, where the answer depends on the interplay of multiple visual elements. For example, in the "NLVR" (Natural Language Vision Reasoning) challenge, models must determine if a sentence is true given a pair of images. This requires comparing visual attributes across images and applying logical operators, a task that is nearly impossible without a structured reasoning process. M-CoT provides the scaffolding for such tasks by forcing the model to break down the comparison into discrete, verifiable steps.

Furthermore, the ability to generate visual rationales significantly improves the interpretability and trustworthiness of MLLMs. By exposing the intermediate reasoning steps, users can diagnose where a model might have failed—whether it looked at the wrong part of the image, misidentified an object, or made a logical error. This transparency is crucial for deploying MLLMs in high-stakes domains like medical imaging or autonomous driving, where understanding the "why" behind a decision is as important as the decision itself. The "Fact" framework, for instance, emphasizes the generation of faithful and concise rationales to teach MLLMs. By using verifiable visual programming to generate executable code that guarantees faithfulness, Fact ensures that the generated rationales are tightly coupled with the visual input, reducing hallucinations and improving the logical consistency of the model's output [91].

In addition to improving reasoning accuracy, M-CoT techniques are being explored to enhance the generalization capabilities of MLLMs. By learning to generate intermediate steps, models can better handle out-of-distribution examples that require novel combinations of visual perception and logical inference. The "Plug-and-Play Grounding of Reasoning" (P2G) framework exemplifies this trend. P2G proposes a modular approach where an MLLM can dynamically invoke external tools (like OCR models or object detectors) to ground its reasoning. The MLLM generates a plan, calls the necessary tools to extract specific visual information, and then integrates this information into its reasoning chain. This agentic approach to M-CoT allows the model to solve problems that are too complex for a single end-to-end model, effectively outsourcing difficult sub-tasks to specialized experts [92].

The evolution of M-CoT is also driving innovation in evaluation benchmarks. Traditional metrics like accuracy on multiple-choice questions are insufficient for assessing the quality of reasoning chains. New benchmarks are emerging that evaluate the coherence, factuality, and visual grounding of the generated rationales. These benchmarks often require models to produce open-ended explanations, which are then scored using LLM-as-a-Judge or human evaluation. This shift towards evaluating the reasoning process itself reflects a broader trend in the field: the recognition that true intelligence is not just about getting the right answer, but about the ability to explain how one got there.

Despite these advances, significant challenges remain. One major issue is the computational overhead. Generating detailed visual rationales requires processing the image multiple times or maintaining a longer context window, which increases latency and cost. Techniques like "Visual Tokens Withdrawal" and efficient attention mechanisms are being explored to mitigate this, but balancing reasoning depth with efficiency is an ongoing trade-off. Another challenge is the "hallucination" of reasoning steps. An MLLM might generate a plausible-sounding chain of thought that is not actually supported by the visual evidence. This is particularly problematic in M-CoT because the entire premise of the method is to ground reasoning in visual facts. Addressing this requires robust training data and potentially adversarial training methods that penalize ungrounded reasoning.

Looking forward, the integration of M-CoT with "System 2" reasoning architectures represents a promising direction. While current M-CoT methods largely operate in a "System 1" (fast, intuitive) manner, future systems may incorporate slower, more deliberate planning. This could involve generating multiple potential reasoning paths, evaluating their visual consistency, and selecting the best one—a process that mimics deep human thought. The "Cognitive Architectures" discussed in the broader literature suggest that moving towards such System 2 capabilities is essential for achieving Artificial General Intelligence (AGI).

In conclusion, Multimodal Chain-of-Thought and Visual Rationale Generation are transforming MLLMs from passive pattern matchers into active reasoners. By explicitly linking visual perception to linguistic logic through intermediate steps, these techniques unlock the ability to solve complex, compositional problems that were previously out of reach. The development of specialized architectures like Cantor, high-quality datasets like Visual CoT, and frameworks for faithful rationale generation like Fact are laying the groundwork for a new generation of MLLMs that are not only more capable but also more transparent and trustworthy. As research continues to refine these methods, we can expect MLLMs to become increasingly proficient at navigating the intricate interplay of sight and logic that characterizes human intelligence.

### 6.2 Complex Logical and Abstract Reasoning

The exploration of complex logical and abstract reasoning represents a critical frontier in the evaluation of Multimodal Large Language Models (MLLMs), moving beyond basic visual perception to assess higher-order cognitive functions. While early MLLMs demonstrated proficiency in descriptive tasks like image captioning and simple Visual Question Answering (VQA), the community has increasingly turned its attention to their ability to perform deductive, abductive, and analogical reasoning. These capabilities are essential for achieving Artificial General Intelligence (AGI), as they require the model to synthesize information, infer relationships, and solve problems that lack explicit textual cues. The evaluation of these skills often relies on specialized benchmarks that challenge the model's capacity to integrate visual data with logical constraints [90].

Deductive reasoning, which involves deriving specific conclusions from general premises, is a primary focus of current research. In multimodal contexts, this often translates to tasks where visual information must be processed to satisfy a set of logical rules. Benchmarks such as LogicVista are designed to probe these capabilities by presenting models with visual scenes alongside logical puzzles or syllogisms that require the model to identify the correct outcome based on visual evidence. Similarly, abductive reasoning—inferring the most likely explanation for an observed event—is tested through scenarios where models must hypothesize causes based on visual effects. Analogical reasoning, the ability to identify relationships between different concepts or domains, is evaluated using visual puzzles that require mapping structural similarities between images. These tasks collectively assess the model's ability to handle non-verbal reasoning, a significant leap from simple recognition to genuine understanding.

A particularly challenging domain for MLLMs is the resolution of nonverbal abstract reasoning tasks, exemplified by Raven's Progressive Matrices (RPM). RPM tests require subjects to complete a matrix of abstract shapes by identifying the underlying rule governing their arrangement. For MLLMs, solving RPMs demands a high level of visual abstraction, pattern recognition, and logical inference without relying on linguistic crutches. Research indicates that while MLLMs show some promise in these areas, their performance is far from human-level proficiency. The models must parse the visual input to extract abstract attributes (e.g., shape, size, rotation, color) and then apply logical operations to predict the missing element. This task highlights the "modality gap" often discussed in MLLM literature, where the alignment between visual features and the language model's reasoning capabilities is insufficient for purely abstract tasks.

The performance gap between open-source and proprietary MLLMs is particularly pronounced in these complex reasoning domains. Proprietary models like GPT-4V and Gemini consistently outperform their open-source counterparts on benchmarks like InfiMM-Eval and LogicVista. This disparity can be attributed to several factors, including the scale of pre-training data, the quality of instruction tuning, and the architectural sophistication of the models. For instance, proprietary models have likely been exposed to a vast array of multimodal data that includes abstract patterns and logical puzzles, allowing them to develop more robust reasoning heuristics. In contrast, many open-source models rely on smaller, less diverse datasets and may lack the computational resources to train on such complex tasks effectively.

Furthermore, the architectural design of MLLMs plays a crucial role in their reasoning capabilities. Models that utilize advanced fusion mechanisms, such as cross-attention layers or unified Transformer architectures, tend to perform better on tasks requiring deep integration of visual and linguistic information. However, even these advanced architectures struggle with the high degree of abstraction required for tasks like RPM. The challenge lies in translating visual features into a format that the language model can manipulate logically. Some research suggests that models with larger language backbones and better vision encoders exhibit better abstract reasoning, but the correlation is not linear, indicating that data quality and training strategies are equally important.

The evaluation of complex reasoning also reveals the limitations of current benchmarks. While benchmarks like InfiMM-Eval and LogicVista provide valuable insights, they are often static and may suffer from data contamination, where models have inadvertently seen similar problems during training. To address this, researchers are developing dynamic benchmarks and adversarial evaluation methods that test the model's ability to generalize to novel reasoning scenarios. Additionally, there is a growing interest in evaluating the model's ability to generate intermediate reasoning steps, or "Chain-of-Thought" (CoT) explanations, in multimodal contexts. This allows for a more nuanced assessment of whether the model is genuinely reasoning or merely pattern-matching.

In conclusion, the ability to perform complex logical and abstract reasoning is a defining characteristic of advanced MLLMs. While significant progress has been made, the performance gap between open-source and proprietary models remains substantial, particularly in abstract domains like Raven's Progressive Matrices. Future research must focus on improving the alignment between visual perception and logical reasoning, developing more robust and contamination-resistant benchmarks, and exploring training strategies that explicitly teach models to perform deductive, abductive, and analogical reasoning. The path toward AGI requires MLLMs to not only see and read but to think abstractly and logically across modalities. This focus on cognitive depth provides a necessary foundation for the subsequent discussion on "Advanced Perception and Instruction Following," which explores how MLLMs can be steered to apply these reasoning skills with precision and fine-grained control.

### 6.3 Advanced Perception and Instruction Following

The evolution of Multimodal Large Language Models (MLLMs) has transcended basic visual question answering and image captioning, moving towards capabilities that demand precise visual understanding and the ability to follow complex, multi-faceted instructions. This subsection, "Advanced Perception and Instruction Following," explores the frontier where MLLMs demonstrate "steerability"—the capacity to be directed via diverse input modalities beyond plain text—and tackle tasks requiring spatial reasoning and fine-grained interaction.

A significant leap in this domain is the integration of visual prompting mechanisms. Unlike traditional text-only queries, visual prompts such as bounding boxes, segmentation masks, points, and scribbles allow users to specify regions of interest with pixel-level accuracy. This capability is crucial for tasks that require the model to reason about specific image parts rather than the entire scene. Research has demonstrated that MLLMs can effectively interpret these visual cues to generate localized descriptions, perform attribute recognition on selected objects, or execute complex reasoning based on the spatial relationships defined by the prompts. For instance, the ability to point to an object and ask "What is this?" or draw a box around a region and request a detailed description represents a fundamental shift towards more natural and intuitive human-AI interaction. This "steerability" is not merely about recognizing shapes; it involves a deep semantic understanding of the visual content within the specified boundaries. The paper **[93]** makes a notable contribution here by proposing a training-free method to inject visual referring capabilities into existing MLLMs. By optimizing visual tokens during inference to control the attention mechanisms between text prompts and visual regions, **[93]** enables detailed region description and reasoning without the prohibitive cost of retraining large models. This approach highlights the inherent potential within pre-trained MLLMs to adapt to visual inputs, suggesting that the cross-modal alignment learned during pre-training is robust enough to support such fine-grained control mechanisms.

Furthermore, the challenge of processing high-resolution images while maintaining the ability to follow instructions regarding fine details has been addressed through architectural innovations. Standard MLLMs often struggle with high-resolution inputs due to the quadratic complexity of vision encoders or the limitation on the number of visual tokens that can be processed by the LLM backbone. To overcome this, methods like **[18]** introduce novel cropping and enhancement modules. **[18]** utilizes a Dual-perspective Cropping Module (DCM) to process sub-images containing both local details and global context, followed by a Dual-perspective Enhancement Module (DEM) to fuse these features. This allows the model to effectively follow instructions that require understanding intricate details (e.g., reading small text within a sub-image) while retaining the global context necessary for coherent reasoning. Similarly, the concept of "TokenPacker" introduced in **[16]** addresses the efficiency of visual token generation. By employing a coarse-to-fine scheme, **[16]** condenses visual tokens significantly (by 75%~89%) without sacrificing performance on diverse benchmarks. This efficiency is vital for instruction following, as it allows the LLM to process more visual information within its context window, enabling it to handle complex instructions that might reference multiple objects or detailed scenes. The ability to compress visual information effectively means that MLLMs can be deployed in scenarios requiring real-time interaction, where following precise instructions quickly is paramount.

Beyond 2D image understanding, advanced perception in MLLMs is expanding into the 3D spatial domain. This requires models to comprehend the geometry and spatial relationships of objects in three-dimensional space, a capability essential for applications in robotics, autonomous navigation, and augmented reality. Benchmarks such as **[94]** are pivotal in evaluating and driving progress in this area. These benchmarks typically involve tasks like 3D visual question answering (e.g., "Is the red cube to the left of the blue sphere?") or spatial reasoning about object arrangements. Developing MLLMs that can accurately interpret 3D point clouds or multi-view images and reason about spatial configurations is a complex challenge that goes beyond 2D perception. It requires the model to build an internal representation of the 3D environment from limited visual inputs. The integration of such capabilities signifies a move towards "embodied" intelligence, where the model can perceive and reason about the physical world in a way that is actionable.

The concept of "steerability" is further reinforced by studies on the internal mechanics of MLLMs. The paper **[35]** provides an intriguing insight into how MLLMs acquire domain-specific visual capabilities. Through experiments on datasets from domains like dermatology and agriculture, the authors find that even when only the cross-modal projection module is fine-tuned, the model gains visual capabilities, but the relevant domain-specific attributes are actually modeled by the LLM itself. This suggests that the LLM plays a more central role in visual understanding than previously assumed, and that the projection module acts more as a bridge than a complex feature extractor. This understanding has implications for instruction following: it implies that fine-tuning strategies can be targeted more effectively. For example, to improve instruction following in a specific domain, one might focus on adapting the LLM's knowledge base or the instruction data, rather than solely overhauling the vision-language connector. This "reinterpretation of the role of cross-modal projections" **[35]** suggests that the reasoning engine is the key component for advanced perception and instruction adherence.

Moreover, the ability to follow complex instructions is closely tied to the model's capacity for reasoning and avoiding hallucinations. Advanced perception is not just about identifying objects but about understanding their attributes, relationships, and the context in which they appear, all in response to specific directives. The paper **[15]** emphasizes the importance of preserving local context from visual features for spatial understanding, which is crucial for following instructions that involve spatial prepositions or relative positioning. By designing a projector that is both flexible (in managing token numbers) and locality-enhanced, **[15]** demonstrates superior performance on benchmarks requiring fine-grained understanding. This architectural choice directly supports the model's ability to execute instructions that require pinpointing specific visual elements.

In summary, the frontier of advanced perception and instruction following in MLLMs is characterized by the integration of diverse prompting modalities, the handling of high-resolution and 3D visual data, and a deeper understanding of the model's internal mechanics. Innovations like **[93]** and **[16]** enable fine-grained control and efficiency, while benchmarks like **[94]** push the boundaries into spatial reasoning. Furthermore, insights from **[35]** guide us toward more effective fine-tuning strategies. Collectively, these advancements are transforming MLLMs from passive observers into active, steerable agents capable of executing complex, multi-modal commands with high precision.

### 6.4 Evaluation of Integrated Capabilities and Reasoning

The evaluation of Multimodal Large Language Models (MLLMs) has traditionally focused on isolated capabilities, such as object recognition in perception tasks or logical deduction in purely textual reasoning benchmarks. However, the true measure of advanced intelligence lies not in the mastery of individual skills, but in the seamless integration of these skills to solve complex, multi-step problems. As MLLMs evolve, the evaluation landscape is shifting towards benchmarks that demand the simultaneous application of visual perception and abstract reasoning. This subsection reviews prominent benchmarks designed to assess this integration of capabilities, such as MM-Vet and MMT-Bench, while also examining the methodological evolution from rigid multiple-choice metrics to flexible LLM-as-a-Judge evaluations for open-ended responses. Furthermore, it highlights the critical importance of measuring model robustness against leading questions and adversarial attacks to ensure reliability.

The need for holistic evaluation frameworks that bridge the gap between perception and cognition has become increasingly apparent. While traditional benchmarks like VQAv2 [1] effectively measure visual question answering, they often require simple retrieval of visual information rather than complex reasoning over the perceived content. To address this, researchers have developed benchmarks that explicitly target the intersection of visual understanding and higher-order cognitive functions. MM-Vet is a representative example of this new class of benchmarks. It defines a taxonomy of six core capabilities: recognition, knowledge, language generation, spatial awareness, temporal reasoning, and math. Crucially, MM-Vet tasks models with problems that require the composition of these capabilities. For instance, a model might need to recognize objects in an image, access external knowledge to understand their relationships, and then generate a coherent textual description that captures a complex scene. This moves beyond simple classification or captioning to assess how well a model can "connect the dots" between what it sees and what it knows. Similarly, MMT-Bench (Multimodal Task Benchmark) is designed to evaluate the multi-modal multi-task ability of MLLMs. It encompasses a wide range of tasks that require integrated perception and reasoning, such as interpreting scientific diagrams, understanding memes, or solving visual logic puzzles. The design of these benchmarks reflects a growing consensus that evaluating MLLMs requires moving away from narrow, siloed tasks toward more open-ended, integrative challenges that better reflect real-world problem-solving.

A significant methodological shift accompanying the rise of these complex benchmarks is the move from simple, automatic metrics to more nuanced, model-based evaluation paradigms. For tasks with objective, ground-truth answers (e.g., multiple-choice questions), metrics like accuracy are straightforward and reliable. However, the open-ended nature of advanced MLLM capabilities—such as generating creative stories from images, providing detailed explanations of visual phenomena, or engaging in complex dialogue—makes automatic evaluation extremely difficult. Traditional metrics like CIDEr or BLEU, which rely on n-gram overlap with reference text, are notoriously poor at capturing semantic correctness, coherence, or creativity. To overcome these limitations, the community has increasingly adopted the "LLM-as-a-Judge" paradigm, where a powerful LLM (e.g., GPT-4) is used to evaluate the responses of another model. This approach allows for the assessment of complex criteria such as helpfulness, truthfulness, and instruction following on a Likert scale or through pairwise comparisons. For example, in the context of evaluating integrated capabilities, an LLM judge can be prompted to assess whether a model's explanation of a visual scene accurately reflects the visual content while also providing sound reasoning. This paradigm is explored in depth in works like [95], which investigates the potential and pitfalls of using MLLMs themselves as evaluators for multimodal tasks. The study reveals that while MLLMs demonstrate remarkable human-like discernment in pairwise comparisons, they still face challenges with biases and inconsistencies, highlighting that the LLM-as-a-Judge method, while powerful, is not a perfect solution and requires further refinement.

However, as benchmarks become more sophisticated and evaluation methods more flexible, a new challenge emerges: the susceptibility of models to manipulation, particularly through leading questions and adversarial attacks. A model may perform well on standard benchmarks but fail catastrophically when presented with subtly biased prompts or visually perturbed inputs. This fragility is a major obstacle to deploying MLLMs in high-stakes environments. Therefore, robustness evaluation has become a critical component of assessing integrated capabilities. This involves testing models not just on their "happy path" performance but on their resilience to challenging inputs. For example, benchmarks are now incorporating adversarial examples where visual inputs are slightly altered to fool the model's perception module, or textual prompts are designed to elicit hallucinations or biased responses. The goal is to measure the model's ability to maintain consistency and factual grounding even when faced with deceptive or difficult inputs. The importance of this is underscored by the development of specialized benchmarks like MM-SafetyBench and JailBreakV-28K, which, while focused on safety, demonstrate the broader principle of stress-testing models against adversarial conditions. The insights from [28] are particularly relevant here. Their qualitative study, which evaluated models across 230 manually designed cases, revealed that even state-of-the-art MLLMs exhibit significant gaps in trustworthiness and causal reasoning. They found that models are often susceptible to generating plausible-sounding but incorrect answers, especially when faced with leading questions that exploit their statistical biases rather than their true reasoning capabilities. This underscores the necessity of evaluating models not just on whether they can answer a question correctly, but on whether they do so for the right reasons and can withstand adversarial probing.

The integration of perception and reasoning is further complicated by the evaluation of specific, high-level cognitive abilities that require a deep understanding of both modalities. For instance, spatial reasoning tasks, which ask models to manipulate or reason about the positions of objects in 2D or 3D space, test the model's ability to translate visual information into a structured representation suitable for logical operations. Similarly, mathematical reasoning over visual content, as seen in benchmarks like MathVista, requires parsing diagrams, charts, or geometric figures and applying mathematical principles to solve problems. Evaluating these capabilities often requires specialized benchmarks that provide a controlled environment for testing specific reasoning skills. The survey [90] categorizes these emerging trends, highlighting the frontier of MLLM reasoning from deductive and abductive reasoning to more abstract forms of intelligence. This survey points out that current evaluations often fail to disentangle the effect of visual recognition from pure reasoning, a gap that new benchmarks like NPHardEval4V aim to fill by converting textual reasoning problems into visual representations, thereby isolating the model's core reasoning ability. The findings from such benchmarks often reveal a significant performance gap between open-source and proprietary models, suggesting that the integration of robust reasoning capabilities remains a key frontier in MLLM development.

In conclusion, the evaluation of integrated capabilities and reasoning in MLLMs is a rapidly evolving field that reflects the increasing sophistication of the models themselves. The shift from simple, siloed tasks to holistic, integrative benchmarks like MM-Vet and MMT-Bench provides a more accurate picture of a model's ability to solve complex, real-world problems. This shift is accompanied by a methodological evolution towards LLM-as-a-Judge evaluations, which offer a scalable way to assess the nuanced, open-ended responses that advanced MLLMs are capable of generating. However, this progress is tempered by the critical need to assess model robustness. As highlighted by studies like [28], the ability of MLLMs to withstand leading questions and adversarial attacks is a key determinant of their trustworthiness and reliability. Ultimately, a comprehensive evaluation strategy must therefore be multi-faceted, combining integrative task design, flexible evaluation metrics, and rigorous robustness testing to truly gauge the progress of MLLMs on the path toward more general and intelligent forms of reasoning.

### 6.5 Tool Use and Agentic Reasoning

### 6.5 Tool Use and Agentic Reasoning

Building on the evaluation of integrated capabilities and reasoning, the evolution of Multimodal Large Language Models (MLLMs) has transcended static perception and description, moving towards dynamic agentic systems capable of planning, reasoning, and interacting with their environment. This subsection investigates MLLMs as agents capable of planning and utilizing external tools to solve complex tasks. By integrating external APIs, expert models, and control interfaces, MLLMs function as the central "brain" of complex systems, capable of orchestrating workflows that require both visual understanding and textual reasoning. This paradigm shift enables models to ground their reasoning in external reality, mitigating hallucinations and extending their capabilities far beyond their parametric knowledge.

#### Frameworks for Tool Integration and External Grounding

A primary avenue of research involves developing frameworks that allow MLLMs to interface with external tools. These frameworks treat the MLLM not just as a generator of text, but as a controller that can invoke specific functions based on multimodal inputs. For instance, frameworks like **MLLM-Tool** and **MM-REACT** establish protocols where the model analyzes an image or video, determines that its internal knowledge is insufficient or that a specific computation is required, and then calls an external tool to perform the task. This process involves parsing the user request, formulating a plan, selecting the appropriate tool (e.g., a visual search engine, a code interpreter, or a specialized image processing algorithm), and synthesizing the tool's output into a coherent final response.

The core challenge addressed by these frameworks is the "grounding" problem. While MLLMs excel at semantic understanding, they often struggle with precise localization or fine-grained attribute recognition. By outsourcing these tasks to specialized tools—such as OCR engines for text extraction or object detectors for bounding box generation—the system achieves higher accuracy and reliability. This modular approach allows the MLLM to focus on high-level reasoning and integration, while external tools handle low-level, high-precision operations. The synergy between the MLLM's reasoning capabilities and the deterministic nature of external tools creates a robust agentic system.

#### MLLMs as the "Brain" for Robotics and Embodied AI

The concept of MLLMs as the "brain" for robotics and embodied AI represents a significant leap towards general-purpose autonomous agents. In this context, the MLLM processes visual inputs from the robot's cameras, interprets natural language instructions, and generates high-level plans or low-level control actions. This capability transforms robots from machines executing pre-programmed scripts to agents that can understand and adapt to novel environments.

Research in this area explores how MLLMs can decompose complex instructions (e.g., "make me a cup of coffee") into a sequence of actionable steps (e.g., locate the coffee machine, pick up a mug, press the start button). The MLLM leverages its visual understanding to identify objects and its reasoning capabilities to infer affordances and dependencies between actions. For example, an MLLM agent might recognize that a lever needs to be pulled to dispense coffee, a nuance that requires understanding the physical interaction between objects. This capability is crucial for deployment in unstructured environments where explicit programming of every possible scenario is infeasible.

Furthermore, MLLMs can facilitate communication between different modules within a robotic system. They can translate high-level goals into commands for motion planners or perception modules, effectively acting as an interface that unifies disparate subsystems. This "central controller" architecture simplifies the development of complex robots, as developers can focus on defining the capabilities of individual tools and relying on the MLLM to orchestrate them effectively.

#### Planning and Hierarchical Reasoning in Agentic Systems

Agentic reasoning requires the ability to formulate and execute multi-step plans. MLLMs demonstrate an emerging capability for hierarchical planning, where they break down a complex task into sub-tasks and assign resources to each. This involves maintaining a state of the world, predicting the outcomes of actions, and adjusting the plan based on feedback from the environment or tool outputs.

For instance, in a visual question answering task that requires external knowledge, an MLLM agent might first attempt to answer from its internal knowledge. If confidence is low, it might formulate a web search query, execute the search via an API, and then use the retrieved text to formulate the final answer. This iterative process of "perception-plan-action-observation" is fundamental to agentic behavior. The model must decide when to stop searching, how to integrate conflicting information, and how to handle errors from external tools. This level of autonomy requires sophisticated reasoning capabilities that go beyond simple pattern matching, touching on aspects of causal inference and strategic thinking.

#### Challenges and Limitations of Agentic MLLMs

Despite the promise, deploying MLLMs as agents introduces several challenges. One major issue is the "hallucination of tool usage," where the model might confidently invoke a non-existent tool or use an existing tool incorrectly. This is particularly problematic in safety-critical applications like robotics, where an incorrect action could lead to physical damage. Ensuring that the MLLM adheres to strict tool usage protocols and fails gracefully when tools are unavailable is an active area of research.

Another challenge is the computational overhead and latency associated with tool use. Each tool invocation adds to the response time, which can be prohibitive in real-time applications. Optimizing the decision-making process to minimize unnecessary tool calls while maintaining task success rates is crucial. Additionally, training MLLMs to effectively use tools requires diverse datasets that include tool usage trajectories, which are difficult to curate. Synthetic data generation, where powerful models like GPT-4V are used to generate tool-using dialogues, is a promising direction to address this data scarcity.

#### Evaluation of Agentic Capabilities

Evaluating the performance of MLLM agents is more complex than traditional benchmarks. Metrics must account not just for the final output, but for the efficiency of the plan, the correctness of tool usage, and the robustness of the system to perturbations. Benchmarks are emerging that test these capabilities, such as those requiring the model to navigate a visual environment or solve a puzzle that requires external knowledge retrieval.

These evaluations often reveal that while MLLMs possess strong zero-shot tool-use capabilities, they struggle with long-horizon planning and error recovery. For example, if a tool returns an unexpected result, the MLLM may fail to diagnose the issue and retry with a modified query. Developing agents that can perform such self-correction is essential for building reliable systems. Furthermore, evaluating the safety and alignment of agentic MLLMs is paramount, as these systems have the potential to execute actions with real-world consequences.

#### Future Directions: Towards Autonomous Agentic Systems

The future of agentic MLLMs lies in creating more tightly integrated systems where the model learns to use tools in a more end-to-end fashion, rather than relying solely on prompting. This could involve fine-tuning MLLMs on tool-using datasets or developing architectures where tool embeddings are part of the model's vocabulary. Another direction is the development of multi-agent systems, where multiple MLLMs collaborate, each specializing in different tasks or tools, to solve problems that are too complex for a single agent.

Ultimately, the goal is to create MLLMs that can autonomously discover and learn to use new tools, adapting to novel environments without explicit human programming. This requires moving beyond static toolsets to dynamic environments where the agent can explore, experiment, and build a repertoire of skills. As MLLMs continue to scale and improve in reasoning, their role as the "brain" for embodied AI and complex software systems will become increasingly central to the field of artificial intelligence.

## 7 Applications and Real-World Deployments

### 7.1 Multimodal Retrieval and Retrieval-Augmented Generation (RAG)

Multimodal Retrieval-Augmented Generation (RAG) has emerged as a critical application paradigm for Multimodal Large Language Models (MLLMs), addressing the inherent limitations of standalone models in factual accuracy and knowledge currency. While MLLMs demonstrate impressive capabilities in understanding and generating content across modalities, they remain susceptible to hallucinations and knowledge cutoffs. Multimodal RAG systems mitigate these issues by grounding model responses in external, verifiable knowledge sources that can include images, videos, audio files, and structured multimodal databases. This approach transforms MLLMs from closed-book generators into open-book reasoners that can retrieve and synthesize information from diverse sources before formulating responses.

The fundamental architecture of Multimodal RAG extends the traditional text-only RAG pipeline by incorporating multi-modal retrieval and cross-modal reasoning capabilities. The process begins with a multimodal query, which may include text, images, or both. The system first encodes the query into a unified representation space, then performs retrieval across a corpus of multimodal documents. The retrieved information, which may include images with captions, video segments with transcripts, or audio with metadata, is then provided as context to the MLLM for generation. This architecture enables the model to access up-to-date information and domain-specific knowledge that wasn't present in its training data, while also providing citations and visual evidence for its claims.

Recent research has explored various architectures for Multimodal RAG. Some systems employ separate encoders for different modalities and fuse their representations for retrieval, while others leverage unified multimodal encoders that can process any modality in a shared space. The choice of retrieval strategy significantly impacts performance. Dense retrieval using learned embeddings has shown superior performance over traditional sparse methods like BM25 for cross-modal matching, especially when dealing with visual content where keyword-based approaches fail. The integration of MLLMs as both retrievers and generators creates a symbiotic system where the model can iteratively refine its understanding by retrieving additional information based on intermediate reasoning steps.

One of the key challenges in Multimodal RAG is the modality alignment problem during retrieval. When a user provides a text query asking about information visible in images, the system must bridge the semantic gap between textual descriptions and visual content. This requires sophisticated alignment techniques that go beyond simple text-to-text or image-to-image similarity. Some approaches use MLLMs to generate rich textual descriptions of images, enabling text-based retrieval, while others maintain separate vector spaces for different modalities and learn cross-modal attention mechanisms to match queries with relevant visual content. The effectiveness of these approaches varies depending on the domain and the nature of the multimodal corpus.

The role of MLLMs in Multimodal RAG extends beyond simple generation. They serve as intelligent routers that can analyze queries to determine which modality of information would be most helpful, synthesize information from multiple sources across different modalities, and verify the consistency of retrieved evidence. For instance, when answering a complex question about a historical event, the system might retrieve textual documents, archival photographs, and video footage, then use the MLLM to cross-reference these sources and provide a comprehensive answer with visual evidence. This capability is particularly valuable in domains like scientific research, where insights often emerge from connecting information across papers, figures, and experimental data.

Multimodal RAG has found applications across numerous domains. In e-commerce, systems can retrieve product images, specifications, and user reviews to answer customer questions about specific features visible in photos. In education, students can upload images of textbook diagrams or classroom whiteboards and receive explanations with references to relevant educational materials. Healthcare applications use Multimodal RAG to help clinicians retrieve relevant medical imaging studies, research papers, and patient records when diagnosing conditions. The legal domain benefits from systems that can retrieve case law, evidence photographs, and document scans to support legal research and case preparation.

The evaluation of Multimodal RAG systems presents unique challenges beyond traditional RAG metrics. While retrieval precision and recall remain important, systems must also be evaluated on their ability to correctly interpret multimodal queries, synthesize information across modalities, and provide accurate visual grounding for their claims. Benchmarks are emerging that test these capabilities, including tasks that require models to answer questions by retrieving and analyzing both textual and visual evidence. The presence of hallucinations in MLLMs is particularly problematic in RAG contexts, where the model might confidently state information that contradicts the retrieved evidence or invent details not present in any source.

Recent advances have focused on improving the integration between retrieval and generation components. Some approaches use the MLLM itself to generate synthetic queries for training the retriever, creating a feedback loop that improves both components. Others explore iterative retrieval, where the MLLM generates follow-up queries based on initial retrieved results to gather additional relevant information. The use of Mixture of Experts (MoE) architectures in MLLMs, as explored in models like [96], shows promise for Multimodal RAG by enabling the model to selectively activate different expert pathways depending on the modality and task requirements.

The efficiency of Multimodal RAG is another critical consideration. Processing high-resolution images and long videos is computationally expensive, both for encoding and retrieval. Some systems address this by using multi-stage retrieval pipelines that first perform coarse-grained filtering using efficient encoders, then apply more expensive MLLM-based reranking only to promising candidates. Others explore progressive processing, where the system initially retrieves low-resolution or frame-sampled versions of media, then requests higher-fidelity versions only when needed for detailed analysis. The use of efficient architectures like [97] can reduce computational overhead while maintaining performance.

Privacy and security considerations are paramount in Multimodal RAG deployments. When systems retrieve from sensitive corpora containing personal photos, medical images, or proprietary documents, access controls must be enforced at both retrieval and generation stages. Differential privacy techniques can help protect individual records in the retrieval corpus, while content filtering and safety alignment in the MLLM prevent the generation of harmful or inappropriate content even when retrieved from compromised sources. The modular nature of some MLLM architectures, such as [98], allows for the implementation of safety modules that can operate on retrieved content before it reaches the generation stage.

Looking forward, Multimodal RAG systems are evolving toward more sophisticated reasoning capabilities. The integration of Chain-of-Thought (CoT) prompting, as discussed in [90], enables models to explicitly reason about which modalities to retrieve from and how to combine evidence. Some research directions explore using MLLMs as agentic systems that can plan and execute complex retrieval strategies, potentially interacting with external tools and APIs to gather multimodal information. The development of unified frameworks like [5] suggests a future where Multimodal RAG can seamlessly handle any modality without specialized architectures for each.

The challenges of data quality and curation in Multimodal RAG cannot be overlooked. The effectiveness of retrieval depends heavily on the quality of the corpus indexing. This requires robust preprocessing pipelines that can extract meaningful representations from diverse multimodal content, handle missing modalities gracefully, and maintain metadata that supports effective retrieval. The principles outlined in [99] emphasize that data quality is crucial not just for training MLLMs, but also for building effective retrieval corpora. Synthetic data generation techniques, as explored in [100], can help create diverse training examples for improving retrieval models.

As Multimodal RAG systems become more sophisticated, they raise important questions about attribution and trust. Users need to understand not just what information the system retrieved, but how it was used in generating the response. Some systems are exploring ways to provide fine-grained citations, linking specific claims to specific retrieved documents or visual regions. This is particularly important for applications in scientific and legal domains where traceability is essential. The evaluation frameworks discussed in [29] and [45] provide foundations for measuring these capabilities, though new evaluation methods are needed specifically for Multimodal RAG systems.

In conclusion, Multimodal Retrieval-Augmented Generation represents a significant advancement in making MLLMs more reliable, knowledgeable, and useful in real-world applications. By grounding generation in external multimodal knowledge, these systems address key limitations of standalone MLLMs while opening up new possibilities for information access and synthesis across modalities. As research continues to improve the integration between retrieval and generation, the efficiency of processing, and the quality of multimodal understanding, Multimodal RAG will likely become a standard architecture for deploying MLLMs in domains where factual accuracy and comprehensive knowledge are essential.

### 7.2 Autonomous Systems and Robotics

The integration of Multimodal Large Language Models (MLLMs) into autonomous systems and robotics represents a paradigm shift from rigid, module-specific pipelines to flexible, generalist agents capable of complex reasoning in dynamic environments. By leveraging the semantic understanding and reasoning capabilities of LLMs alongside visual perception, MLLMs serve as the "brain" for autonomous vehicles and robotic agents, enabling them to interpret complex scenes, generate high-level plans, and provide interpretable rationales for their actions. This subsection explores the application of MLLMs in autonomous driving and robotics, focusing on their roles in environment perception, decision-making, and the generation of interpretable reasoning for navigation and control tasks.

### Environment Perception and Semantic Understanding

In traditional autonomous systems, perception is often treated as a detection and tracking problem, identifying objects like cars, pedestrians, and traffic signs. However, MLLMs elevate this process to a level of semantic and contextual understanding. Instead of merely bounding boxes, MLLMs can generate detailed descriptions of the environment, reason about the relationships between objects, and infer the underlying intent of other agents.

For instance, in autonomous driving, an MLLM can process high-resolution images or video sequences to provide a comprehensive understanding of the driving scene. This goes beyond simple object detection to include reading text (OCR), understanding traffic signals in context, and interpreting complex scenarios such as construction zones or unusual pedestrian behavior. The ability to process high-resolution inputs is crucial, as highlighted by models like **InternVL 1.5**, which introduces dynamic high-resolution support up to 4K, allowing the model to capture fine-grained details essential for safe navigation [30]. Similarly, **MM1** demonstrates that careful scaling of vision encoders and resolution significantly impacts performance on visual perception tasks, which is foundational for any downstream autonomous application [3].

Furthermore, MLLMs can handle multi-modal inputs beyond just vision, incorporating sensor fusion naturally. While current models primarily focus on vision and language, the architectural principles allow for the integration of LiDAR or radar data represented as visual tokens. This holistic perception capability allows the system to build a rich, semantic map of the world that is not just geometric but also contextual, essential for navigating unstructured environments.

### Decision-Making and Planning

Once the environment is perceived, the agent must make decisions. Traditional robotics relies on hard-coded rules or reinforcement learning policies that are often opaque and difficult to generalize. MLLMs introduce a new paradigm of "semantic planning," where the model uses its language understanding capabilities to break down complex tasks into executable steps.

In the context of robotics, **VisionLLM v2** proposes an end-to-end generalist framework that unifies visual perception, understanding, and generation, enabling the model to handle hundreds of vision-language tasks through a unified interface [101]. This unification suggests that a single MLLM can serve as a universal controller, translating high-level natural language commands (e.g., "pick up the blue block and place it on the table") into a sequence of visual grounding and control actions. The model utilizes a "super link" mechanism to connect the MLLM with task-specific decoders, allowing for flexible information transmission and gradient feedback, which is vital for training generalist agents.

For autonomous driving, MLLMs facilitate complex decision-making by reasoning about causality and future predictions. By analyzing video sequences, an MLLM can predict the likely behavior of other vehicles or pedestrians based on their movements and the current traffic context. This capability is explored in the survey **Vision Language Models in Autonomous Driving and Intelligent Transportation Systems**, which highlights how integrating language data allows vehicles to deeply understand real-world environments, improving safety and efficiency [102]. The MLLM acts as a high-level planner, generating interpretable reasoning for navigation and control tasks. Instead of outputting raw steering angles, the model might output a reasoning trace: "The pedestrian is waiting to cross, so I must yield," which can then be translated into control signals.

### Interpretable Reasoning and Explainability

One of the critical challenges in autonomous systems is the "black box" nature of deep learning models. MLLMs offer a significant advantage in generating interpretable reasoning, which is crucial for debugging, safety verification, and user trust. When an autonomous vehicle makes a decision, it is beneficial for the system to explain *why* it made that decision.

MLLMs excel at Multimodal Chain-of-Thought (M-CoT) reasoning. As discussed in **Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning**, M-CoT extends standard chain-of-thought prompting to the visual domain, allowing models to generate intermediate visual and textual steps to solve complex problems [90]. For example, in a robotics task, an MLLM might generate a rationale such as: "I see a cup on the edge of the table [103]. If I move quickly, the cup might fall [104]. Therefore, I will move slowly and approach from the center [105]." This explicit reasoning process allows human operators to understand the agent's "thought process," making it easier to identify failures in perception or logic.

This capability is further enhanced by frameworks like **MM-REACT**, which enable MLLMs to ground their reasoning in external APIs or expert models, allowing for a mix of internal reasoning and external tool use [42]. By generating textual rationales alongside visual inputs, MLLMs bridge the gap between low-level control signals and high-level semantic understanding, providing a level of explainability that is difficult to achieve with traditional neural network controllers.

### Challenges and Future Directions

Despite the promise, deploying MLLMs in autonomous systems and robotics faces several challenges. The computational cost of processing high-resolution video in real-time is significant. While efficiency optimizations like **Visual Tokens Withdrawal** (which removes redundant visual tokens in deep layers to boost inference speed) are promising [19], real-time deployment on resource-constrained robotic platforms remains difficult. Furthermore, the "hallucination" problem, where MLLMs generate plausible but factually incorrect information, poses a severe safety risk in safety-critical applications like autonomous driving. As noted in **Hallucination of Multimodal Large Language Models: A Survey**, mitigating these hallucinations is a critical area of research [106].

Another significant challenge is the lack of physical grounding. While MLLMs possess vast semantic knowledge, they often lack an understanding of physics and causality. Integrating World Models, as suggested in **World Models and Embodied AI**, is a promising future direction to bridge this gap, allowing agents to predict the physical outcomes of their actions before executing them [78]. This integration would combine the semantic reasoning of MLLMs with the predictive power of physics simulators, leading to more robust and safer autonomous agents.

In conclusion, MLLMs are transforming autonomous systems and robotics by providing a unified framework for perception, planning, and reasoning. By leveraging natural language as an interface, these models enable more flexible, interpretable, and generalizable agents. As research progresses in efficiency, safety, and physical grounding, MLLMs are poised to become the central cognitive architecture for the next generation of autonomous machines.

### 7.3 Healthcare and Biomedicine

The integration of Multimodal Large Language Models (MLLMs) into the healthcare and biomedical sectors represents a paradigm shift in clinical practice, moving beyond traditional diagnostic tools toward comprehensive systems capable of multimodal reasoning. By synthesizing visual data from medical imaging with textual inputs such as electronic health records (EHRs), clinical notes, and biomedical literature, MLLMs offer unprecedented opportunities to enhance diagnostic accuracy, streamline clinical workflows, and democratize access to medical expertise. This subsection reviews the deployment of MLLMs in medical domains, specifically for analyzing biomedical images and texts, generating clinical reports, and assisting in computer-aided diagnosis.

### Analyzing Biomedical Images and Generating Clinical Reports

One of the most critical applications of MLLMs in healthcare is the analysis of biomedical images, ranging from radiology and pathology to ophthalmology and dermatology. Traditional deep learning models for medical imaging are often specialized, trained on specific tasks like tumor segmentation or classification, and lack the flexibility to handle diverse clinical queries or generate descriptive reports. MLLMs bridge this gap by combining visual perception with linguistic reasoning. For instance, a model can ingest a chest X-ray and not only detect anomalies but also generate a comprehensive radiology report detailing the findings, impression, and differential diagnoses. This capability is underpinned by the generalist nature of MLLMs, which allows them to function across various imaging modalities without task-specific retraining.

The ability to process high-resolution medical images is crucial here, as subtle textures or minute structural changes can be diagnostic indicators. Recent advancements in vision encoders and resolution scaling strategies, such as those explored in **MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training**, demonstrate that increasing image resolution and the number of visual tokens significantly improves performance on detail-oriented tasks. Furthermore, the "Mixture of Encoders" approach found in models like **Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders** suggests that combining different vision experts—perhaps one specialized in global context and another in local details—could be highly beneficial for analyzing complex medical scans.

The generation of clinical reports is a specific and high-value task where MLLMs excel. Automating the drafting of radiology reports can alleviate the burden on radiologists, reduce turnaround times, and minimize reporting errors. MLLMs can be trained on large-scale datasets of image-report pairs to learn the mapping between visual findings and their textual descriptions. The quality of this training data is paramount. As highlighted in **The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective**, there is a symbiotic relationship between data and model development, making high-quality, curated medical datasets essential for training reliable models. The use of synthetic data generation is particularly promising in the medical field where patient privacy concerns often limit data availability. Powerful proprietary models can be used to generate synthetic but realistic medical image-text pairs, augmenting training corpora and improving the model's ability to describe complex pathologies. Moreover, the instruction tuning phase is critical for teaching the model the specific format and terminology required in clinical reports. Datasets like **LaVy: Vietnamese Multimodal Large Language Model**, which focus on a specific language, demonstrate the importance of domain-specific and culturally adapted data, a principle that extends to the specialized jargon of medicine.

### Assisting in Computer-Aided Diagnosis and Agentic Workflows

Beyond reporting, MLLMs are increasingly used to assist in computer-aided diagnosis (CAD) by acting as reasoning engines. They can integrate a patient's imaging data with their medical history (text) to provide a more holistic diagnostic assessment. For example, in oncology, an MLLM could analyze a CT scan while simultaneously referencing the patient's genetic markers and previous treatment history to suggest personalized treatment plans. This requires the model to possess complex reasoning capabilities, moving beyond simple pattern recognition. The survey **A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks** notes that MLLMs address the complexities of real-world applications far beyond single-modality systems, which is precisely the nature of medical diagnosis.

However, the transition from general-purpose MLLMs to medical specialists is non-trivial. The paper **Beyond Specialization Assessing the Capabilities of MLLMs in Age and Gender Estimation** provides a relevant case study, showing that while general MLLMs can perform specialized tasks like age and gender estimation, they often fall short of specialized models. This suggests that for high-stakes medical diagnosis, fine-tuning or specialized architectures are necessary. The study **Mysterious Projections Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections** offers an intriguing insight: it finds that domain-specific visual capabilities are often modeled by the LLM itself, even when only the projection module is fine-tuned. This implies that the heavy lifting of medical visual understanding might be occurring within the language model component, guided by the projection, which has implications for how we design efficient medical MLLMs.

Furthermore, the agentic capabilities of MLLMs open up possibilities for autonomous clinical support systems. An MLLM agent could be tasked with retrieving relevant medical literature from a database (Retrieval-Augmented Generation or RAG) to support a diagnosis, or orchestrating a pipeline of specialized AI models (e.g., a segmentation model followed by a classification model) to analyze a complex case. The framework **MLLM-Tool** demonstrates how models can ground their reasoning in external APIs, which in a medical context could be connections to hospital information systems or specialized diagnostic software. This transforms the MLLM from a passive analyzer into an active assistant that can plan, reason, and utilize tools to solve complex clinical problems.

### Challenges and Future Directions

The deployment of MLLMs in healthcare also faces significant challenges, primarily concerning robustness, safety, and hallucination. In a medical context, a hallucination—where the model invents a pathology that isn't present or misses a critical finding—can have severe consequences. The pervasive issue of hallucination in MLLMs is a major research focus. Models may generate confident but incorrect diagnoses, a risk that is amplified when they are applied to out-of-distribution data or rare conditions. To mitigate this, rigorous evaluation is essential. While general benchmarks are useful, the medical domain requires specialized evaluation suites. The paper **A Survey on Benchmarks of Multimodal Large Language Models** highlights the need for domain-specific benchmarks, and indeed, specialized benchmarks for medical VQA and report generation are emerging.

Furthermore, the safety of MLLMs is paramount. They must be robust against adversarial attacks that could subtly alter an image to mislead the diagnosis. Additionally, alignment failures, where the model provides harmful or incorrect medical advice, must be prevented. The use of guardrails and multi-agent debate systems could be adapted to create "consensus" mechanisms among multiple medical AI agents before presenting a diagnosis to a clinician. Efficiency is another critical consideration for real-world deployment in settings that may range from centralized data centers to resource-constrained edge devices like portable ultrasound machines. The computational cost of processing high-resolution medical images and long text contexts is substantial. Research into efficient MLLMs, as surveyed in **Efficient Multimodal Large Language Models: A Survey**, is therefore highly relevant. Techniques such as quantization, pruning, and parameter-efficient fine-tuning (PEFT) like LoRA can reduce the memory footprint and computational overhead. The development of hybrid architectures, such as **LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently Via Hybrid Architecture**, which combines state-space models like Mamba with Transformers, offers a path to handling long contexts—such as a patient's entire medical history—efficiently, which is crucial for longitudinal analysis.

In conclusion, the deployment of MLLMs in healthcare and biomedicine is a rapidly advancing field with the potential to revolutionize patient care. By leveraging the ability to process and reason over both visual and textual medical data, these models can assist in everything from initial image screening to complex differential diagnosis and report generation. However, realizing this potential requires overcoming significant hurdles related to data curation, model robustness, safety, and efficiency. The ongoing research into architectural innovations, training paradigms, and evaluation benchmarks will be critical in building trustworthy and effective medical AI systems that can safely augment the capabilities of human clinicians.

### 7.4 Finance and Data Analysis

The integration of Multimodal Large Language Models (MLLMs) into the financial sector represents a significant leap forward in automating complex analytical tasks that traditionally required human expertise. Finance is an inherently multimodal domain, where critical information is distributed across textual reports, numerical tables, and visual data representations such as stock charts and infographics. MLLMs, with their capacity to synthesize information across these diverse formats, are increasingly being deployed to interpret financial documents, analyze market trends, and support high-stakes decision-making processes. This subsection explores the specific applications of MLLMs in finance, focusing on their ability to parse complex documents, interpret visual data, and function as analytical agents.

### Interpreting Financial Documents and Visual Data

One of the most immediate and impactful applications of MLLMs in finance is the automated interpretation of financial documents. These documents, such as 10-K and 10-Q reports, contain a mixture of prose, tables, and charts. By leveraging vision encoders like ViT and CLIP, MLLMs can ingest these documents as images and extract structured information, generating summaries and answering specific queries without relying on separate Optical Character Recognition (OCR) systems. This capability streamlines workflows for financial analysts who need to quickly synthesize information from vast repositories of regulatory filings and earnings reports.

Furthermore, MLLMs demonstrate remarkable proficiency in analyzing financial charts and graphs. A key challenge in financial analysis is interpreting visualizations to understand market sentiment or company performance. MLLMs can identify trends in line graphs, compare volumes in bar charts, and even interpret complex candlestick patterns used in technical analysis. This visual reasoning capability allows models to answer questions like "What was the trend in revenue over the last three quarters based on this chart?" or "Does the stock price show signs of a 'head and shoulders' pattern?". The ability to ground linguistic reasoning in visual evidence is crucial for reducing hallucinations and providing accurate, context-aware analysis. This aligns with the broader trend of MLLMs evolving into generalist systems capable of handling diverse inputs, as noted in surveys like [1].

### Assisting in Market Analysis and Decision-Making

Beyond document parsing, MLLMs are becoming integral to market analysis and strategic decision-making. They can process real-time data streams that include news articles, social media sentiment (text), and market tickers (numerical), synthesizing a holistic view of market conditions. For instance, an MLLM could analyze a news headline about a merger alongside a corresponding stock chart to predict potential impacts on share prices. This synthesis of disparate data types mirrors the capabilities observed in advanced proprietary models like GPT-4V and Gemini, which have shown strong performance in reasoning across multiple modalities [25].

The role of MLLMs extends to acting as financial advisors or "co-pilots" for traders and portfolio managers. By integrating with external tools and APIs, as explored in frameworks like [107], these models can execute complex workflows. For example, an MLLM could be instructed to "Analyze the latest quarterly report of Company X, compare its P/E ratio with competitors using a web search, and visualize the comparison in a bar chart." This requires not only understanding the multimodal input but also planning and utilizing external tools to fetch data and generate outputs. This agentic capability transforms MLLMs from passive information processors into active participants in the analytical workflow.

### Challenges and the Path to Robust Financial MLLMs

Despite their potential, deploying MLLMs in finance comes with significant challenges, primarily centered around robustness, safety, and hallucination. Financial decision-making is high-stakes, and errors can lead to substantial monetary losses. Therefore, the "emergent" capabilities of MLLMs must be rigorously evaluated. The phenomenon of hallucination, where models generate plausible but factually incorrect information, is particularly dangerous in finance. For example, an MLLM might misinterpret a chart or invent a financial metric that does not exist. Research into hallucination mitigation is critical for building trust in these systems.

Moreover, the evaluation of financial MLLMs requires specialized benchmarks. General-purpose benchmarks like MME or SEED-Bench may not capture the nuances of financial reasoning. Domain-specific evaluation is necessary to assess capabilities such as interpreting regulatory language, understanding financial jargon, and performing accurate numerical reasoning. The development of benchmarks that test these specific skills is an active area of research. The success of models like InternVL, which achieve strong performance on OCR-related tasks through high-quality bilingual datasets [30], suggests that targeted data curation is a viable path forward for financial MLLMs. By training on high-quality, domain-specific instruction data, similar to the methods proposed in [34], it is possible to enhance the reliability and accuracy of MLLMs for financial applications.

In conclusion, MLLMs are poised to revolutionize the financial industry by automating the extraction of insights from multimodal data. Their ability to interpret complex documents, analyze visual market data, and function as agentic co-pilots offers unprecedented efficiency and analytical depth. However, realizing this potential requires overcoming significant hurdles related to factual accuracy and domain-specific robustness. Continued research into specialized training, evaluation, and safety mechanisms will be essential for deploying MLLMs in a sector where precision is paramount.

### 7.5 Content Generation, Editing, and Mixed Reality

The integration of Multimodal Large Language Models (MLLMs) into creative workflows and immersive environments marks a significant paradigm shift from passive analysis to active creation and augmentation. This subsection explores the burgeoning applications of MLLMs in multimodal content generation, visual editing, and their transformative potential within Mixed Reality (MR) systems. By bridging the gap between linguistic intent and visual realization, these models are evolving into universal engines for synthetic media and personalized user companions.

### Multimodal Content Generation: From Text to Visuals

The most prominent application of MLLMs in the creative domain is text-to-content generation. While early generative models focused on single modalities, modern MLLMs are increasingly capable of orchestrating complex scenes described in natural language. This involves not just generating static images but synthesizing coherent video sequences and dynamic visual narratives. The core capability lies in the model's ability to understand abstract concepts, spatial relationships, and stylistic attributes from textual prompts and translate them into pixel space.

However, generating high-fidelity content requires overcoming the "modality gap"—the semantic disconnect between text and visual representations. Research into data curation has been pivotal here. High-quality instruction-tuning datasets, such as those mentioned in the context of **ShareGPT4V**, provide the necessary alignment signals to teach models the nuances of visual description. Furthermore, the generation of synthetic data using powerful proprietary models like GPT-4V, as referenced in **Genixer**, allows for the scaling of training data to cover diverse creative domains. This data-centric approach ensures that generative MLLMs can handle complex instructions involving composition, lighting, and emotional tone.

The architectural underpinnings of these generative systems often involve a tight coupling between a language model and a diffusion model or a transformer-based generator. The MLLM acts as the "brain," interpreting the prompt and potentially retrieving relevant visual concepts, while the generative component acts as the "hand" that renders the output. This synergy enables capabilities beyond simple image synthesis, such as generating code for web design or creating assets for virtual environments.

### Intelligent Visual Editing and Manipulation

Beyond generation, MLLMs are revolutionizing visual editing by enabling semantic manipulation of existing media. Traditional editing tools require precise manual inputs (e.g., masks, coordinates), but MLLMs allow for "instruction-based editing." Users can issue high-level commands like "remove the background," "change the lighting to sunset," or "replace the car with a bicycle." This requires the model to possess a deep understanding of object permanence, occlusion, and scene geometry.

The ability to follow complex visual instructions is a direct result of advanced training paradigms. Visual instruction tuning, utilizing diverse datasets like **LLaVA-Instruct** and **Vision-Flan**, teaches models to associate specific textual tokens with visual operations. Moreover, the integration of visual prompts—such as bounding boxes or segmentation masks provided by the user—enhances the "steerability" of the model. As noted in studies regarding advanced perception, allowing the model to process these auxiliary visual inputs alongside the image enables fine-grained control over the editing process. This moves the user interface from pixel-based manipulation to intent-based interaction.

### Integration into Mixed Reality (MR) Systems

The convergence of MLLMs with Mixed Reality represents the frontier of human-computer interaction. In MR environments, where digital overlays coexist with the physical world, MLLMs serve as the cognitive layer that interprets the user's context and intent. This transforms MR headsets or smart glasses from passive display devices into proactive, personalized companions.

The role of the MLLM in MR is threefold: perception, reasoning, and generation. First, the model perceives the environment through the device's cameras, identifying objects, text (OCR), and spatial layouts. Second, it reasons about the user's needs based on gaze, gestures, and voice commands. For example, if a user looks at a broken appliance, the MLLM can retrieve the manual, highlight the relevant component in the MR view, and generate step-by-step visual instructions. This capability relies on the OCR-free reasoning and complex instruction following highlighted in **GPT-4V** and **Gemini**-like systems.

Third, the model generates real-time digital content that is contextually anchored. This could range from generating a virtual character that interacts with the physical environment to synthesizing a decorative overlay for a room. The challenge here is latency and efficiency. As discussed in **FastNet**, techniques like Visual Tokens Withdrawal and Mixture of Experts (MoE) are crucial to run these massive models on edge devices with limited computational resources. Furthermore, the use of parameter-efficient fine-tuning (PEFT) techniques like LoRA allows for the customization of the MLLM for specific MR applications (e.g., a specialized repair assistant) without the prohibitive cost of full fine-tuning.

### Agentic Capabilities and Tool Use in Creative Workflows

The evolution of MLLMs in creative and MR applications is increasingly agentic. Rather than merely generating a static output, these models are being integrated into workflows where they can plan, execute, and refine tasks using external tools. Frameworks like **MM-REACT** and **MLLM-Tool** enable MLLMs to call upon specialized APIs—for instance, using a code interpreter to generate 3D models or a search engine to fetch reference images.

In the context of content generation, this agentic behavior allows for iterative refinement. An MLLM can generate a draft image, critique its own output based on the prompt, and then issue a refined instruction to the generator. In MR, this manifests as the ability to interact with the environment. An MLLM agent could control smart home devices, adjust lighting to match a generated mood, or even guide a robot to clear a workspace for a virtual presentation. This requires the model to understand not just the content of the image but the physics and affordances of the physical world, a capability that is being accelerated by the integration of World Models.

### Challenges and Future Directions

Despite the rapid progress, significant challenges remain. The "hallucination" problem, extensively discussed in **The Pervasive Issue of Hallucination**, is particularly detrimental in creative and MR contexts. An MLLM generating a tutorial for a physical task must be factually accurate; fabricating a step or misidentifying a part could lead to physical harm or frustration. Similarly, in creative generation, models often struggle with compositional complexity, failing to place multiple objects correctly or maintain consistent styles.

Safety and robustness are also paramount. As MLLMs become the interface for MR, they become vectors for adversarial attacks. A maliciously crafted sticker on a physical object could trick the MLLM into misclassifying it or generating harmful content, as explored in **Adversarial Robustness and Attack Surfaces**. Ensuring that the model's alignment holds up in the open-ended environment of MR is an unsolved problem.

Looking forward, the trajectory points toward unified models that seamlessly handle understanding and generation. The move toward **Unified Multimodal Understanding and Generation** (e.g., NExT-GPT) suggests a future where the same MLLM architecture can interpret a user's sketch and generate a photorealistic rendering of it in real-time. Furthermore, the integration of **Cognitive Architectures and System 2 Reasoning** will allow these systems to plan complex creative projects over long horizons, moving from simple "copilots" to true creative partners.

In conclusion, the application of MLLMs in content generation, editing, and Mixed Reality is redefining the boundaries of digital creativity and human augmentation. By leveraging high-quality data, efficient architectures, and agentic frameworks, these models are transitioning from tools that process information to systems that create and interact with the world. However, realizing their full potential requires addressing fundamental issues of robustness, safety, and computational efficiency, ensuring that these powerful systems are reliable companions in both the virtual and physical realms.

### 7.6 Agentic Systems and Tool Use

### 7.6 Agentic Systems and Tool Use

The evolution of Multimodal Large Language Models (MLLMs) has transcended static perception and reasoning, moving toward dynamic agentic systems capable of planning, decision-making, and interacting with external environments. This paradigm shift transforms MLLMs from passive question-answering systems into active controllers that can leverage external resources—such as APIs, expert models, and knowledge bases—to solve complex, multi-step problems. In this context, MLLMs serve as the central cognitive unit, or "brain," of an agent, interpreting multimodal inputs, understanding tool instructions, and orchestrating workflows to achieve user-defined goals. This capability is a natural extension of the creative and immersive applications discussed previously, where MLLMs move beyond simple generation to become proactive collaborators in complex tasks.

#### The MLLM as a Universal Controller

At the core of agentic systems is the ability of the MLLM to function as a universal foundation model that interfaces with diverse tools. Unlike traditional models that output text based solely on internal knowledge, agentic MLLMs generate structured plans that invoke external functions. This requires a deep understanding of tool descriptions, input requirements, and output formats. The model must parse the user's multimodal query (e.g., an image accompanied by a text instruction), decompose the task into sub-steps, and select the appropriate tools to execute these steps.

Research in this domain has focused on developing frameworks that enable seamless integration between MLLMs and external tools. For instance, [108] proposes a system where the MLLM is explicitly trained to understand tool instructions and generate executable code or API calls. This framework allows the model to ground its reasoning in external tools, significantly expanding its capabilities beyond the limitations of its training data. Similarly, [109] introduces a reasoning and action framework that combines MLLMs with specialized vision modules. In this setup, the MLLM acts as a supervisor, decomposing complex visual tasks and dispatching them to expert vision modules (e.g., for object detection or OCR), then synthesizing the results into a coherent response. These works demonstrate that MLLMs can effectively function as universal controllers, orchestrating a pipeline of diverse tools to solve problems that require both visual understanding and external computation.

The concept of a "universal" agent is further supported by the development of unified architectures that can handle multiple modalities and tasks within a single framework. Models like [110] and [111] contribute to this vision by offering efficient and flexible architectures. [110], for example, provides on-demand spatial-temporal understanding at arbitrary resolutions, which is crucial for agents that need to process high-resolution visual inputs from cameras or documents to make informed decisions. [111] introduces a novel parameter space alignment paradigm that represents visual information as model weights, reducing the computational overhead of processing visual tokens. This efficiency is vital for agentic systems that require low-latency responses in real-time interactions. By minimizing the computational burden of visual perception, these architectures enable MLLMs to dedicate more resources to planning and tool use, making them more viable as real-time agents.

#### Tool Use and Complex Workflow Execution

The ability to execute complex workflows is a defining characteristic of agentic MLLMs. This involves not just calling a single tool, but chaining multiple tools in a logical sequence, potentially involving iterative refinement based on intermediate results. For example, an agent tasked with "analyzing the financial trend shown in this chart and summarizing the key findings" might first use a vision tool to extract data points from the chart, then a data analysis tool to compute trends, and finally a language model to generate a summary.

The development of benchmarks and frameworks is essential for evaluating and advancing these capabilities. [112] offers a framework for decoupling and assessing the capabilities of Vision-Language Models (VLMs), separating perception from reasoning. This is particularly relevant for agentic systems, as it allows researchers to identify whether failures stem from poor visual understanding or flawed reasoning about tool use. By isolating these components, [112] facilitates a more targeted improvement of the agent's planning and execution abilities. Furthermore, the integration of visual prompts, as explored in [113], enhances the agent's ability to precisely identify and interact with specific elements in its environment, which is critical for tasks like robotic manipulation or GUI automation.

The role of MLLMs in robotics and autonomous systems represents a significant application of agentic capabilities. In these scenarios, the MLLM processes sensory inputs (e.g., camera feeds) and generates high-level commands for actuators. The model must understand spatial relationships, object affordances, and the consequences of actions. The ability to reason about dynamic environments and predict outcomes is a step towards more sophisticated embodied intelligence. The integration of World Models, as discussed in future directions, will further enhance this by allowing agents to simulate and evaluate potential action sequences before execution.

#### Efficiency and Scalability in Agentic Systems

Deploying MLLMs as agentic systems in real-world applications requires addressing significant efficiency challenges. Agentic workflows often involve multiple inference steps, which can be computationally expensive. To mitigate this, researchers have explored parameter-efficient fine-tuning (PEFT) techniques and Mixture of Experts (MoE) architectures. [114] demonstrates how a sparse MoE architecture can achieve performance comparable to dense models with a fraction of the active parameters. For agentic systems, this means that different "experts" within the model could specialize in different tools or types of reasoning, activating only the relevant pathways for a given task. This sparsity leads to faster inference and lower computational costs, making it feasible to deploy complex agents at scale.

Furthermore, the design of the interaction loop between the MLLM and tools is crucial for efficiency. Instead of processing the entire high-resolution image at every step, an agent can employ a "divide and conquer" strategy, as seen in [115]. The agent can first analyze a low-resolution overview to identify regions of interest, then selectively invoke tools to process high-resolution patches of those regions. This adaptive processing ensures that computational resources are focused on relevant parts of the input, mirroring human visual attention.

#### Challenges and Future Directions

Despite the progress, several challenges remain in building robust agentic systems based on MLLMs. One major issue is the reliability and safety of tool use. An agent with access to external APIs or the ability to generate code can cause unintended consequences if its planning is flawed or its understanding of tool capabilities is inaccurate. Ensuring that agents adhere to safety constraints and fail gracefully is an active area of research. The evaluation of such systems also needs to evolve beyond static benchmarks to dynamic, interactive environments that measure the agent's ability to achieve long-horizon goals.

Another challenge is the grounding of abstract instructions. While MLLMs are good at following explicit commands, they may struggle with ambiguous or high-level goals that require common sense reasoning to decompose into actionable steps. The development of more sophisticated instruction-following datasets and training paradigms, such as those that incorporate multi-turn dialogues and feedback loops, will be essential.

In conclusion, the transition of MLLMs from passive models to active agentic systems represents a pivotal development in AI. By leveraging their powerful multimodal understanding and integrating them with external tools, we can create universal controllers capable of executing complex workflows in diverse real-world domains. The advancements in architectures like [110], frameworks like [108] and [109], and efficiency techniques like [114] provide a strong foundation. As research continues to address challenges in safety, reliability, and efficiency, MLLM-based agents are poised to become integral components of future AI-native applications, from autonomous robotics to intelligent personal assistants.

### 7.7 Wireless Systems and Network Management

The integration of Multimodal Large Language Models (MLLMs) into telecommunications represents a paradigm shift toward managing AI-native wireless networks and 6G systems. As the complexity of network infrastructures escalates with the advent of hyper-connected devices, autonomous systems, and the Internet of Things (IoT), traditional network management strategies—often reliant on rigid, rule-based algorithms or unimodal data analysis—have become increasingly insufficient. The telecommunications sector is currently grappling with the "data deluge," where vast amounts of heterogeneous data are generated by network sensors, user equipment, and environmental monitors. MLLMs, with their ability to process and reason over text, images, audio, and structured data simultaneously, offer a transformative solution. By acting as universal interfaces, these models can synthesize multi-modal sensing data to provide holistic situational awareness, automate complex decision-making processes, and optimize resource allocation in real-time.

The core utility of MLLMs in wireless systems lies in their capacity to bridge the gap between raw sensory inputs and high-level semantic reasoning. In a typical 6G environment, data flows from diverse sources: Radio Frequency (RF) signals, spectral scans, LiDAR point clouds for environment mapping, camera feeds for visual monitoring of physical infrastructure, and textual logs from network operations centers. Historically, these modalities were processed by isolated subsystems, leading to fragmented insights and delayed responses to network anomalies. MLLMs disrupt this siloed approach by unifying these data streams. For instance, an MLLM can correlate visual data from a camera monitoring a cell tower with RF signal strength data and textual maintenance logs to diagnose a physical obstruction or hardware failure. This capability is crucial for "Zero-Touch Network and Service Management" (ZSM), a key vision for 6G that aims for highly automated, self-healing, and self-optimizing networks.

A significant application area is the optimization of the Radio Access Network (RAN). The O-RAN (Open RAN) architecture, which promotes interoperability and virtualization, generates massive amounts of data through the RAN Intelligent Controller (RIC). MLLMs can ingest this multi-modal data to perform advanced traffic prediction and resource scheduling. Unlike traditional deep learning models that might only analyze time-series signal data, an MLLM can interpret the context of network usage by analyzing visual cues (e.g., a crowded stadium detected via satellite imagery or public webcams) and textual reports of local events. This allows the network to proactively allocate bandwidth and adjust antenna tilts, ensuring Quality of Service (QoS) during peak loads. Furthermore, MLLMs can assist in automated spectrum management. By analyzing spectrograms (visual representations of radio signals) alongside textual descriptions of interference sources, the model can identify and mitigate jamming or interference sources more effectively than systems relying solely on signal statistics.

The deployment of MLLMs in wireless environments also addresses the critical challenge of network security. Adversarial attacks in wireless networks often manifest as subtle anomalies across different signal modalities. MLLMs can act as sophisticated intrusion detection systems that understand the "intent" behind signal patterns. For example, by processing RF signals as images and correlating them with network traffic logs, an MLLM can detect sophisticated spoofing attacks or unauthorized device connections that would be invisible to unimodal detectors. This multi-faceted analysis is essential for securing the vast attack surface of 6G networks, where the distinction between physical and cyber threats is increasingly blurred.

However, deploying these models in real-world telecommunications settings introduces specific architectural and efficiency challenges. Wireless edge devices often operate under strict constraints regarding latency, power consumption, and computational resources. The high computational cost of processing high-resolution visual data or long sequences of RF signals can be prohibitive. To mitigate this, researchers are exploring parameter-efficient fine-tuning (PEFT) techniques and architectural optimizations specifically designed for MLLMs. For instance, methods that reduce the number of visual tokens required for processing, or hybrid architectures that combine lightweight feature extractors with powerful LLM backbones, are vital for real-time inference at the network edge. The "Law of Vision Representation" [116] suggests that optimizing the alignment between visual and language representations can significantly improve performance without necessarily increasing model size, a crucial consideration for scalable deployment in telecommunications.

Moreover, the concept of "Agentic Reasoning" is particularly relevant here. MLLMs in wireless systems are not just passive analyzers; they are evolving into active agents that can utilize external tools. An MLLM agent could be tasked with optimizing a network slice for a specific application (e.g., autonomous driving). It would query external databases for user location, access real-time traffic maps, and utilize propagation models to simulate signal coverage, then generate a configuration script to update the network parameters. This agentic capability transforms the MLLM from a mere observer to a controller of the network infrastructure.

The transition to 6G involves not just faster speeds but a fundamental integration of the physical and digital worlds, often referred to as the "Internet of Senses." This requires networks that can understand and adapt to complex physical environments. MLLMs are uniquely positioned to serve as the cognitive engine for such systems. By processing multi-modal data, they can create a rich semantic understanding of the network environment, enabling applications like holographic communications and high-fidelity digital twins of the physical network. For example, in a digital twin scenario, an MLLM could update the twin based on visual changes in the physical environment (e.g., new construction obstructing line-of-sight) and adjust the network simulation parameters accordingly.

In conclusion, the application of MLLMs in wireless systems and network management is a burgeoning field that promises to realize the vision of AI-native 6G networks. By leveraging their ability to synthesize multi-modal sensing data, MLLMs enable unprecedented levels of automation, security, and efficiency. While challenges regarding computational efficiency and robustness remain, the ongoing research into efficient architectures and agentic capabilities suggests that MLLMs will become indispensable components of future telecommunications infrastructure. As the industry moves toward more open and disaggregated networks like O-RAN, the flexibility and generalization capabilities of MLLMs will be the key to unlocking the full potential of next-generation wireless systems.

### 7.8 Recommendation Systems

The integration of Multimodal Large Language Models (MLLMs) into recommendation systems represents a paradigm shift from traditional collaborative filtering and content-based methods towards more holistic, user-centric understanding. While conventional recommender systems often rely on structured data like user-item interaction matrices or shallow text features, MLLMs offer the capability to process and reason over a rich tapestry of unstructured multimodal data. This subsection investigates the burgeoning field of multimodal sequential recommendation, where user preferences are inferred from diverse data streams including images, videos, and text, ultimately enabling more accurate and context-aware suggestions.

### The Evolution towards Multimodal Sequential Recommendation

Traditional recommendation models, such as Matrix Factorization or Neural Collaborative Filtering, primarily focus on historical interaction sequences. However, they often struggle to capture the semantic richness of the items or the nuanced context of user behavior. The advent of Large Language Models (LLMs) introduced the ability to understand textual descriptions and user reviews, but the true nature of user consumption—especially in domains like e-commerce, social media, and entertainment—is inherently multimodal. A user’s preference is often dictated by the visual appeal of a product, the narrative of a video, or the combination of both.

MLLMs bridge this gap by serving as powerful feature extractors and reasoning engines. By aligning visual and textual modalities into a unified semantic space, these models can generate representations that capture the complex interplay between what a user sees and what they read. This allows for a shift from ID-based sequential modeling to content-aware sequential modeling. For instance, in a fashion recommendation scenario, an MLLM can analyze a sequence of images the user interacted with, understand the visual attributes (e.g., "floral pattern," "oversized fit"), and correlate them with textual queries or reviews to predict the next item of interest.

### Multimodal Sequential Recommendation with MLLMs

The core of this advancement lies in the ability of MLLMs to handle multimodal sequential inputs. Unlike standard sequential recommenders that process item IDs, multimodal sequential recommenders process the raw content of the items. The MLLM acts as a bridge, converting high-dimensional visual pixels and unstructured text into dense embeddings that preserve semantic meaning.

One prominent approach involves utilizing MLLMs to generate rich item representations. For example, models like [5] demonstrate the feasibility of unifying eight different modalities, including images and text, into a single embedding space. In the context of recommendation, such a unified encoder can process a user's history consisting of product images and associated descriptions simultaneously. The resulting sequence of multimodal embeddings is then fed into a sequential model (e.g., a Transformer or a state-space model) to predict the next item. This method overcomes the limitations of sparse ID representations and enables "cold-start" recommendations for new items, as the model can infer similarity based on content rather than past interactions.

Furthermore, the reasoning capabilities of MLLMs allow for the extraction of implicit user intents. Instead of merely matching patterns, MLLMs can "understand" why a user liked a specific item. For instance, if a user frequently interacts with images of hiking boots and reads reviews about "durability," the MLLM can infer a preference for outdoor activities and rugged gear, even if the user has never explicitly stated this intent. This level of semantic reasoning is a significant leap forward compared to previous methods that relied on statistical correlations.

### Efficiency and Architecture: The Role of State Space Models

A major challenge in applying MLLMs to sequential recommendation is the computational cost. Recommendation systems often require processing long sequences of user history (hundreds or thousands of interactions), and the quadratic complexity of standard Transformer attention mechanisms becomes a bottleneck. This is where recent architectural innovations, particularly State Space Models (SSMs) like Mamba, play a crucial role.

Models such as [24] and [117] highlight the shift towards linear complexity architectures that can handle long contexts efficiently. In a sequential recommendation setting, where the input sequence length corresponds to the user's interaction history, the ability of Mamba-based models to process nearly a thousand images (or items) on a single GPU is transformative. [21] further validates that replacing the Transformer backbone with Mamba allows for fast inference and linear scaling in sequence length. This efficiency enables real-time updating of user profiles and recommendation generation, which is critical for user engagement in dynamic environments.

Moreover, the hybrid architectures proposed in [118] combine Mamba blocks with Transformer blocks to balance efficiency with the ability to capture complex global dependencies. This is particularly relevant for recommendation, where local patterns (recent items) and global patterns (long-term preferences) both matter. By leveraging these efficient architectures, systems can scale to handle millions of users and items without incurring prohibitive computational costs.

### Mitigating Modality Gaps and Enhancing Alignment

The effectiveness of multimodal sequential recommendation relies heavily on how well the visual and textual modalities are aligned with the user's intent. A common issue in MLLMs is the "modality gap," where visual features and language features do not interact optimally. In recommendation, this can lead to misalignment between what the user sees and the textual context provided.

Research into visual projectors and alignment modules is addressing this. For example, [16] proposes a coarse-to-fine scheme to generate condensed visual tokens, ensuring that fine-grained visual details (crucial for distinguishing similar products) are preserved while reducing redundancy. In a recommendation context, this means that subtle visual differences between two fashion items can be captured and utilized for precise ranking. Similarly, [15] emphasizes the preservation of local context from visual features, which is vital for spatial understanding—such as identifying the specific part of a furniture item that a user might be interested in.

Furthermore, [119] argues against "double abstraction" (where the projector and LLM both abstract the visual features), proposing instead to compress tokens at the patch level and let the LLM handle semantic abstraction. This approach ensures that the recommendation model has access to the most relevant visual information without losing critical details during the projection phase.

### Agentic Reasoning and Tool Use in Recommendations

Beyond simple retrieval, MLLMs enable agentic behaviors in recommendation systems. As highlighted in [120], extending the context window allows the agent to consider a much broader history of user behavior. An MLLM agent can be tasked with not just recommending an item, but explaining *why* it is being recommended, comparing it to previous items in the user's history, and even generating a narrative summary of the user's evolving taste.

This agentic capability is supported by frameworks that allow MLLMs to utilize external tools. In a sophisticated recommendation scenario, an MLLM agent could access a database of current trends, check inventory levels, or even query external knowledge graphs to validate the relevance of a recommendation. For example, if a user is looking for a gift, the agent could analyze the multimodal content of potential gifts, cross-reference it with the recipient's social media profile (processed via MLLM), and generate a tailored suggestion with a rationale.

### Challenges and Future Directions

Despite the promise, several challenges remain. One significant issue is the potential for hallucination, where the model might recommend items based on non-existent features or incorrect reasoning. In a recommendation system, this could lead to user mistrust. Mitigation strategies involve grounding the recommendations strictly in the provided multimodal content and utilizing retrieval-augmented generation (RAG) techniques to verify facts against a knowledge base.

Another challenge is the computational cost of inference. While models like [121] and [22] offer linear complexity, deploying these models at the scale of major e-commerce platforms requires significant optimization. Techniques such as [122] and [123] (as surveyed in [124]) are essential for making these models practical.

Privacy is also a concern. MLLMs capable of processing user-generated images and text must be designed to respect privacy constraints. Federated learning approaches or on-device processing of sensitive multimodal data might be necessary to maintain user trust.

Looking forward, the integration of "World Models" and "Embodied AI" principles could allow recommendation systems to simulate user interactions in a virtual environment before making a recommendation. Furthermore, the move towards "Unified Multimodal Understanding and Generation" suggests that future systems might not only recommend existing items but also generate novel multimodal content (e.g., a custom image of a product configuration) tailored to the user's specific sequential history.

In conclusion, the integration of MLLMs into recommendation systems transforms them from passive retrievers to active, reasoning agents. By leveraging the rich semantic understanding of multimodal data, efficient architectures for long sequences, and advanced alignment techniques, these systems can offer a level of personalization and relevance previously unattainable. The journey involves navigating the trade-offs between model capability, efficiency, and safety, but the trajectory points towards a future where recommendations are truly multimodal, context-aware, and deeply aligned with human intent.

## 8 Challenges: Robustness, Safety, and Hallucination

### 8.1 Adversarial Robustness and Attack Surfaces

The rapid advancement of Multimodal Large Language Models (MLLMs) has unlocked unprecedented capabilities in understanding and generating content across text, images, and other modalities. However, this increased capability also expands the model's attack surface, making them susceptible to a variety of adversarial threats. Adversarial robustness refers to the model's ability to maintain correct and safe behavior when inputs are intentionally manipulated to cause errors or misalignment. In the context of MLLMs, these attacks can originate from either the visual or textual modality, or a malicious combination of both, exploiting the complex integration mechanisms that bridge these distinct data types.

One of the most prominent vulnerabilities lies in the visual modality. Unlike discrete text tokens, image data is continuous and high-dimensional, allowing for subtle, imperceptible perturbations that can drastically alter a model's output. These "visual adversarial examples" are crafted by adding a carefully calculated noise pattern to an image, which is often invisible to the human eye but can mislead the MLLM's vision encoder. This can cause the model to misclassify objects, misread text, or generate incorrect descriptions. More alarmingly, recent research has demonstrated that these visual perturbations can be weaponized to bypass the safety alignment of MLLMs. By embedding adversarial patterns in seemingly benign images, attackers can "jailbreak" the model, prompting it to generate harmful, biased, or otherwise prohibited content. This reveals a critical weakness: the safety guardrails, often trained primarily on textual data, may not be robust to adversarially manipulated visual inputs. The continuous nature of images provides a much larger and more complex search space for attackers compared to the discrete token space of text, making defense significantly more challenging.

The risks are compounded by the fact that MLLMs often process visual information through encoders like CLIP or ViT, which are themselves known to have adversarial vulnerabilities. An attacker does not need to attack the entire MLLM; they can focus their efforts on the vision component. The adversarial examples generated to fool the vision encoder can then propagate through the projection layer to the LLM backbone, effectively poisoning the entire reasoning process. For instance, an adversarial image could cause the vision encoder to produce features that are highly correlated with a concept like "bomb" or "violence," even if the image contains no such content. The LLM, receiving these misleading features, would then be primed to generate text related to that concept, regardless of the user's benign prompt or the model's safety training. This highlights a fundamental disconnect and potential point of failure in the alignment between the visual and linguistic components of MLLMs.

Furthermore, the attack surface is not limited to imperceptible perturbations. Attackers can also exploit more perceptible but semantically confusing visual inputs. For example, complex visual scenes with occluded objects, or abstract patterns can cause the model to hallucinate or make factual errors. Adversarial attacks can also target the model's understanding of spatial relationships or compositional logic. By manipulating an image in a way that alters the relationships between objects, an attacker can force the model to make incorrect logical deductions. This is particularly dangerous in applications that require precise visual understanding, such as autonomous systems or medical diagnosis, where a single misinterpretation could have severe consequences.

In addition to visual attacks, textual adversarial attacks remain a potent threat to MLLMs. These attacks leverage the linguistic understanding of the LLM backbone. Classic NLP attack strategies like prompt injection, where a malicious instruction is hidden within a seemingly innocuous text prompt, are still effective. Attackers can also use obfuscation, synonym replacement, or complex sentence structures to confuse the model and steer its output. When combined with visual inputs, these textual attacks can become even more potent. For example, an attacker could present an image of a benign scene alongside a text prompt that contains a hidden command. The model, tasked with integrating both inputs, might be tricked into executing the hidden command, effectively bypassing user intent and safety protocols.

A particularly insidious class of attacks involves the combination of visual and textual adversarial examples. In these "multimodal jailbreaks," an attacker might craft an image that, when paired with a specific text prompt, triggers a harmful response. The image itself might be harmless, and the text prompt might appear benign, but their combination creates a context that the model's safety filters fail to recognize as malicious. This exploits the "mismatched generalization" phenomenon, where the model's alignment is robust to attacks in a single modality but fails when threats are distributed across modalities. The model's attempt to synthesize information from both sources can be turned against it, creating a vulnerability that is unique to multimodal systems.

The evaluation of adversarial robustness in MLLMs is an active area of research, but it faces significant challenges. Standard benchmarks for unimodal models are insufficient. New benchmarks are needed that specifically test the model's resilience to combined modality attacks. For instance, benchmarks like [29] are designed to evaluate the susceptibility of MLLMs to adversarial and jailbreak attacks, providing a standardized way to measure and compare model robustness. However, the dynamic nature of adversarial attacks means that as soon as a defense is proposed, a new, more sophisticated attack is likely to emerge. This creates a continuous cat-and-mouse game between developers and attackers.

To mitigate these threats, researchers are exploring a range of defense mechanisms. One approach is adversarial training, where models are trained on a mixture of clean and adversarially perturbed data. This can help the model learn to recognize and resist common attack patterns. However, adversarial training is computationally expensive and may not generalize to novel, unseen attacks. Another promising direction is the use of input sanitization or pre-processing techniques, such as image transformations or denoising filters, that can remove adversarial perturbations before they reach the model. However, these methods must be carefully designed to avoid degrading the model's performance on legitimate inputs.

More advanced defenses involve architectural changes. For example, some researchers propose using ensemble methods, where multiple models with different architectures or training regimes are used to cross-validate outputs, making it harder for an adversarial example to fool all models simultaneously. Others are exploring the use of certified defenses, which provide mathematical guarantees of robustness within a certain perturbation budget, though these are currently limited to smaller models and specific threat models. A particularly interesting area of research is the use of LLMs themselves to detect adversarial inputs. By prompting an LLM to analyze the consistency and plausibility of the MLLM's output, it may be possible to flag suspicious responses that are likely the result of an attack.

In conclusion, adversarial robustness is a critical challenge for the safe and reliable deployment of MLLMs. The continuous nature of visual data, combined with the complex interplay between modalities, creates a rich and largely unexplored landscape of potential attack vectors. From subtle pixel-level perturbations that bypass safety filters to complex multimodal jailbreaks that exploit the model's reasoning process, the threats are diverse and evolving. While benchmarks and initial defense strategies are emerging, the field is still in its early stages. Building truly robust MLLMs will require a concerted effort to develop new evaluation methodologies, architectural innovations, and training paradigms that account for the unique vulnerabilities of multimodal systems. Without such efforts, the promise of generalist AI assistants may be undermined by their susceptibility to manipulation and misuse.

### 8.2 Safety Alignment Failures and Jailbreaking

### 8.2 Safety Alignment Failures and Jailbreaking

The rapid advancement of Multimodal Large Language Models (MLLMs) has brought unprecedented capabilities to AI systems, enabling them to process and reason across text, images, and other modalities. However, this increased power introduces significant safety risks, particularly regarding the robustness of their alignment mechanisms. Safety alignment in MLLMs aims to ensure that models refuse to generate harmful, unethical, or illegal content. Despite these efforts, MLLMs remain highly susceptible to "jailbreaking" attacks—sophisticated methods designed to bypass these safety guardrails. This subsection investigates the nature of these alignment failures, focusing on how multimodal inputs can be exploited to elicit prohibited responses. We explore the underlying mechanisms, such as competing objectives and mismatched generalization, and review the specific attack vectors that leverage the unique properties of visual and textual data. These vulnerabilities are closely related to the broader challenge of adversarial robustness, as jailbreaking can be considered a specific form of adversarial attack that targets the model's safety alignment rather than just its predictive accuracy.

#### The Anatomy of Alignment Failures in MLLMs

At the core of the safety alignment problem in MLLMs is the challenge of integrating robust, text-based safety protocols with a visual modality that is inherently continuous and less understood by the language model "brain." Traditional safety alignment in LLMs often relies on Reinforcement Learning from Human Feedback (RLHF) or supervised fine-tuning on refusal datasets. When extended to MLLMs, this alignment is typically applied to the language component, while the vision encoder and the projection module that bridges the modalities are often not subjected to the same rigorous safety training. This architectural separation creates a fertile ground for adversarial attacks.

A primary theoretical framework for understanding these failures is the concept of "competing objectives" and "mismatched generalization," as identified in research on LLMs and extended to the multimodal domain. Competing objectives arise when a model is prompted with a request that pits its helpfulness directive (to follow instructions) against its safety directive (to refuse harmful requests). In the multimodal context, this conflict is amplified. An attacker can craft an image that appears benign or even helpful on the surface (satisfying the helpfulness objective) but contains subtle cues or is paired with a text prompt that, when interpreted by the model, triggers a harmful response (violating the safety objective). The model, lacking a unified understanding of safety across both modalities, often prioritizes the immediate, explicit instruction over the abstract safety constraint.

Mismatched generalization refers to the phenomenon where a model's safety training fails to generalize to inputs that differ, even slightly, from its training distribution. MLLMs are typically trained on vast, general-purpose image-text pairs, but their safety alignment data is often much narrower, focusing on explicit textual harms. Consequently, they fail to generalize safety principles to visual inputs. An image that a human would easily recognize as depicting a dangerous activity or a sensitive scenario might not trigger the model's safety filters because the visual patterns were not represented in its alignment training data. This vulnerability is exacerbated by the fact that visual information can encode complex concepts that are difficult to filter using simple text-based heuristics. As noted in [1], the surprising emergent capabilities of MLLMs also suggest a potential path to artificial general intelligence, but these same capabilities can be co-opted for malicious purposes if safety is not robustly integrated.

#### Multimodal Attack Vectors: Exploiting the Visual Modality

Jailbreaking attacks on MLLMs are uniquely powerful because they can exploit the visual channel, which is often less scrutinized by safety mechanisms than the textual channel. Attackers have developed several strategies to craft multimodal inputs that bypass safety guardrails.

1.  **Adversarial Images:** This is one of the most direct forms of attack. Adversarial images involve making imperceptible (to humans) perturbations to an image that cause the MLLM to misinterpret its content and violate its safety guidelines. For example, an attacker might generate an image that, when processed by the vision encoder, produces visual features that are highly correlated with the tokens of a harmful instruction. The language model then "sees" a visual representation of a harmful prompt and proceeds to fulfill it, believing it is a legitimate user request. This method effectively translates a harmful text prompt into a visual format that the model's safety filters are not equipped to handle. The continuous nature of image data makes it a much richer and more flexible medium for such attacks compared to the discrete tokens of text. Research in [106] implicitly highlights this vulnerability by discussing how models generate outputs inconsistent with visual content; attackers can reverse-engineer this principle to *force* the generation of specific, harmful content by manipulating the visual input.

2.  **Benign-Looking Images with Malicious Text Prompts:** A more subtle approach involves pairing a visually innocuous or abstract image with a carefully engineered text prompt. The image itself contains no harmful content, so it passes through any visual safety checks. The malicious intent is encoded entirely in the text, but the presence of the image serves as a distraction or a context-setter that misleads the model's safety alignment. For instance, an attacker could present an image of a historical painting and ask a question that, in the context of the painting, seems academic but is actually a prompt for generating dangerous instructions. The model's focus on interpreting the image and providing a helpful response can lower its guard against the malicious intent embedded in the text. This exploits the model's primary objective to be a helpful multimodal assistant.

3.  **Image-to-Text Transformation Attacks:** Some attacks leverage the model's own capabilities against it. An attacker might provide an image containing text that, when read by the model via OCR capabilities, forms a harmful instruction. The model is then instructed to "read the text in the image and follow its instructions." This creates a chain of reasoning where the model first performs a perception task (reading) and then a generation task (following instructions), effectively bypassing direct safety checks on the initial prompt. This is a form of indirect prompt injection, where the harmful payload is delivered through the visual modality.

#### The Role of Competing Objectives and Mismatched Generalization

The concepts of competing objectives and mismatched generalization are not just theoretical; they are the fundamental mechanisms exploited by the attacks described above.

*   **Competing Objectives in Practice:** When a user provides an adversarial image and a prompt like "Describe this image in detail," the model's helpfulness objective is to provide a rich, descriptive caption. However, if the image has been perturbed to visually encode a harmful concept, the "detailed description" might involve generating that harmful content. The model is caught between being helpful (providing a detailed response) and being safe (refusing to generate harmful content). In many cases, the helpfulness objective, being more directly tied to the model's core function, wins out. This is a classic example of how safety guardrails can be subverted by framing the harmful request as a standard task.

*   **Mismatched Generalization as the Root Cause:** The primary reason these attacks succeed is that MLLMs have not generalized their safety training to the visual domain. Their safety training data likely contains countless examples of harmful text prompts and appropriate refusals, but it contains very few, if any, examples of adversarial images or benign images paired with malicious text. As a result, the model's "safety manifold" in its high-dimensional representation space does not extend to these multimodal adversarial regions. The model simply does not recognize the visual patterns as being associated with a safety violation. This is a critical gap, as highlighted in [2], which notes the need for comprehensive evaluation across different capabilities, including trustworthiness. Current evaluation benchmarks often fail to capture these nuanced multimodal jailbreaks, allowing models to be deployed with these fundamental generalization flaws.

#### Mitigation Strategies and The Path Forward

Addressing these safety alignment failures requires a multi-pronged approach that goes beyond simple text filtering. Mitigation strategies must be integrated into the core architecture and training pipeline of MLLMs.

1.  **Multimodal Safety Alignment Training:** The most direct solution is to incorporate multimodal data into the safety alignment process. This involves creating a dataset of harmful multimodal prompts (e.g., adversarial images, benign images with malicious text) and training the model to refuse them using techniques like RLHF or Direct Preference Optimization (DPO). This would help the model generalize its safety principles to the visual domain. However, as noted in [34], creating high-quality, diverse, and complex instruction data is a significant challenge. Generating a robust multimodal safety dataset is even more difficult.

2.  **Input Sanitization and Anomaly Detection:** Before the multimodal input reaches the core LLM, it can be passed through a separate safety checker. This could involve a specialized vision model trained to detect adversarial perturbations or a system that analyzes the combined semantic meaning of the image and text for potential harm. However, this adds complexity and can be circumvented by novel attack methods.

3.  **Decoupled Vision Systems and Guardrails:** Some research proposes decoupling the vision system from the language model to allow for more specialized processing [125]. A similar principle could be applied to safety, where a dedicated "safety vision module" acts as a gatekeeper, analyzing the visual input for potential threats before it is ever passed to the main LLM for processing.

4.  **Adaptive Attention and Computational Redundancy:** Techniques like adaptive attention [126] that reduce computational load by focusing on critical parts of the input could be repurposed for safety. By identifying and potentially down-weighting or flagging anomalous visual features, the model could be made more robust to adversarial noise.

In conclusion, safety alignment failures and jailbreaking in MLLMs represent a critical and evolving threat landscape. The unique properties of the visual modality—its continuous nature and the model's mismatched generalization of safety principles to it—create fertile ground for attacks that exploit competing objectives. While the field has made progress in understanding these vulnerabilities, particularly through the lens of hallucination analysis [106], robust and scalable defenses are still in their infancy. Future research must focus on developing inherently multimodal safety alignment techniques and creating comprehensive evaluation benchmarks that can effectively probe these complex failure modes, ensuring that the path toward more capable AI does not come at the cost of safety and trustworthiness. This challenge is a key focus of ongoing research in the field, as reflected in the broader discussion of adversarial robustness and safety in MLLMs.

### 8.3 The Pervasive Issue of Hallucination

### 8.3 Hallucination in Multimodal Large Language Models

The pervasive issue of hallucination represents one of the most significant challenges undermining the reliability and trustworthiness of Multimodal Large Language Models (MLLMs). This phenomenon, where models generate outputs inconsistent with provided visual content or fabricate entities, attributes, or relationships, is a critical failure mode that directly relates to the safety alignment and robustness issues discussed previously. While unimodal Large Language Models (LLMs) are known to hallucinate by generating plausible-sounding but factually incorrect text, the problem is exacerbated in multimodal settings due to the inherent complexity of aligning two distinct modalities and the model's reliance on statistical correlations rather than genuine visual understanding. As MLLMs are increasingly deployed in high-stakes domains such as healthcare, autonomous driving, and content moderation, understanding and mitigating these hallucinations is critical for ensuring their safe and responsible use.

Hallucinations in MLLMs can be broadly categorized into three primary types: object hallucinations, attribute hallucinations, and relation hallucinations. Object hallucinations occur when the model describes or identifies objects that are not present in the image. For instance, when presented with an image of a cat on a couch, a model might erroneously mention a dog or a person, driven by the statistical likelihood of such co-occurrences in its training data. Attribute hallucinations involve the misassignment of properties to existing objects, such as describing a red apple as green or a wooden table as metallic. Relation hallucinations are perhaps the most subtle and challenging, where the model infers incorrect spatial, temporal, or causal relationships between objects, such as claiming a person is holding a cup when their hands are empty or that a car is driving on a sidewalk. These errors stem from a fundamental disconnect between the model's linguistic priors and the visual evidence.

The underlying causes of hallucinations in MLLMs are multifaceted, but they primarily originate from the "modality gap" and the models' reliance on statistical patterns learned during pre-training. The modality gap refers to the difficulty in mapping visual features extracted by a vision encoder (e.g., ViT, CLIP) into the semantic space of the language model. This gap is often bridged by a relatively simple projection layer (e.g., a linear projector or Q-Former), which may fail to capture the fine-grained details necessary for precise visual grounding. Consequently, the LLM backbone, which is the primary driver of text generation, often defaults to its powerful linguistic priors and statistical biases learned from vast text corpora, effectively "ignoring" the visual signals when they are ambiguous or insufficient. This leads to a situation where the model generates text that is fluent and contextually plausible based on its language modeling objective, but visually inaccurate. This phenomenon is particularly evident in models that prioritize fluency over factual grounding, a trade-off that is difficult to balance.

Recent research has provided empirical evidence for the prevalence of these issues. The study "Investigating the Catastrophic Forgetting in Multimodal Large Language Models" [127] highlights that fine-tuning MLLMs can lead to a loss of generalizability and an increase in hallucinations, even when the vision encoder remains frozen. This suggests that the alignment process itself can be fragile, causing the model to over-rely on the text generation capabilities at the expense of visual fidelity. Furthermore, the work "Beyond Specialization Assessing the Capabilities of MLLMs in Age and Gender Estimation" [14] demonstrates that even powerful general-purpose MLLMs struggle with specialized tasks requiring precise visual perception, often generating confident but incorrect answers, which is a form of hallucination. The model's inability to accurately perceive specific visual attributes leads to fabricated outputs that lack grounding.

The architectural design choices also play a crucial role in mitigating or exacerbating hallucinations. For example, the "Mixture of Encoders" approach explored in "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders" [23] aims to enhance visual perception by combining multiple vision encoders at different resolutions. By providing richer and more detailed visual tokens, such architectures can reduce the modality gap and provide the LLM with more accurate information, thereby decreasing object and attribute hallucinations. Similarly, the "Dense Connector" introduced in "Dense Connector for MLLMs" [128] leverages multi-layer visual features from the vision encoder, not just the final layer, to inject more comprehensive visual context into the LLM. This richer feature representation helps the model ground its responses more accurately in the visual input, reducing the likelihood of fabricating details that are not supported by the image.

Furthermore, the efficiency of the vision-language projector and the handling of visual tokens are critical. The "TokenPacker" method [16] proposes a coarse-to-fine scheme to compress visual tokens while preserving essential details. By reducing the number of tokens, it might seem counterintuitive to reduce hallucinations, but the key is that TokenPacker aims to retain the most salient information, preventing the LLM from being overwhelmed by redundant or noisy visual tokens that could distract from the core visual content. Conversely, models that simply reduce token count without careful preservation of information risk increasing hallucinations by removing crucial visual cues. The "Visual Tokens Withdrawal" technique [19] further illustrates this point by removing visual tokens in deeper layers of the model. While this is done for efficiency, it relies on the premise that visual information has already been integrated into the text tokens in earlier layers. If this integration is imperfect, it could lead to hallucinations in the later stages of generation as the model relies solely on the text representation.

The reliance on statistical patterns is a root cause that transcends specific architectures. MLLMs are trained on massive datasets of image-text pairs, where the text often contains rich descriptions that may include objects or attributes not strictly visible in the image, or conversely, omit details that are present. The model learns these statistical regularities. For example, if the training data frequently associates "beach" images with "ocean" and "sand," the model might hallucinate these elements even if the image only shows a beach with rocks. This is a manifestation of the model learning a "concept" rather than perceiving the specific instance. The challenge is that the model's objective during pre-training is typically to predict the next text token, not to verify the visual accuracy of the prediction. This misalignment between the training objective and the goal of visual grounding is a fundamental source of hallucination.

To address these challenges, researchers are exploring various mitigation strategies. One approach involves improving the quality and diversity of training data, as highlighted in "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective" [10]. By curating datasets that explicitly require fine-grained visual understanding and by using synthetic data generation to create challenging examples, models can be trained to be more visually attentive. Another direction is the development of more sophisticated alignment techniques that go beyond simple linear projections. For instance, "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models" [36] proposes dynamic tuning of the projector and LLM parameters using visual and language guidance. This adaptive approach allows the model to better modulate the influence of visual versus linguistic information, potentially reducing hallucinations by preventing the LLM's priors from dominating when visual evidence is strong.

The evaluation of hallucinations is also an active area of research. Benchmarks specifically designed to probe for hallucinations, such as those mentioned in "A Survey on Benchmarks of Multimodal Large Language Models" [29], are essential for measuring progress. These benchmarks often include "adversarial" or "trick" images designed to induce hallucinations, or they require models to perform detailed counting or attribute verification tasks that are sensitive to factual accuracy. The use of "LLM-as-a-Judge" for evaluation, as discussed in "Revisiting Multi-Modal LLM Evaluation" [129], offers a scalable way to assess the factual consistency of model outputs, though it introduces its own biases.

In conclusion, hallucination is a pervasive and complex issue in MLLMs, rooted in the modality gap, architectural limitations, and the statistical nature of their training. It manifests as object, attribute, and relation errors that compromise the models' reliability. While significant progress has been made through architectural innovations like dense connectors and token packers, and through improved training strategies, a complete solution remains elusive. Future research must focus on developing models that can explicitly verify their visual grounding, perhaps through iterative reasoning or the integration of external verification tools, and on creating evaluation frameworks that robustly measure and incentivize factual accuracy over fluent fabrication. Until then, users of MLLMs must remain cognizant of this fundamental limitation and the potential for models to confidently generate plausible falsehoods.

### 8.4 Evaluation Benchmarks for Safety and Robustness

As Multimodal Large Language Models (MLLMs) are increasingly deployed in real-world applications, the imperative to evaluate their safety and robustness has become a central concern in the research community. The unique vulnerabilities introduced by the integration of visual and textual modalities necessitate specialized evaluation frameworks that go beyond traditional accuracy metrics. These frameworks must probe the models' susceptibility to adversarial attacks, jailbreaking attempts, and their propensity to generate harmful or misleading content. A comprehensive survey of the landscape reveals a growing ecosystem of benchmarks designed to stress-test these capabilities, providing a quantitative basis for assessing trustworthiness [29]. This subsection reviews the most prominent benchmarks in this domain, focusing on those designed to evaluate adversarial robustness, jailbreak vulnerabilities, and the pervasive issue of hallucination.

### Adversarial Robustness and Jailbreak Benchmarks

One of the most critical areas of safety evaluation is assessing an MLLM's resilience against adversarial attacks, where malicious actors craft inputs specifically designed to induce erroneous or harmful behavior. Unlike text-based models where inputs are discrete tokens, the continuous nature of image data presents a unique and expansive attack surface. Adversaries can introduce imperceptible perturbations to images that, when combined with carefully engineered text prompts, can bypass safety alignments that would otherwise be effective. To systematically study these vulnerabilities, researchers have developed benchmarks that simulate such attack scenarios.

A seminal work in this area is **MM-SafetyBench**, a comprehensive benchmark designed to evaluate the safety of MLLMs against a wide array of adversarial attacks. It comprises thousands of adversarial examples that test the model's ability to resist generating harmful content when prompted with unsafe instructions embedded within visual or textual contexts. The benchmark highlights how visual inputs can be used to "jailbreak" a model's safety training, exploiting the model's difficulty in jointly understanding the harmful intent of the text and the benign appearance of the image [2].

Building on this, **JailBreakV-28K** is another large-scale benchmark that focuses specifically on jailbreaking MLLMs. It contains 28,000 test cases across various risk categories, including illegal activities, hate speech, and misinformation. The benchmark employs a diverse range of jailbreak strategies, such as using adversarial images, leveraging context from multiple images, and employing complex logical reasoning to circumvent safety filters. Findings from evaluations on JailBreakV-28K reveal that even state-of-the-art MLLMs like GPT-4V and Gemini are not immune to these attacks, demonstrating significant gaps in their safety alignment [2]. These benchmarks underscore the "competing objectives" and "mismatched generalization" issues, where models are trained to be helpful but not sufficiently robust against multimodal inputs designed to elicit harmful outputs [2].

Furthermore, the evaluation of adversarial robustness extends beyond jailbreaking to include robustness against more subtle manipulations. For instance, models are tested on their ability to maintain factual accuracy when presented with images containing slight alterations or misleading visual cues. The continuous nature of image data makes it particularly challenging to defend against such attacks compared to the discrete token space of text. Therefore, benchmarks that incorporate these adversarial examples are essential for developing defense mechanisms and understanding the true robustness of MLLMs.

### Hallucination Evaluation Benchmarks

Hallucination, the generation of outputs that are factually incorrect or not grounded in the provided multimodal input, remains a pervasive and critical challenge for MLLMs. This issue can be categorized into object hallucinations (inventing objects that do not exist in the image), attribute hallucinations (misattributing properties to existing objects), and relation hallucinations (falsely describing relationships between objects). To quantify and mitigate this problem, several benchmarks have been proposed.

**AMBER** is a prominent benchmark designed to provide a holistic evaluation of hallucination in MLLMs. It systematically assesses models across different dimensions of hallucination, including object existence, attribute binding, and object relationships. By providing a fine-grained analysis, AMBER allows researchers to pinpoint specific weaknesses in a model's visual grounding capabilities. For example, it can reveal whether a model is more prone to attribute hallucination (e.g., saying a "red" car is "blue") or object hallucination (e.g., mentioning a "dog" when there is none). This level of detail is crucial for developing targeted mitigation strategies [2].

Another notable benchmark is **CorrelationQA**, which focuses on evaluating factual hallucinations by testing the model's ability to correlate information between the image and the text. It presents scenarios where the visual and textual information are either consistent or contradictory, forcing the model to rely on the correct modality. This is particularly important as MLLMs can be overly reliant on their powerful language priors, leading them to generate plausible-sounding but visually inconsistent text [2]. The underlying causes of hallucination, such as the modality gap and the statistical patterns learned during training, are discussed in detail in the literature [2].

The development of these benchmarks is an active area of research, with new methods emerging to create more challenging and realistic test cases. For instance, some benchmarks generate synthetic data to create "unanswerable" questions, forcing the model to admit uncertainty rather than hallucinate. Others use adversarial filtering to select examples that are most likely to induce hallucinations. The insights gained from these evaluations are vital for advancing the field, as they provide a clear target for improvement and a standardized way to measure progress towards building more reliable and truthful MLLMs.

### Broader Safety and Trustworthiness Assessment

Beyond adversarial attacks and hallucinations, comprehensive safety evaluation encompasses a wider range of risks, including privacy leakage, toxic content generation, and the potential for the model to be used for malicious purposes. Benchmarks in this category often assess the model's alignment with human values and ethical principles.

For example, some benchmarks evaluate the model's ability to refuse to answer requests that violate privacy norms or generate harmful stereotypes. These evaluations often require a nuanced understanding of context and social cues, which can be challenging for MLLMs. The use of **LLM-as-a-Judge** methodologies, where another powerful LLM evaluates the responses of the target MLLM, has emerged as a scalable way to assess these qualitative aspects of safety and trustworthiness [95]. This approach allows for more dynamic and holistic evaluation compared to static multiple-choice benchmarks.

The importance of these evaluations is highlighted by the fact that MLLMs are being integrated into sensitive domains such as healthcare, finance, and autonomous systems, where failures can have severe consequences. Therefore, benchmarks that assess robustness in these specific domains are also emerging. For instance, benchmarks for medical imaging require models to be not only accurate but also robust against adversarial attacks that could lead to misdiagnosis.

In conclusion, the evaluation of safety and robustness is a multifaceted and rapidly evolving field. Benchmarks like **MM-SafetyBench**, **JailBreakV-28K**, and **AMBER** provide essential tools for quantifying the vulnerabilities of MLLMs to adversarial attacks, jailbreaking, and hallucination. As models become more capable and are deployed in increasingly critical applications, the development of more sophisticated and comprehensive evaluation frameworks will be paramount to ensuring their safe and responsible integration into society. The continuous cycle of identifying vulnerabilities through benchmarks, developing more robust models, and re-evaluating them on new, more challenging benchmarks is the key to building trustworthy multimodal AI systems.

### 8.5 Mitigation Strategies and Defense Mechanisms

The mitigation of safety risks, robustness vulnerabilities, and hallucinations in Multimodal Large Language Models (MLLMs) requires a multi-faceted approach. As these models become increasingly integrated into safety-critical domains, the development of effective defense mechanisms has shifted from simple post-hoc filtering to sophisticated, integrated strategies. These strategies can be broadly categorized into training-free interventions, training-based alignment techniques, and system-level architectural safeguards. This subsection surveys the landscape of these mitigation strategies, highlighting the diverse methodologies employed to enhance the trustworthiness of MLLMs.

**Training-Free Mitigation Strategies**

Training-free methods are highly attractive due to their computational efficiency and ability to be applied to pre-trained models without requiring expensive retraining or fine-tuning. These approaches typically intervene at inference time, either by transforming inputs or by constraining outputs.

One prominent strategy involves prompt engineering and "shield prompting." This technique augments the user input or the system prompt with explicit safety instructions, guiding the model to refuse harmful requests or avoid generating toxic content. While simple, these methods can be bypassed by adversarial inputs, leading to the development of more adaptive shielding mechanisms that dynamically adjust safety constraints based on the input modality. Another approach focuses on input transformation, where visual inputs are converted into a textual format before being processed by the LLM. By transducing images into text descriptions, these methods attempt to leverage the more mature safety alignment of text-only LLMs, effectively reducing the attack surface presented by the continuous nature of pixel data. This modality bridging can help mitigate risks where visual cues are used to bypass safety filters.

Furthermore, inference-time detection and filtering play a crucial role. Systems can analyze the model's output logits or internal activations to estimate the likelihood of harmful content. For instance, auxiliary classifiers can be deployed to flag outputs that violate safety policies. However, the effectiveness of these methods is often limited by the "competing objectives" problem, where the model is tasked with being helpful and harmless simultaneously, potentially leading to failures in either or both objectives.

**Training-Based Alignment and Robustness Techniques**

To build inherent robustness, training-based alignment techniques are essential. These methods aim to instill safety behaviors directly into the model's weights during the training process.

Reinforcement Learning from Human Feedback (RLHF) and its variants have been the cornerstone of aligning LLMs, and these principles are extended to MLLMs. By training on preference data where human annotators rank model outputs based on safety and helpfulness, models learn to internalize safety norms. However, the complexity of multimodal inputs makes collecting high-quality preference data significantly more challenging than for text.

Adversarial training is another critical technique for enhancing robustness against adversarial attacks. This involves training the model on specifically crafted adversarial examples designed to exploit vulnerabilities. By exposing the model to these "worst-case" inputs during training, the model's decision boundaries are hardened, making it more resilient to perturbations in both visual and textual inputs. This is particularly important for defending against visual adversarial examples that can jailbreak safety alignments. Research has shown that models with the same parameter count can exhibit vast differences in robustness based on architectural choices and training regimens [130]. This highlights the importance of incorporating robustness objectives directly into the architecture search and training pipeline.

Recent work has also explored parameter-efficient fine-tuning (PEFT) for safety alignment. Instead of full fine-tuning, techniques like LoRA (Low-Rank Adaptation) can be used to inject safety modules or adapt existing models to be more robust, reducing the computational cost of safety hardening.

**System-Level Architectural Safeguards and Multi-Agent Systems**

Beyond individual model training, system-level approaches leverage the power of multiple models or external tools to create a robust safety ecosystem.

External guardrails act as a safety net, independent of the core MLLM. These are typically smaller, specialized models trained to detect harmful content in the inputs and outputs of the main MLLM. If a guardrail detects a violation, it can block the input or intervene in the generation process. This separation of concerns allows for specialized safety monitoring without burdening the primary model.

A more advanced system-level approach involves multi-agent debate or collaboration. In these frameworks, multiple MLLM instances (or a single MLLM interacting with itself) critique and refine outputs. For example, one agent might generate a response, and a second "critic" agent evaluates it for safety, hallucinations, and adherence to policy. This iterative process can surface and correct errors that a single-pass generation would miss. This is analogous to "System 2" reasoning, where a more deliberate, reflective process is used to verify the intuitive outputs of "System 1". These multi-agent systems can also be used to detect subtle forms of hallucination and factual inconsistency by cross-referencing generated claims against external knowledge sources or by having agents debate the factual grounding of a statement.

Finally, the integration of MLLMs with Retrieval-Augmented Generation (RAG) frameworks serves as a mitigation strategy against factual hallucination. By grounding responses in retrieved, verifiable multimodal information, the model is less likely to fabricate facts. This is particularly relevant for applications in domains like healthcare or finance, where factual accuracy is paramount [131]. The use of external tools and APIs within an agentic framework allows the model to offload tasks to specialized verifiers, further enhancing the reliability and safety of the final output.

In conclusion, mitigating the risks associated with MLLMs is a complex challenge that cannot be solved by a single technique. A defense-in-depth strategy, combining training-free interventions, robust training paradigms, and sophisticated system-level architectures, is necessary to build safe and trustworthy multimodal AI systems.

## 9 Future Directions and Conclusion

### 9.1 Towards Unified Multimodal Understanding and Generation

The evolution of Multimodal Large Language Models (MLLMs) has predominantly been characterized by a bifurcation of capabilities: one stream focused on multimodal understanding (interpreting visual or auditory inputs to generate textual descriptions or answers) and another on multimodal generation (synthesizing images, video, or audio from textual prompts). However, a significant frontier in AI research is the convergence of these paradigms into unified frameworks capable of both perceiving and creating across modalities seamlessly. This subsection explores the transition toward such unified architectures, where the boundaries between understanding and generation blur, creating a holistic multimodal agent.

Historically, the separation between understanding and generation stems from the distinct nature of the underlying tasks. Understanding tasks, such as Visual Question Answering (VQA) or image captioning, rely heavily on encoders to map rich sensory data into a latent space that a language model can reason over. In contrast, generative tasks often utilize diffusion models or autoregressive transformers to synthesize high-dimensional data from noise or textual guidance. Early MLLMs, such as those following the LLaVA paradigm [1], primarily focused on connecting a frozen vision encoder to a frozen LLM via a simple projection layer, enabling strong visual understanding but lacking the ability to generate visual content. Conversely, generative models like Stable Diffusion or DALL-E excel at creation but possess limited reasoning capabilities regarding the content they produce.

The drive toward unified models is motivated by the desire to create "omni" models that can function as universal interfaces. Recent efforts have attempted to bridge this gap by integrating Large Language Models (LLMs) with diffusion models. In these architectures, the LLM acts as the central cognitive engine, processing multimodal inputs and generating textual representations that serve as conditions for diffusion-based image or video generators. This approach allows for complex, reasoning-intensive generation tasks that were previously difficult for text-to-image models alone. For instance, a unified model could take a complex instruction involving multiple constraints and visual references, reason about them using its language capabilities, and then execute the generation process. The survey [6] highlights this trend, noting that LLMs are increasingly playing various roles in multimodal generation, moving beyond simple text conditioning to act as controllers for complex editing and synthesis pipelines.

A key architectural innovation in this transition is the development of end-to-end frameworks that share parameters or feature spaces between understanding and generation modules. Models like NExT-GPT represent a pioneering step in this direction. By utilizing a multimodal transformer architecture that can handle both multimodal inputs and outputs, these models demonstrate the feasibility of a single system that can "see," "think," and "speak" (or "draw"). The core challenge lies in aligning the representation spaces of different modalities not just for input (which has been the focus of earlier MLLMs), but also for output. This requires a unified tokenization strategy where visual or acoustic tokens can be generated autoregressively alongside text tokens, or where the LLM’s hidden states can directly modulate a diffusion process without losing semantic fidelity.

However, unifying understanding and generation introduces significant technical hurdles, primarily concerning the balancing of distinct objectives. Understanding is typically an inference task driven by cross-entropy loss on text prediction, while generation is a synthesis task often relying on diffusion losses or variational objectives. Training a single model on these disparate tasks can lead to catastrophic interference, where optimizing for generation degrades reasoning performance, and vice versa. To mitigate this, researchers are exploring modular architectures that preserve modality-specific strengths while sharing a common reasoning backbone. The work [98] suggests that modality collaboration is key; rather than treating generation as a separate module, shared functional modules can facilitate knowledge transfer between text and visual tasks, potentially improving performance in both domains.

Furthermore, the transition to unified models necessitates a rethinking of data pipelines. Traditional datasets for understanding (e.g., LAION, COCO) consist of static image-text pairs, whereas generative training requires paired prompts and synthetic or real images. Unified training requires massive, interleaved datasets containing instructions for both perception and creation. The survey [10] emphasizes the co-evolution of data and models, suggesting that high-quality, diverse data is essential for unlocking these emergent capabilities. Specifically, data that mixes descriptive and generative instructions is crucial for teaching the model when to "listen" and when to "create."

Another critical aspect is the efficiency of these unified models. As noted in [132], the computational cost of running both an LLM and a diffusion model is prohibitive. Unified architectures aim to reduce this overhead by sharing the heavy lifting of the transformer layers. For example, instead of passing hidden states through a separate diffusion U-Net, the unified model might use the LLM’s residual stream to guide a lightweight decoder. This approach aligns with the findings in [3], which emphasize that the design of the vision-language connector and the integration of interleaved data are more critical than simply scaling parameters. By optimizing the architecture for both tasks, we can achieve "Data-Efficient and Compute-Efficient" performance [108], making unified models more accessible.

The implications of successful unified models are profound. They pave the way for "Agentic" systems that can not only perceive the environment but also manipulate it through generation. For instance, an agent could analyze a user's sketch (understanding), refine it into a photorealistic image (generation), and then explain the changes made (reasoning). This is a step towards the "World Models" discussed in future directions, where the model simulates outcomes before executing them. The paper [101] illustrates this potential by unifying perception, understanding, and generation within a single framework, using a "super link" mechanism to connect the LLM with various task decoders.

However, significant challenges remain in balancing the distinct objectives of understanding and generation. Understanding requires high-level abstraction and semantic consistency, while generation requires high-fidelity detail and adherence to physical laws (e.g., lighting, physics). A unified model must navigate the trade-off between semantic reasoning (which LLMs excel at) and pixel-level precision (which diffusion models excel at). Current research suggests that a hybrid approach, perhaps utilizing a "mixture of experts" [96] or specialized adapters, might be necessary. By routing inputs to specific experts—visual understanding experts for VQA and visual generation experts for painting—the model can maintain high performance in both domains without interference.

In conclusion, the move towards unified multimodal understanding and generation represents the next paradigm shift in MLLM research. It requires not just the mechanical integration of existing models, but a fundamental architectural redesign that fosters modality collaboration and shared reasoning. As we continue to refine these frameworks, the distinction between consuming and creating content will increasingly vanish, leading to AI systems that are truly multimodal in the most holistic sense.

### 9.2 Scaling Laws, Efficiency, and Mixture of Experts (MoE)

The relentless pursuit of more capable Multimodal Large Language Models (MLLMs) has been largely driven by the empirical observation that scaling up model parameters, dataset sizes, and compute budgets yields significant performance improvements. However, as these models grow exponentially in size, they encounter the formidable "Energy Wall"—a point where further scaling becomes prohibitively expensive in terms of training costs, inference latency, and environmental impact. This subsection explores the critical role of scaling laws in MLLM development, examining how architectural innovations like Mixture of Experts (MoE) and parameter-efficient fine-tuning (PEFT) strategies are essential for optimizing performance while managing the computational overhead associated with massive models.

**Scaling Laws and the "Energy Wall"**

The foundational principle of the scaling laws, originally articulated in the context of Large Language Models (LLMs), suggests that model performance scales predictably with increases in model size, data volume, and compute. In the multimodal domain, researchers have sought to establish similar laws to guide the development of MLLMs. The transition from unimodal to multimodal architectures introduces new variables, such as the choice of vision encoder, the resolution of input images, and the alignment mechanism between modalities. Understanding how these variables interact with scaling is crucial for efficient development.

Recent empirical studies have attempted to map out these scaling laws specifically for MLLMs. For instance, the **MM1** paper [3] provides a comprehensive analysis of how different architectural components and data choices impact scaling. Through careful ablations, the authors of [3] demonstrate that the image encoder, specifically its resolution and the resulting token count, has a substantial impact on performance, often more so than the design of the vision-language connector. This suggests that scaling laws in MLLMs are not merely a function of language model parameters but are heavily influenced by the visual perception capabilities. Furthermore, [3] highlights that a careful mix of pre-training data—including image-caption pairs, interleaved image-text data, and text-only data—is crucial for achieving state-of-the-art few-shot results. This indicates that scaling data quality and diversity is as important as scaling model parameters.

However, the "Energy Wall" looms large. Training a dense 30B parameter model requires immense resources, and inference costs scale linearly with model size. This creates a barrier to entry for academic research and limits the deployment of these models in resource-constrained environments. The challenge is to maintain or improve performance while reducing the computational footprint. This has led to a bifurcation in research: one path focuses on making models smaller and more efficient, while the other explores architectures that increase capacity without a linear increase in cost.

**Mixture of Experts (MoE): Scaling without the Linear Cost**

One of the most promising architectural innovations to address the scaling dilemma is the Mixture of Experts (MoE) paradigm. MoE models replace the dense feed-forward networks (FFNs) in the Transformer architecture with sparse FFNs, where each token is processed by only a subset of "expert" parameters. This allows the total number of parameters to increase significantly (e.g., into the trillions) while keeping the computational cost per token roughly constant.

In the context of MLLMs, MoE offers a pathway to build generalist models that are both highly capable and computationally efficient during inference. The **MM1** paper [3] explicitly explores MoE variants, finding that they achieve competitive performance on pre-training metrics and offer a favorable trade-off between parameter count and compute. By activating only a fraction of the model's parameters for any given input, MoE models can handle a diverse range of tasks without incurring the full cost of a massive dense model.

Furthermore, specialized MoE architectures for multimodal tasks are emerging. The **MoME: Mixture of Multimodal Experts** paper [96] proposes a framework specifically designed to mitigate task interference in generalist MLLMs. A generalist MLLM often underperforms specialist models on specific tasks because the parameters are shared across disparate domains. MoME addresses this by introducing a Mixture of Vision Experts (MoVE) and a Mixture of Language Experts (MoLE). MoVE adaptively modulates features from various vision encoders, while MoLE incorporates sparsely gated experts into the LLM. This allows the model to specialize for different tasks while maintaining a unified architecture. The results in [96] show that MoME significantly improves generalist performance across various vision-language tasks, demonstrating the potential of MoE to create more capable and efficient MLLMs.

Another notable example is **MoME: Mixture of Multimodal Experts** [96], which explores scaling MLLMs using a mixture-of-experts architecture to handle multiple modalities. By routing different modalities or tasks to specific experts, these models can scale their capacity effectively. The core advantage remains the same: increasing the total parameter count to improve model capacity and specialization, while keeping the inference cost manageable by leveraging sparsity. This is particularly important for MLLMs, which must process high-dimensional visual data and long textual contexts.

**Parameter-Efficient Fine-Tuning (PEFT) Strategies**

While MoE addresses the scaling of pre-trained models, Parameter-Efficient Fine-Tuning (PEFT) techniques are vital for adapting these massive models to downstream tasks without incurring the cost of full fine-tuning. Full fine-tuning of a billion-parameter MLLM requires updating all parameters, which is computationally expensive and storage-intensive (requiring a full model copy for each task). PEFT methods, such as Low-Rank Adaptation (LoRA), QLoRA, and LayerNorm tuning, freeze the majority of the pre-trained model and inject small, trainable adapters into specific layers.

In the multimodal domain, PEFT is particularly relevant because it allows for the integration of new modalities or specialized capabilities without disrupting the pre-trained knowledge. The **Parameter-Efficient Fine-Tuning (PEFT) Techniques** subsection in the survey outline highlights how these methods mitigate data conflicts and reduce computational costs. For example, **MoME** [96] utilizes PEFT principles in its design, allowing for efficient adaptation. Similarly, **MobileVLM V2** [133] demonstrates how a delicate orchestration of architectural design and improved training schemes can yield strong performance at smaller scales (1.7B to 3B parameters), making them amenable to efficient fine-tuning and deployment.

PEFT also plays a role in addressing the "Energy Wall" by reducing the memory footprint during training and inference. Techniques like QLoRA [132] (though not explicitly cited in the provided list, it is a standard PEFT method) further reduce memory usage by quantizing the base model. In the context of the provided papers, the focus is on architectural efficiency and smart training strategies. For instance, **Efficient Multimodal Large Language Models: A Survey** [132] specifically reviews the timeline and strategies for efficient MLLMs, emphasizing the importance of lightweight models for edge computing scenarios. This survey underscores that efficiency is not just about training cost but also about deployment feasibility.

**Optimization and Inference Efficiency**

Beyond architectural choices like MoE and PEFT, optimization techniques are crucial for managing the computational costs of MLLMs. The **Visual Tokens Withdrawal (VTW)** technique [19] is a prime example of an inference-time optimization. MLLMs typically encode images into a sequence of visual tokens, which significantly increases the context length and computational load in the deeper layers of the LLM. VTW observes that visual tokens receive minimal attention in the deep layers and that visual information migrates to text tokens early in the process. By strategically withdrawing visual tokens at a specific layer, VTW reduces the computational overhead by over 40% across diverse multimodal tasks while maintaining performance. This highlights that scaling efficiency isn't just about model architecture but also about optimizing the inference pipeline.

Similarly, **A-VL: Adaptive Attention for Large Vision-Language Models** [126] proposes adaptive attention mechanisms that dynamically reduce computational redundancy. By managing attention separately for visual and language inputs, A-VL reduces memory usage and computational load without compromising performance. These methods are essential for pushing the boundaries of what is possible within a given energy budget.

**The Role of Data in Scaling**

It is impossible to discuss scaling laws without addressing data. The **Synergy between Data and Multi-Modal Large Language Models** survey [10] emphasizes that the development of models and data is interconnected. As models scale, the demand for high-quality, diverse data increases. However, simply scaling data volume is insufficient; quality is paramount. The **MMEvol** framework [34] addresses this by generating more complex and diverse instruction data, thereby improving model capabilities without necessarily increasing model size. This suggests that scaling laws should also account for data complexity. Furthermore, **MM1** [3] found that dataset quality and task diversity are more important than scale, even during pre-training. This implies that efficient scaling involves not just more data, better data curation and generation strategies.

**Future Trajectory: Balancing Scale and Efficiency**

Looking forward, the trajectory of MLLM development will likely involve a hybrid approach. We will see the continued scaling of dense models for foundational research, but deployment will increasingly rely on efficient architectures. MoE will become a standard tool for building trillion-parameter-scale models that remain usable. PEFT will be the default method for adapting these models to specific domains, ensuring that the "Energy Wall" does not prevent the widespread application of MLLMs.

Moreover, the integration of these efficiency techniques with emerging capabilities, such as multimodal chain-of-thought reasoning and agentic behaviors, will be critical. As MLLMs evolve into systems that can plan, use tools, and interact with the world, the computational cost of each interaction must be minimized. The principles outlined in **Efficient Multimodal Large Language Models: A Survey** [132] and the innovations in **MobileVLM V2** [133] demonstrate that it is possible to achieve strong performance at smaller scales, challenging the notion that bigger is always better.

In conclusion, scaling laws in MLLMs are complex and multifaceted, influenced by model architecture, data quality, and modality alignment. To navigate the "Energy Wall," the field is turning to Mixture of Experts architectures to scale parameter count without a linear increase in compute, and to Parameter-Efficient Fine-Tuning strategies to adapt these massive models efficiently. Coupled with inference optimizations like Visual Tokens Withdrawal and adaptive attention, these approaches are paving the way for the next generation of MLLMs that are not only more capable but also more accessible and sustainable. The future of MLLMs lies not just in raw scale, but in the intelligent application of efficiency principles to unlock their full potential.

### 9.3 World Models and Embodied AI

The integration of World Models and Embodied AI principles represents a critical frontier in the evolution of Multimodal Large Language Models (MLLMs), moving them from passive observers of static data to active agents capable of physical grounding and interaction with dynamic environments. This shift is fundamental to the pursuit of Artificial General Intelligence (AGI), as it requires models to not only perceive and describe the world but also to predict the outcomes of actions and reason about causality within physical spaces [57]. By simulating future states, these systems can plan, act, and learn from feedback, effectively bridging the gap between digital intelligence and physical reality and providing the necessary foundation for the System 2 reasoning and agentic behaviors discussed previously.

**The Evolution and Architecture of World Models**

The concept of a World Model, originally popularized in reinforcement learning, involves training an agent to build an internal representation of its environment to predict future states. In the context of MLLMs, this concept has been expanded to encompass visual and temporal dynamics. Recent advancements have seen the rise of models like Sora and WorldGPT, which demonstrate the ability to generate coherent video sequences, effectively simulating physical interactions and object permanence. These models function as "simulators" of the physical world, allowing agents to "imagine" the consequences of potential actions without executing them in the real world. This capability is crucial for planning and safety in real-world deployment.

While traditional MLLMs excel at understanding static images and text, they often lack a coherent model of physical dynamics. The integration of World Model principles addresses this by enforcing consistency across time and action. For instance, a model capable of predicting the trajectory of a moving object or the result of a complex manipulation task relies on a deep understanding of physics and spatial relationships. This is a significant step beyond simple Visual Question Answering (VQA) or image captioning, moving towards a form of "System 2" reasoning where the model must simulate complex scenarios [90].

The architectural shift required to support World Models often involves moving beyond the standard "Encoder-Projector-LLM" paradigm. While the "Flamingo" architecture established the utility of cross-attention for interleaved visual-textual data [12], World Models require mechanisms to maintain state over long sequences of interactions. This has led to the exploration of hybrid architectures that combine the strengths of Transformers with more efficient sequence models. For example, the use of State Space Models (SSMs) like Mamba has shown promise in handling long contexts efficiently, which is essential for processing extended video sequences or interaction histories [24; 21]. These models offer linear scaling with sequence length, making the simulation of long-horizon tasks computationally feasible.

**Embodied AI: Grounding Language in Action**

Embodied AI focuses on training agents to perform tasks within specific physical environments, often using language instructions as goals. The synergy between MLLMs and Embodied AI lies in using the MLLM as the "brain" for the agent, processing visual inputs from the environment and generating high-level plans or low-level control commands. This requires the model to ground its vast knowledge of the world, acquired from internet-scale text and images, into specific sensory-motor loops.

A key challenge in Embodied AI is the "sim-to-real" gap: transferring policies learned in simulation to the real world. World Models help mitigate this by providing high-fidelity simulations that closely mimic real-world physics. By training agents to predict visual outcomes of actions within a World Model, the learned policies become more robust and generalize better to unseen scenarios. Furthermore, MLLMs enable "instruction following" in natural language, allowing humans to communicate complex goals to robots or virtual agents. For instance, an MLLM integrated into a robotic system could interpret a command like "pick up the blue cup on the table," process the visual scene to identify the target, and reason about the necessary sequence of actions.

The integration of MLLMs into Embodied AI also highlights the importance of "agentic" capabilities. Rather than just answering questions, MLLMs are increasingly being used as controllers for complex workflows, utilizing external tools and APIs to interact with the environment [134]. In an embodied context, the "tools" are the robot's actuators and sensors. The MLLM plans a sequence of actions, executes them (or simulates them in a World Model), observes the results, and refines its plan based on the feedback. This closed-loop interaction is a hallmark of intelligent behavior and a necessary component for autonomous systems.

**Bridging to AGI: Simulation, Prediction, and Interaction**

The convergence of World Models and Embodied AI via MLLMs serves as a direct bridge to AGI by addressing the fundamental limitations of current static models. AGI requires the ability to interact with the world dynamically, learn from experience, and adapt to novel situations. World Models provide the predictive capability necessary for planning and foresight, while Embodied AI provides the grounding mechanism to ensure that these predictions are physically valid and actionable.

One of the primary limitations of current MLLMs is their tendency to hallucinate—generating outputs that are inconsistent with visual evidence or factual reality [127]. World Models can act as a "reality check" by simulating the proposed state or action to verify its consistency with physical laws. For example, before generating a description of a complex scene, an MLLM could use a World Model to ensure that the spatial relationships and object interactions described are physically plausible.

Furthermore, the integration of these concepts enables the development of "Cognitive Architectures" that mimic human reasoning. This involves moving beyond System 1 intuition (fast, instinctive reactions) towards System 2 reasoning (slow, deliberate planning) [90]. World Models are inherently a System 2 mechanism, as they require the model to simulate multiple future steps. Embodied AI provides the context for this reasoning, grounding it in specific tasks and environments.

Recent research has begun to explore these integrations explicitly. For instance, frameworks that enable MLLMs to act as agents capable of planning and utilizing external tools [134] lay the groundwork for embodied control. The ability to process multi-modal inputs (vision, language, and potentially proprioception) and generate multi-modal outputs (language descriptions, action commands) is essential. The "Unified Multimodal LLMs" discussed in recent surveys [4] aim to handle both understanding and generation, which is crucial for an agent that must both perceive the world and act upon it.

**Challenges and Future Directions**

Despite the promise, significant challenges remain. Training World Models requires massive amounts of video data, which is harder to curate and process than text. Furthermore, simulating complex physical interactions requires high-fidelity rendering and physics engines, which are computationally expensive. The "Modality Gap" between visual features and language representations [35] is exacerbated in dynamic environments, where the model must align visual changes over time with linguistic descriptions of actions and consequences.

Another challenge is the evaluation of these systems. Standard benchmarks for MLLMs focus on static perception and reasoning [29]. Evaluating an embodied agent with a World Model requires new benchmarks that measure physical consistency, long-term planning, and the ability to recover from errors. Benchmarks that assess robustness against adversarial attacks and safety are also critical, as embodied agents operating in the real world must be safe and reliable.

Looking forward, the integration of World Models and Embodied AI will likely drive several key research directions. First, we will see the development of more efficient architectures that can handle long temporal horizons without prohibitive computational costs. The success of hybrid architectures [24] and State Space Models [21] suggests that the Transformer may not be the final architecture for these tasks. Second, training strategies will evolve to emphasize "grounded" pre-training, where models learn from video data that includes action labels and consequences, rather than just static image-text pairs. Third, the concept of "self-improvement" will become more prominent, where agents use their World Models to generate synthetic data for training, creating a virtuous cycle of learning.

The ultimate goal is to create a single, generalist model that can perceive, reason, plan, and act across a wide range of domains. This aligns with the vision of "Unified Multimodal Understanding and Generation" [4], where the distinction between understanding the world and acting in it blurs. By simulating the world (World Models) and interacting with it (Embodied AI), MLLMs can achieve a level of grounding and adaptability that is a prerequisite for true AGI. This trajectory suggests that the future of MLLMs lies not just in scaling up parameters or data, but in fundamentally changing their architecture and training objectives to embrace the dynamics of the physical world.

### 9.4 Cognitive Architectures and System 2 Reasoning

The rapid advancement of Multimodal Large Language Models (MLLMs) has demonstrated remarkable capabilities in System 1 intuition—fast, automatic perception and response generation. Models like GPT-4V [26] and Gemini [25] exhibit impressive performance across a wide array of tasks, often rivaling human-level perception in specific domains. However, these models frequently struggle with tasks requiring deliberate, multi-step reasoning, planning, and the integration of structured knowledge—a cognitive process often referred to as System 2 reasoning. This limitation highlights a critical frontier in AI research: the transition from purely reactive systems to those capable of deep, logical reasoning and agentic behavior. To bridge this gap, the community is increasingly looking towards cognitive architectures that explicitly model reasoning processes and integrate structured knowledge bases.

Current MLLMs largely operate as end-to-end systems trained on vast datasets of image-text pairs, optimizing for next-token prediction. While this approach has yielded emergent capabilities, it often falls short in scenarios requiring complex problem-solving that goes beyond pattern matching. The "Sparks of Artificial General Intelligence" observed in GPT-4 [27] suggest a potential for reasoning, yet systematic evaluation reveals significant gaps. For instance, benchmarks designed to test pure reasoning abilities, such as NPHardEval4V, show that MLLMs lag behind their text-only counterparts when visual inputs are transformed into reasoning problems, indicating that visual grounding does not automatically translate to enhanced logical deduction [47]. This suggests that simply scaling up data and parameters may not be sufficient to achieve robust System 2 capabilities.

To address these limitations, researchers are proposing the integration of cognitive architectures into MLLMs. These architectures aim to simulate human-like thought processes by decomposing complex tasks into manageable steps, utilizing external tools, and maintaining structured internal representations. A prominent example of this shift is the development of agentic systems where the MLLM acts as a central controller or "brain" for a multi-agent framework. The "Small LLMs Are Weak Tool Learners" paper highlights the difficulty smaller models face in tool use, but it also suggests a path forward through decomposition: separating the roles of planning, tool calling, and summarization into specialized sub-agents. This modular approach allows for more robust reasoning, as each component can be optimized for its specific function. Similarly, frameworks like "MLLM-Tool" [107] demonstrate how MLLMs can be trained to select and utilize external tools based on multimodal instructions, effectively grounding their reasoning in external APIs and expert models.

The concept of "World Models" is central to enabling System 2 reasoning in embodied AI and agentic systems. Unlike current MLLMs that primarily process static data, world models allow an agent to predict the outcomes of actions and simulate future states. This capability is essential for planning and decision-making in dynamic environments. The integration of world models with MLLMs is a key future direction, as it would allow models to move beyond passive perception to active interaction with their environment. This aligns with the vision of "Embodied AI," where the model is not just a conversational assistant but an agent capable of physical grounding and interaction. The "UnifiedMLLM" [135] framework attempts to unify various tasks under a single representation, outputting task tokens and grounding tokens that can be routed to specific expert models. This unified approach is a step towards a generalist cognitive architecture that can handle diverse tasks by leveraging specialized sub-systems.

Furthermore, the move towards System 2 reasoning necessitates the incorporation of structured knowledge and explicit reasoning steps. The "Chain-of-Thought" (CoT) prompting technique, which has been highly effective in LLMs, is being extended to multimodal contexts. "Multimodal Chain-of-Thought (M-CoT)" [90] involves generating intermediate visual and textual steps to solve complex problems. For example, "Image-of-Thought" and "Cantor" methods leverage visual cues to guide the reasoning process, while datasets like "Visual CoT" provide annotations for these intermediate steps. This explicit reasoning process is crucial for complex logical and abstract reasoning tasks, such as solving nonverbal puzzles or mathematical problems, where the model must articulate its logic rather than jumping directly to a conclusion.

However, building these advanced cognitive architectures presents significant challenges. One major hurdle is the "modality gap"—the difficulty in aligning visual representations with the semantic space of language models, which is essential for coherent reasoning. While pre-training on large-scale image-text data helps, it does not guarantee the model can reason about the relationships between visual elements. The "Generalist Multimodal AI" survey [136] emphasizes the need for architectures that support "Unifiability, Modularity, and Adaptability." These principles are vital for cognitive architectures that must integrate diverse inputs and knowledge sources while remaining flexible enough to adapt to new tasks.

Another critical aspect is the evaluation of these reasoning capabilities. Traditional benchmarks often focus on perception or simple question-answering, which are insufficient for assessing System 2 reasoning. New benchmarks are emerging to fill this gap. For instance, "NPHardEval4V" is designed to isolate reasoning abilities by converting textual reasoning problems into visual formats, preventing models from relying solely on visual recognition. Similarly, "LogicVista" and "InfiMM-Eval" test logical and abstract reasoning within multimodal contexts. The "MLLM-as-a-Judge" benchmark [95] proposes using MLLMs themselves to evaluate complex, open-ended responses, which is a step towards more holistic assessment of reasoning and cognitive alignment.

The integration of active inference and structured knowledge is also gaining traction. "Active inference" refers to the process of actively seeking information to reduce uncertainty, a hallmark of intelligent behavior. This requires the model to not just passively process inputs but to formulate hypotheses and plan actions to gather necessary data. Frameworks like "OpenAGI" and "UnifiedVisionGPT" are exploring how to combine LLMs with structured knowledge graphs and external reasoning modules to facilitate this. By offloading specific reasoning tasks to dedicated modules or "expert" models, the MLLM can focus on high-level planning and integration. This is evident in the "Expert Token Routing" approach [137], where a meta-LLM routes queries to specialized expert LLMs, effectively creating a cognitive system composed of multiple specialized components.

Moreover, the challenge of "General Capabilities Integration" (GCI) [138] is particularly relevant for cognitive architectures. As MLLMs are specialized for specific domains (e.g., medicine, finance), they risk losing the general reasoning abilities necessary for complex, cross-domain problem-solving. The proposed "ALoRA" method attempts to address this by enabling dynamic switching between domain-specific knowledge and general competencies. This is crucial for cognitive architectures that must operate across a wide range of scenarios without forgetting foundational reasoning skills.

The path towards System 2 reasoning also involves addressing the inherent limitations of current transformer architectures. While transformers are excellent at pattern recognition, they lack explicit mechanisms for long-term planning or recursive reasoning. Research into hybrid architectures, such as combining transformers with state-space models like Mamba [118], aims to handle long contexts more efficiently, which is a prerequisite for complex, multi-step reasoning. Additionally, the "Sparks of Artificial General Intelligence" paper [27] posits that moving beyond next-word prediction towards a new paradigm that incorporates world models and reasoning engines is necessary for achieving deeper AGI.

In conclusion, the transition from System 1 intuition to System 2 reasoning in MLLMs requires a fundamental shift in architecture and training paradigms. It is no longer sufficient to rely solely on scaling data and parameters. Instead, the future lies in cognitive architectures that explicitly model reasoning processes, integrate structured knowledge, and utilize external tools and expert models. Frameworks like UnifiedMLLM, MLLM-Tool, and the principles of agentic systems provide a blueprint for this evolution. By incorporating explicit reasoning steps like M-CoT, leveraging world models for simulation, and developing robust evaluation benchmarks that target cognitive depth, we can move closer to MLLMs that are not just perceptually gifted but truly capable of complex, deliberate problem-solving. This trajectory directly informs the subsequent challenges in evaluation, safety, and alignment, as these advanced cognitive architectures introduce new complexities in assessing reliability and ensuring robust, ethical behavior.

### 9.5 AGI Evaluation, Safety, and Alignment

The trajectory toward Artificial General Intelligence (AGI), propelled by advancements in Multimodal Large Language Models (MLLMs), necessitates a fundamental rethinking of evaluation paradigms, safety protocols, and alignment strategies. As models evolve from narrow task-specific solvers to generalist systems capable of complex reasoning and tool use, the traditional metrics of performance—primarily accuracy on held-out test sets—are becoming increasingly insufficient. The emergent capabilities of MLLMs, such as those observed in proprietary systems like GPT-4V and Gemini, require evaluation frameworks that can assess not just perceptual accuracy, but abstract reasoning, robustness, and ethical adherence. This subsection reviews the evolution of these evaluation benchmarks, the critical challenges in ensuring safety and robustness, and the alignment techniques required to deploy these systems responsibly.

**The Evolution of AGI Evaluation Benchmarks**

The evaluation of MLLMs is rapidly shifting from static, multiple-choice datasets to dynamic, holistic assessments that mirror cognitive science principles. Early benchmarks focused on foundational perception and understanding, such as VQAv2 and COCO, but these are no longer sufficient to distinguish between models that merely memorize patterns and those that reason. Consequently, the field is moving toward "Levels of AGI" frameworks, which categorize models based on their generality and proficiency, moving beyond narrow task performance.

To assess higher-order cognitive functions, new benchmarks have emerged that probe logical, mathematical, and spatial reasoning. For instance, benchmarks like MME and SEED-Bench evaluate complex reasoning capabilities, while MathVista assesses mathematical problem-solving in visual contexts. However, as models become more capable, they also become more susceptible to subtle failures. The "MM-Vet" and "MMT-Bench" frameworks represent a shift toward evaluating the integration of perception and reasoning, often utilizing "LLM-as-a-Judge" methodologies to score open-ended responses that traditional automated metrics struggle to evaluate. This evolution acknowledges that true AGI evaluation requires measuring the depth of understanding and the ability to synthesize information across modalities, rather than just identifying objects or generating captions [139].

Furthermore, the evaluation of AGI must extend beyond single-image inputs to encompass temporal and multi-image understanding. Benchmarks such as MIBench and MuirBench are designed to test the model's ability to reason over sequences of images or video, a crucial capability for real-world deployment where context evolves over time. The ultimate goal of these benchmarks is to create a comprehensive test suite that can reliably predict a model's behavior in open-world scenarios, ensuring that the metrics used for development align with the desired properties of AGI [140].

**The Pervasive Challenge of Hallucination and Robustness**

A defining characteristic of current MLLMs, and a major barrier to AGI, is the phenomenon of hallucination. Hallucinations occur when a model generates outputs that are factually incorrect or inconsistent with the provided visual input. These can be categorized into object hallucinations (inventing objects that do not exist), attribute hallucinations (misassigning properties), and relation hallucinations (fabricating relationships between entities). The root cause often lies in the "modality gap"—the disconnect between the visual encoder's representation space and the language model's semantic space—leading models to rely on statistical priors from text rather than grounding their responses in visual evidence [141].

To combat this, researchers have developed specific benchmarks like AMBER to quantify hallucination rates across various dimensions. However, hallucination is closely linked to the broader issue of model robustness. MLLMs are notoriously brittle; they can be easily misled by adversarial examples. Adversarial robustness is a critical concern for AGI, as models must be resilient to both visual perturbations (e.g., imperceptible noise in images) and textual jailbreaking attempts. Recent studies have shown that visual adversarial examples can bypass safety alignments, leading to the generation of harmful content. This vulnerability stems from the continuous nature of image data, which offers a much larger attack surface compared to discrete text tokens [60].

Moreover, the relationship between testing coverage and robustness has been scrutinized. While software engineering relies on coverage metrics to ensure code quality, analogous metrics for DNNs—such as neuron coverage—have shown limited correlation with actual robustness. Empirical studies on hundreds of models suggest that improving structural coverage does not necessarily improve resistance to adversarial attacks [32]. This disconnect implies that achieving AGI requires moving beyond simple structural testing toward metrics that directly measure the model's resilience and the integrity of its decision-making process.

**Safety Alignment and Jailbreaking**

Safety alignment ensures that models adhere to human values and refuse to execute harmful instructions. However, the multimodal nature of MLLMs introduces unique vulnerabilities. "Jailbreaking" in MLLMs involves crafting inputs that bypass these safety guardrails. Attackers often exploit "competing objectives" (balancing helpfulness and safety) and "mismatched generalization" (where safety training in one modality does not transfer to another) to elicit prohibited content. For example, a seemingly innocuous image paired with a specific text prompt can trick a model into generating dangerous instructions.

The susceptibility of MLLMs to these attacks highlights the difficulty of ensuring safety in high-dimensional input spaces. Unlike unimodal LLMs, where safety filters can be applied to text, MLLMs must process and align safety constraints across heterogeneous data streams. This complexity necessitates robust evaluation benchmarks like MM-SafetyBench and JailBreakV-28K, which are specifically designed to test the model's vulnerability to multimodal adversarial attacks. The findings from these benchmarks underscore the urgent need for more sophisticated alignment techniques that account for the interplay between modalities [142].

**Mitigation Strategies and Ethical Guardrails**

Addressing hallucinations, robustness, and safety requires a multi-faceted approach involving both training-time and inference-time strategies. On the training front, data curation plays a pivotal role. High-quality instruction-tuning datasets, such as ShareGPT4V and LLaVA-Instruct, are essential for grounding models in visual evidence. Furthermore, synthetic data generation using powerful proprietary models (e.g., GPT-4V) is being used to bridge the modality gap and augment training data with complex reasoning examples. However, data selection strategies must be refined to prioritize data that maximizes robustness rather than just accuracy. Techniques like gradient-based value estimation (TIVE) and difficulty-based filtering (Cherry_LLM) are emerging to identify the most impactful training samples for improving model reliability [143].

At the architectural level, parameter-efficient fine-tuning (PEFT) techniques like LoRA allow for targeted safety alignment without full model retraining. However, inference-time defenses are equally critical. "Adaptive shield prompting" and "image-to-text transformation" are training-free methods that can be applied to detect and mitigate harmful outputs before they are generated. Additionally, multi-agent debate systems, where multiple models critique each other's outputs, offer a promising avenue for detecting hallucinations and safety violations.

Ethical guardrails for deployment must also consider the "trustworthiness" of the model. This involves not just preventing harm but also ensuring that the model's predictions are reliable under uncertainty. The concept of "Trust Index," proposed in recent literature, suggests evaluating models based on their optimal risk profile, acknowledging that high accuracy on standard benchmarks does not always correlate with low failure rates in critical scenarios [144]. Establishing these ethical guardrails requires a shift from purely performance-driven development to a safety-first mindset, where robustness and alignment are treated as first-class design constraints.

**Conclusion: The Path to Trustworthy AGI**

The journey toward AGI is not merely a race for scale but a pursuit of reliability. The evolution of evaluation benchmarks from simple perception tasks to complex cognitive and safety assessments reflects the maturity of the field. However, the persistent issues of hallucination, adversarial vulnerability, and alignment failures indicate that current MLLMs are still far from being truly trustworthy. The research community must prioritize the development of models that are not only capable but also robust and aligned with human values. This requires a concerted effort to refine evaluation methodologies, improve data quality, and implement rigorous safety protocols. Only by addressing these challenges can we ensure that the trajectory toward AGI leads to systems that are beneficial and safe for society [131].

### 9.6 Conclusion: The Trajectory Toward AGI

The journey of Multimodal Large Language Models (MLLMs) represents a monumental leap in artificial intelligence, transitioning from narrow, task-specific systems to generalist agents capable of perceiving, reasoning, and acting across diverse modalities. As we stand at the cusp of what may be the dawn of Artificial General Intelligence (AGI), it is crucial to synthesize the architectural, data-centric, and reasoning advancements chronicled in this survey to chart the trajectory ahead. The evolution from unimodal giants like GPT-3 and vision encoders such as ResNet to unified systems like GPT-4V and Gemini signifies a paradigm shift where the integration of sensory inputs and linguistic reasoning has birthed emergent capabilities previously thought to be exclusive to human cognition. This conclusion reflects on the current state of the field, highlighting how the convergence of efficient architectures, high-quality data ecosystems, and advanced reasoning paradigms is paving the way toward capable, efficient, and aligned AGI.

The architectural foundation of MLLMs has undergone a radical transformation, moving beyond the simple "CLIP-MLP-LLM" pipeline to embrace efficiency, flexibility, and scale. Early models established the viability of aligning visual and textual spaces, but the limitations of fixed-resolution processing and quadratic computational complexity quickly became apparent. The community's response has been a flurry of innovations aimed at overcoming these bottlenecks. For instance, the exploration of the design space for mixture of encoders, as demonstrated in [23], revealed that simple concatenation of complementary visual features from diverse experts can significantly enhance perception. This idea of leveraging multiple specialized vision encoders is further refined in [145], which introduces a coarse-to-fine routing mechanism to dynamically select the most suitable visual expert based on the multimodal context, thereby mitigating the biases of any single encoder. Similarly, [146] highlights the utility of incorporating perception modalities like segmentation maps to improve object-level understanding, a crucial step toward fine-grained visual reasoning.

Simultaneously, the challenge of high-resolution processing has been a central theme. The quadratic complexity of Vision Transformers (ViTs) with increasing image resolution necessitates innovative token management strategies. [68] addresses this by dividing native-resolution images into variable-sized slices and employing a compression module, allowing the model to handle images up to 672x1088 resolution with minimal computational overhead. This approach of dynamic resolution scaling is echoed in [147], which introduces a Naive Dynamic Resolution mechanism that processes images of varying resolutions into different numbers of visual tokens, mimicking human perception more closely. Furthermore, the "less is more" philosophy, where fewer but more informative tokens lead to better performance, is explored in [71] and [148]. These works demonstrate that compressing visual information through learnable queries or mixture-of-resolution adapters can maintain performance while drastically reducing token counts. The use of hierarchical backbones like ConvNeXt in [149] offers another path to efficiency, compressing high-resolution images into rich features without generating excessive tokens. These architectural strides are not merely about performance; they are about making MLLMs deployable and scalable, a prerequisite for AGI.

Beyond the core architecture, the role of the data ecosystem and training paradigms cannot be overstated. The transition from large-scale, noisy pre-training corpora like LAION to high-quality, meticulously curated instruction-tuning datasets has been pivotal. The curation strategies discussed in [150] underscore the importance of using diverse perception experts to generate hyper-detailed descriptions, thereby bridging the modality gap and enhancing the model's ability to comprehend complex visual scenes. The rise of synthetic data generation, leveraging powerful proprietary models to create instruction data, further accelerates this process. Moreover, the training process itself has become more sophisticated. Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA and MoE architectures, as explored in [151], allow for scaling model capacity without a linear increase in inference cost. [114] notably shows that a sparse model with 3B active parameters can match or exceed the performance of dense 7B or 13B models, a critical insight for building efficient AGI. The integration of Mixture of Experts (MoE) not only scales capacity but also mitigates modality conflicts, as evidenced by [152], which uses MoE to improve performance across diverse tasks while using a fraction of the computational cost.

The true measure of MLLM progress, however, lies in their emergent capabilities, particularly in complex reasoning and instruction following. We have moved beyond simple visual question answering to models that can perform multimodal Chain-of-Thought (M-CoT) reasoning, as exemplified by [153] and the "Image-of-Thought" methodologies. These models generate intermediate visual and textual rationales, mimicking human problem-solving processes. The ability to understand abstract and logical reasoning, as probed by benchmarks like LogicVista, reveals that while significant gaps remain between open-source and proprietary models, the trajectory is clear. Furthermore, the integration of agentic capabilities, where MLLMs act as controllers for external tools and APIs, marks a shift from passive perception to active problem-solving. This is a cornerstone of AGI, where the model is not just a repository of knowledge but an agent that can plan, act, and interact with its environment. The advancements in 3D spatial reasoning, as demonstrated by [154], and video understanding, where models like [110] handle arbitrary temporal lengths, further expand the frontier of what MLLMs can perceive and reason about.

However, the path to AGI is fraught with challenges, primarily concerning robustness, safety, and hallucination. As MLLMs become more integrated into critical systems, their vulnerabilities to adversarial attacks and jailbreaking become unacceptable risks. The analysis of adversarial robustness in [155] highlights how the continuous nature of image data makes it susceptible to manipulation that can bypass safety alignments. The pervasive issue of hallucination, where models fabricate objects or attributes not present in the image, remains a significant hurdle. Benchmarks like [33] and techniques like [156] are emerging to quantify and mitigate these issues, but a fundamental solution is yet to be found. The concept of "competing objectives" where the instruction-following goal conflicts with safety alignment is a critical area of study. Building trustworthy systems requires not just better training data but architectural innovations that enforce grounding and factuality, perhaps through retrieval-augmented generation (RAG) or multi-agent debate systems.

Looking forward, the trajectory toward AGI involves several key directions. First is the move toward unified models that seamlessly integrate understanding and generation. Current systems often treat these as separate tasks, but future MLLMs, inspired by works like [157], will likely employ a single framework capable of processing and generating text, images, audio, and video. This requires solving the challenge of balancing distinct objectives within a single training regimen. Second, scaling laws will continue to guide development, but with a focus on efficiency. The "Energy Wall" necessitates innovations like MoE and advanced quantization to scale model parameters and context lengths without prohibitive costs. The exploration of "World Models," such as those hinted at by [158] and [159], represents a third frontier. By integrating predictive simulation capabilities, MLLMs can move from reactive perception to proactive reasoning about physical dynamics, a prerequisite for embodied AI and robotics. Fourth, achieving System 2 reasoning—the slow, deliberate, logical thought process—remains the holy grail. Current MLLMs largely operate on System 1 intuition. Architectures that incorporate active inference, structured knowledge retrieval, and cognitive architectures like [160] will be essential for complex problem-solving.

Finally, the alignment problem becomes paramount as we approach AGI. Ensuring that these powerful systems are safe, ethical, and aligned with human values is not an afterthought but a foundational requirement. The evolution of evaluation benchmarks from simple accuracy metrics to holistic assessments of safety, robustness, and cognitive science-inspired tests is crucial. The "Levels of AGI" framework provides a roadmap, but we must also develop dynamic, adversarial benchmarks that can keep pace with model capabilities. The journey outlined in this survey, from the architectural blueprints of [23] and [68] to the sophisticated reasoning of [153] and the safety considerations in [83], demonstrates that we are building the necessary components for AGI. The trajectory is not linear but an exponential convergence of better architectures, richer data, deeper reasoning, and stricter safety. While significant challenges in hallucination, robustness, and true understanding remain, the collective progress in Multimodal Large Language Models provides a compelling and tangible path toward the realization of efficient, capable, and aligned Artificial General Intelligence.


## References

[1] A Survey on Multimodal Large Language Models

[2] A Survey on Evaluation of Multimodal Large Language Models

[3] MM1  Methods, Analysis & Insights from Multimodal LLM Pre-training

[4] The (R)Evolution of Multimodal Large Language Models  A Survey

[5] OneLLM  One Framework to Align All Modalities with Language

[6] LLMs Meet Multimodal Generation and Editing: A Survey

[7] Vision-Language Models for Vision Tasks  A Survey

[8] Are Bigger Encoders Always Better in Vision Large Models?

[9] A Survey of Vision-Language Pre-Trained Models

[10] The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective

[11] VisionLLM  Large Language Model is also an Open-Ended Decoder for  Vision-Centric Tasks

[12] Flamingo  a Visual Language Model for Few-Shot Learning

[13] OpenFlamingo  An Open-Source Framework for Training Large Autoregressive  Vision-Language Models

[14] Beyond Specialization  Assessing the Capabilities of MLLMs in Age and  Gender Estimation

[15] Honeybee  Locality-enhanced Projector for Multimodal LLM

[16] TokenPacker: Efficient Visual Projector for Multimodal LLM

[17] NVLM: Open Frontier-Class Multimodal LLMs

[18] INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model

[19] Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference

[20] LLaVA-MoLE  Sparse Mixture of LoRA Experts for Mitigating Data Conflicts  in Instruction Finetuning MLLMs

[21] VL-Mamba  Exploring State Space Models for Multimodal Learning

[22] ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2

[23] Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders

[24] LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture

[25] A Challenger to GPT-4V  Early Explorations of Gemini in Visual Expertise

[26] The Dawn of LMMs  Preliminary Explorations with GPT-4V(ision)

[27] Sparks of Artificial General Intelligence  Early experiments with GPT-4

[28] From GPT-4 to Gemini and Beyond  Assessing the Landscape of MLLMs on  Generalizability, Trustworthiness and Causality through Four Modalities

[29] A Survey on Benchmarks of Multimodal Large Language Models

[30] How Far Are We to GPT-4V  Closing the Gap to Commercial Multimodal  Models with Open-Source Suites

[31] MiniCPM-V: A GPT-4V Level MLLM on Your Phone

[32] There is Limited Correlation between Coverage and Robustness for Deep  Neural Networks

[33] MMEAD  MS MARCO Entity Annotations and Disambiguations

[34] MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct

[35] Mysterious Projections  Multimodal LLMs Gain Domain-Specific Visual  Capabilities Without Richer Cross-Modal Projections

[36] HyperLLaVA  Dynamic Visual and Language Expert Tuning for Multimodal  Large Language Models

[37] PILL  Plug Into LLM with Adapter Expert and Attention Gate

[38] What is the Role of Small Models in the LLM Era: A Survey

[39] Swift for TensorFlow  A portable, flexible platform for deep learning

[40] LLM Inference Serving: Survey of Recent Advances and Opportunities

[41] OptiMUS  Optimization Modeling Using MIP Solvers and large language  models

[42] MM-LLMs  Recent Advances in MultiModal Large Language Models

[43] LLaMA-Adapter V2  Parameter-Efficient Visual Instruction Model

[44] MME  A Comprehensive Evaluation Benchmark for Multimodal Large Language  Models

[45] ChEF  A Comprehensive Evaluation Framework for Standardized Assessment  of Multimodal Large Language Models

[46] LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts

[47] NPHardEval4V  A Dynamic Reasoning Benchmark of Multimodal Large Language  Models

[48] The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large  Language Models

[49] Developing a Reliable, General-Purpose Hallucination Detection and Mitigation Service: Insights and Lessons Learned

[50] Adversarial Attacks and Defences  A Survey

[51] Benchmarking Sequential Visual Input Reasoning and Prediction in  Multimodal Large Language Models

[52] Multimodal Large Language Models for Bioimage Analysis

[53] GQA  A New Dataset for Real-World Visual Reasoning and Compositional  Question Answering

[54] Microsoft COCO  Common Objects in Context

[55] MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V

[56] ReForm-Eval  Evaluating Large Vision Language Models via Unified  Re-Formulation of Task-Oriented Benchmarks

[57] A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks

[58] Grounded 3D-LLM with Referent Tokens

[59] Revisiting Neuron Coverage for DNN Testing  A Layer-Wise and  Distribution-Aware Criterion

[60] A Comprehensive Evaluation Framework for Deep Model Robustness

[61] OpenLLM-Ro -- Technical Report on Open-source Romanian LLMs

[62] MMBench  Is Your Multi-modal Model an All-around Player 

[63] OceanBench  The Sea Surface Height Edition

[64] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis

[65] Qiniu Submission to ActivityNet Challenge 2018

[66] On NVD Users' Attitudes, Experiences, Hopes and Hurdles

[67] LVCHAT  Facilitating Long Video Comprehension

[68] LLaVA-UHD  an LMM Perceiving Any Aspect Ratio and High-Resolution Images

[69] Qwen2 Technical Report

[70] POPE  Partial Order Preserving Encoding

[71] Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models

[72] Stack Exchange Tagger

[73] EAGLE  Speculative Sampling Requires Rethinking Feature Uncertainty

[74] Have a Look at What I See

[75] The Dune Python Module

[76] NFT1000  A Visual Text Dataset For Non-Fungible Token Retrieval

[77] Token Fusion  Bridging the Gap between Token Pruning and Token Merging

[78] Retrospectives on the Embodied AI Workshop

[79] Large Language Models Meet Computer Vision  A Brief Survey

[80] VisionLLaMA  A Unified LLaMA Interface for Vision Tasks

[81] LM4LV: A Frozen Large Language Model for Low-level Vision Tasks

[82] Trends in Integration of Vision and Language Research  A Survey of  Tasks, Datasets, and Methods

[83] Online Safety Analysis for LLMs  a Benchmark, an Assessment, and a Path  Forward

[84] A Unified Seeding Framework

[85] MM-Vet  Evaluating Large Multimodal Models for Integrated Capabilities

[86] The MMT API  A Generic MKM System

[87] Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step

[88] Poisoned LangChain: Jailbreak LLMs by LangChain

[89] An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models

[90] Exploring the Reasoning Abilities of Multimodal Large Language Models  (MLLMs)  A Comprehensive Survey on Emerging Trends in Multimodal Reasoning

[91] Fact  Teaching MLLMs with Faithful, Concise and Transferable Rationales

[92] Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models

[93] ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models

[94] MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era

[95] MLLM-as-a-Judge  Assessing Multimodal LLM-as-a-Judge with  Vision-Language Benchmark

[96] MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models

[97] EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model

[98] mPLUG-Owl2  Revolutionizing Multi-modal Large Language Model with  Modality Collaboration

[99] A Survey of Multimodal Large Language Model from A Data-centric Perspective

[100] CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation

[101] VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks

[102] Vision Language Models in Autonomous Driving and Intelligent  Transportation Systems

[103] Human perception in computer vision

[104] I-PHYRE  Interactive Physical Reasoning

[105] Planning to Chronicle

[106] Hallucination of Multimodal Large Language Models: A Survey

[107] MLLM-Tool  A Multimodal Large Language Model For Tool Agent Learning

[108] LIME: Less Is More for MLLM Evaluation

[109] Research Re  search & Re-search

[110] Platypus  Quick, Cheap, and Powerful Refinement of LLMs

[111] ZipLoRA  Any Subject in Any Style by Effectively Merging LoRAs

[112] Autonomous Agent Behaviour Modelled in PRISM -- A Case Study

[113] Joint Visual and Text Prompting for Improved Object-Centric Perception  with Multimodal Large Language Models

[114] LSVOS Challenge 3rd Place Report: SAM2 and Cutie based VOS

[115] Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models

[116] Law of Vision Representation in MLLMs

[117] Cobra  Extending Mamba to Multi-Modal Large Language Model for Efficient  Inference

[118] ReMamba: Equip Mamba with Effective Long-Sequence Modeling

[119] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models

[120] LLM Maybe LongLM  Self-Extend LLM Context Window Without Tuning

[121] Supporting Process Maturation with the Enhanced CoBRA Method

[122] Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning

[123] Model Compression

[124] Faster and Lighter LLMs  A Survey on Current Challenges and Way Forward

[125] Libra: Building Decoupled Vision System on Large Language Models

[126] A-VL: Adaptive Attention for Large Vision-Language Models

[127] Investigating the Catastrophic Forgetting in Multimodal Large Language  Models

[128] Dense Connector for MLLMs

[129] Revisiting Multi-Modal LLM Evaluation

[130] Neural Architecture Design and Robustness  A Dataset

[131] Unsolved Problems in ML Safety

[132] Efficient Multimodal Large Language Models: A Survey

[133] MobileVLM V2  Faster and Stronger Baseline for Vision Language Model

[134] A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning

[135] UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model

[136] Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities

[137] An Expert is Worth One Token  Synergizing Multiple Expert LLMs as  Generalist via Expert Token Routing

[138] More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs

[139] A Survey on Neural Architecture Search

[140] Generalization in Neural Networks  A Broad Survey

[141] A Comprehensive Survey of Neural Architecture Search  Challenges and  Solutions

[142] RobOT  Robustness-Oriented Testing for Deep Learning Systems

[143] Coverage-centric Coreset Selection for High Pruning Rates

[144] Empirical Optimal Risk to Quantify Model Trustworthiness for Failure  Detection

[145] MoVA  Adapting Mixture of Vision Experts to Multimodal Context

[146] VCoder  Versatile Vision Encoders for Multimodal Large Language Models

[147] Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution

[148] Feast Your Eyes  Mixture-of-Resolution Adaptation for Multimodal Large  Language Models

[149] ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models

[150] DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception

[151] MoE-LLaVA  Mixture of Experts for Large Vision-Language Models

[152] Alternating Gradient Descent and Mixture-of-Experts for Integrated  Multimodal Perception

[153] PerceptionGPT  Effectively Fusing Visual Perception into LLM

[154] M3  Semantic API Migrations

[155] Is CLIP the main roadblock for fine-grained open-world perception 

[156] Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models

[157] Garden-Path Traversal in GPT-2

[158] Sora  A Review on Background, Technology, Limitations, and Opportunities  of Large Vision Models

[159] TimeGPT-1

[160] UnifiedVisionGPT  Streamlining Vision-Oriented AI through Generalized  Multimodal Framework


